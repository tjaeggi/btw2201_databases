{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Beilagen zum Modul btw2201 - Databases","text":"Let's understand the mystery of databases! <p>Auf dieser Webseite finden Sie \u00dcbungen mit L\u00f6sungsvorschl\u00e4gen und weitergehende Informationen zu den Themen in der Vorlesung. Alle Inhalte auf dieser Webseite werden in den Unterlagen auf Moodle referenziert. </p> <p>Bei Fragen/Anregungen oder Korrekturen k\u00f6nnen sie sich jederzeit und gerne in der Vorlesung, via Teams, Moodle oder E-Mail melden.</p> <p>Thomas J\u00e4ggi</p> <p>Berner Fachhochschule, WING</p> <p>thomas.jaeggi@bfh.ch</p>"},{"location":"le01/Loesungsvorschlaege_ue01-1/","title":"L\u00f6sungsvorschlag Punkte 2 und 3","text":"<p>Das Skript ermittelt die Anzahl Records im csv und schreibt f\u00fcr jede Record-Art ein CSV-File:</p> <p>Fragen zu beantworten</p> <ol> <li>Wie viele Records sind im CSV enthalten?</li> <li>Erstellen Sie aus dem einzelnen csv je ein csv pro Record-Art. (=9 CSVs)</li> </ol> <p>L\u00f6sungsvorschlag:</p> <pre><code>\"\"\"\nFile: read_swisspost_csv.py\nAuthor: Thomas J\u00e4ggi\nDate: 25.09.2024\nDescription: \nRead the official Swiss-Post csv with street information and create for each table (REC_ART) a seperate csv-File with the name of the table.\nTable Names are defined by the column REC_ART and are described in \"Anleitung Strassenverzeichnisse.pdf\"\n\"\"\"\n\nimport pandas as pd\n\n# Input-File is a ANSI-File, Windows 1252 encoded, separated with Semicolon\ncsv_file = \"Post_Adressdaten20240903.csv\"\n\n# Delimiter\ndelimiter = ';'\n\n# Since the csv-File does not have the same number of columns for each row, we have to \n# determine the maximum number of columns  and include them in the dataframe as names.\n# The name will be a number from 0 to max_columns-1.\nlargest_column_count = 0\n\n# Loop the data lines and determine the maximum number of columns per row. Name the column with a number from 0 to largest_column_count\nwith open(csv_file, 'r', encoding=\"cp1252\") as temp_f:\n\n    # get No of columns in each line\n    col_count = [ len(row.split(delimiter)) for row in temp_f.readlines() ]\n\n### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)\ncolumn_names = [i for i in range(0, max(col_count))]\n\n\n# Read csv into dataframe df and print. Set Encoding to cp1252 for windows.\ndf = pd.read_csv(csv_file, header=None, delimiter=delimiter, names=column_names, encoding=\"cp1252\")\nprint('First 20 rows in csv')\nprint(df.head(20))\nprint('Last 20 rows in csv')\nprint(df.tail(20))\nprint('Number of lines in csv-File:')\nprint(df.shape[0])\n\n# Generate for each REC_ART one csv-File. groupby([0]): Group by REC_ART which is column 0\nfor (rec_art), group in df.groupby([0]):\n\n     if rec_art == (0,):\n         print('Writing csv NEW_HEA ..')\n         tablename = 'NEW_HEA'\n     if rec_art == (1,):\n         print('Writing csv NEW_PLZ ..')\n         tablename = 'NEW_PLZ1'\n     if rec_art == (2,):\n         print('Writing csv NEW_PLZ2 ..')\n         tablename = 'NEW_PLZ2'\n     if rec_art == (3,):\n         print('Writing csv NEW_COM ..')\n         tablename = 'NEW_COM'\n     if rec_art == (4,):\n         print('Writing csv NEW_STR ..')\n         tablename = 'NEW_STR'\n     if rec_art == (5,):\n         print('Writing csv NEW_STRA ..')\n         tablename = 'NEW_STRA'\n     if rec_art == (6,):\n         print('Writing csv NEW_GEB ..')\n         tablename = 'NEW_GEB'\n     if rec_art == (7,):\n         print('Writing csv NEW_GEBA ..')\n         tablename = 'NEW_GEBA'\n     if rec_art == (8,):\n         print('Writing csv NEW_BOT_B ..')\n         tablename = 'NEW_BOT_B'\n     if rec_art == (12,):\n         print('Writing csv NEW_GEB_COM ..')\n         tablename = 'NEW_GEB_COM'\n     if rec_art == (10,):\n         print('Writing csv NEW_GEO ..')\n         tablename = 'NEW_GEO'\n     if rec_art == (11,):\n         print('Writing csv NEW_HH ..')\n         tablename = 'NEW_HH'\n\n     # Using groupby() method of Pandas we can create multiple CSV files. To create a file we can use the to_csv() method of  Pandas.\n     group.to_csv(f'{tablename}.csv', index=False, encoding=\"cp1252\", sep=delimiter)\n</code></pre>"},{"location":"le01/Loesungsvorschlaege_ue01-1/#losungsvorschlag-punkt-4","title":"L\u00f6sungsvorschlag Punkt 4","text":"<p>Fragen zu beantworten</p> <ol> <li>Beantworten Sie mit dem erstellten CSV, welches REC_ART=1 (=NEW_PLZ1) enth\u00e4lt, folgende Fragen:<ol> <li>Wie viele verschiedene Postleitzahlen gibt es in der Schweiz und im F\u00fcrstentum Lichtenstein?</li> <li>Wie viele Gemeinden tragen die Postleitzahl 4566, 4556 und 1580 und in welchen Kantonen befinden sich diese?</li> <li>Wie viele Gemeinden haben dieselbe PLZ wie ihre Wohngemeinde?</li> </ol> </li> </ol> <p>L\u00f6sungsvorschlag:</p> <pre><code>\"\"\"\nFile: abfragen1file.py\nAuthor: Thomas J\u00e4ggi\nDate: 25.09.2024\nDescription: Perform some queries on the PLZ-Datafile. \n\"\"\"\n\nimport pandas as pd\n\n# Input-File is a ANSI-File, Windows 1252 encoded, separated with Semicolon\ncsv_file = \"NEW_PLZ1.csv\"\n\n# Delimiter\ndelimiter = ';'\n\ndf = pd.read_csv(csv_file, delimiter=delimiter, encoding=\"cp1252\")\n# Print PLZ (Column 4), Ort (Column 7) und Kanton (Column 9)\nprint(df[['4', '7', '9']])\n\n# Number of unique PLZ's in Switzerland\nn = len(pd.unique(df['4']))\nprint(\"Nr of unique PLZ in Switzerland:\", n)\n\n\n\n# Number of Ortschaften with PLZ 3000\n# To count the number of rows that satisfy the condition, you should use first df[] to filter the rows and \n# then use the len() to count the rows after the rows are filtered with the condition. \n# You need to select  column 4 in DataFrame to check if any value of the '4' column is equal to PLZ. \n# When condition matched, len() function counts the number of rows that contain it.\nnr1 = len(df[df['4']==3000])\nprint(\"No of recors with PLZ 3000:\",nr1)\n\n# Number of Records with PLZ 1000\nnr2 = len(df[df['4']==1000])\nprint(\"No of recors with PLZ 1000:\",nr2)\n\n# Number of Records with PLZ 4000\nnr3 = len(df[df['4']==4000])\nprint(\"No of recors with PLZ 4000:\",nr3)\n\n# Number of Records with PLZ 2500\nnr4 = len(df[df['4']==2500])\nprint(\"No of recors with PLZ 2500:\",nr4)\n\n# Number of Records with PLZ 4556\nnr4 = len(df[df['4']==4556])\nprint(\"No of recors with PLZ 4556:\",nr4)\n\n# Welche Ortsbezeichnungen geh\u00f6ren zu PLZ 4566\n# Remember: df[['4', '7', '9']]: This line prints a subset of the DataFrame that includes only the column with name \n# '4' (PLZ) and '7' (Ort) and '9' (Kanton)\ndf2=(df[df['4']==4566])\nprint('Gemeinden 4566:\\n',df2[['4', '7', '9']])\n\n# Welche Ortsbezeichnungen geh\u00f6ren zu PLZ 1580\ndf3=(df[df['4']==1580])\nprint('Gemeinden 1580:\\n',df3[['4', '7', '9']])\n\n# Welche Ortsbezeichnungen geh\u00f6ren zu PLZ 3000\ndf3=(df[df['4']==3000])\nprint('Gemeinden 3000:\\n',df3[['4', '7', '9']])\n\n# Welche Ortsbezeichnungen geh\u00f6ren zu PLZ 2500\ndf3=(df[df['4']==2500])\nprint('Gemeinden 2500:\\n',df3[['4', '7', '9']])\n\n\n# Welche Ortsbezeichnungen geh\u00f6ren zu PLZ 4556\ndf3=(df[df['4']==4556])\nprint('Gemeinden 4556:\\n',df3[['4', '7', '9']])\n</code></pre>"},{"location":"le01/Loesungsvorschlaege_ue01-1/#losungsvorschlag-punkt-5","title":"L\u00f6sungsvorschlag Punkt 5","text":"<p>Fragen zu beantworten</p> <ol> <li>Unter Einbezug des CSV-Files mit Record_Art=4 (NEW_STR) k\u00f6nnen Sie auch folgende Fragen beantworten:<ol> <li>In welchen Gemeinden mit Kanton existiert dieselbe Strasse ihrer Wohnadresse?</li> <li>In welchen Gemeinden gibt es ebenfalls eine \"Quellgasse\"?</li> </ol> </li> </ol> <p>L\u00f6sungsvorschlag:</p> <pre><code>\"\"\"\nFile: abfragen2files.py\nAuthor: Thomas J\u00e4ggi\nDate: 25.09.2024\nDescription: Perform some queries on the PLZ- and STR-Datafiles. Answer the question: In how many villages/cities exist streetname = Quellgasse\n\"\"\"\n\nimport pandas as pd\n\n# Input-File is a ANSI-File, Windows 1252 encoded, separated with Semicolon. Has been written in the previous task.\ncsv_file1 = \"NEW_PLZ1.csv\"\ncsv_file2 = \"NEW_STR.csv\"\n\n# Delimiter\ndelimiter = ';'\n\n#Now we will import the records with the column names specified in \"Anleitung Strassenverzeichnisse.pdf\", page 8 (NEW_PLZ1) and 11 (NEW_STR). Both csv's have 16 columns, so we name the additional columns just with numbers.\n#in order to be consistent with the number of columns in the source file. The values will be NaN-Values, since they are empty.  \ncolnamesPLZ1=['REC_ART', 'ONRP', 'BFSNR', 'PLZ_TYP', 'POSTLEITZAHL', 'PLZ_ZZ', 'GPLZ', 'ORTBEZ18', 'ORTBEZ27', 'KANTON', 'SPRACHCODE', 'SPRACHCODE_ABW', 'BRIEFZ_DURCH', 'GILT_AB_DAT', 'PLZ_BRIEFZUST', 'PLZ_COFF'] \ncolnamesSTR= ['REC_ART','STRID','ONRP','STRBEZK','STRBEZL','STRBEZ2K','STRBEZ2L','STR_LOK_TYP','STRBEZ_SPC','STRBEZ_COFF','STR_GANZFACH','STR_FACH_ONRP','12','13','14','15'] \n\n\n#Read both csv-Files into a dataframe with the defined column names above.\ndfPLZ1 = pd.read_csv(csv_file1, delimiter=delimiter, encoding=\"cp1252\", names=colnamesPLZ1)\ndfSTR = pd.read_csv(csv_file2, delimiter=delimiter, encoding=\"cp1252\", names=colnamesSTR)\n\n# Some checks to be sure there is data..\n\n#print('First 20 rows in PLZ1-csv')\n#print(dfPLZ1.head(20))\n\n#print('Last 20 rows in PLZ1-csv')\n#print(dfPLZ1.tail(20))\n\n#print('Number of lines in PLZ1-csv-File:')\n#print(dfPLZ1.shape[0])\n\n#print('First 20 rows in STR-csv')\n#print(dfSTR.head(20))\n\n#print('Last 20 rows in STR-csv')\n#print(dfSTR.tail(20))\n\n#print('Number of lines in STR-csv-File:')\n#print(dfSTR.shape[0])\n\n# Merge dataframes with column name \"ONRP\". ONRP is the \"Link\" between the 2 files.\ndf = pd.merge(dfSTR, dfPLZ1, on='ONRP')\nprint(df)\n\n\n# Query: In how many villages exists the street X\nqresult = df.query(\"STRBEZ2L == 'Chemin des Pr\u00e9s'\")\nprint(qresult)\n\nqresult = df.query(\"STRBEZ2L == 'H\u00f6heweg'\")\nprint(qresult)\n\nqresult = df.query(\"STRBEZ2L == 'Quellgasse'\")\nprint(qresult)\n\nqresult = df.query(\"STRBEZ2L == 'Ringstrasse'\")\nprint(qresult)\n</code></pre>"},{"location":"le01/ue01-1/","title":"Aufgabe UE01-1","text":"<p>Die Post f\u00fchrt eine Datenbank mit s\u00e4mtlichen Strassen-, Weiler- und Flurbezeichnungen aller Ortschaften in der Schweiz und im F\u00fcrstentum Liechtenstein: das \u00abStrassenverzeichnis\u00bb. Dieses Verzeichnis wird regelm\u00e4ssig aktualisiert und liegt hier als csv-Datei vor. Detailinformationen und Datenstrukturen der Schweizer Strassen k\u00f6nnen dem beigelegten pdf entnommen werden.</p> <p>Im Unterricht wird dazu eine Einf\u00fchrung gegeben.</p> <p>Alle Aufgaben sollen mit Python und der beiliegenden CSV-Datei gel\u00f6st werden. F\u00fcr das Lesen/Schreiben und Bearbeiten von CSV-Dateien eignet sich die Library pandas ausgezeichnet. Ihre Aufgabe besteht aus folgenden T\u00e4tigkeiten:</p> <ol> <li>Lesen Sie die Beschreibung Anleitung Strassenverzeichnisse.pdf</li> <li>Wie viele Records sind im CSV enthalten?</li> <li>Erstellen Sie aus dem einzelnen csv-File je ein csv-File pro Record-Art. (=9 CSVs)</li> <li>Beantworten Sie mit dem erstellten CSV, welches REC_ART=1 (=NEW_PLZ1) enth\u00e4lt, folgende Fragen:<ol> <li>Wie viele verschiedene Postleitzahlen gibt es in der Schweiz und im F\u00fcrstentum Lichtenstein?</li> <li>Wie viele Gemeinden tragen die Postleitzahl 4566, 4556 und 1580 und in welchen Kantonen befinden sich diese?</li> <li>Wie viele Gemeinden haben dieselbe PLZ wie ihre Wohngemeinde?</li> </ol> </li> <li>Unter Einbezug des CSV-Files mit Record_Art=4 (NEW_STR) k\u00f6nnen Sie auch folgende Fragen beantworten:<ol> <li>In welchen Gemeinden mit Kanton existiert dieselbe Strasse ihrer Wohnadresse?</li> <li>In welchen Gemeinden gibt es ebenfalls eine \"Quellgasse\"?</li> </ol> </li> <li>Optionale Aufgabe: Welche Hausnummern gibt es an der Quellgasse in Biel/Bienne? (dazu werden zus\u00e4tzliche CSV-Dateien ben\u00f6tigt).<ol> <li>Wird die Quellgasse 12 (unser Geb\u00e4ude) mit Post bedient? Warum offenbar nicht?</li> </ol> </li> </ol> <p>Die Abgabe der Aufgabe in Form von Python-Skripts ist fakultativ. Die Handhabung von Python im Zusammenhang mit Daten ist pr\u00fcfungsrelevant. </p>"},{"location":"le01/ue01-1/#anleitung-strassenverzeichnis-der-post","title":"Anleitung Strassenverzeichnis der Post","text":"<p>AnleitungStrassenverzeichnisse.zip</p>"},{"location":"le01/ue01-1/#csv-datei-als-zip","title":"CSV-Datei als zip","text":"<p>Post_Adressdaten-20240903.zip</p>"},{"location":"le02/Loesungsvorschlaege_UE02-1/","title":"UE02-2 Aufgaben","text":"<p>Frage 1</p> <p>Wie viele Songs hat der Artist 'U2'?</p> AntwortSQL <pre><code>135\n</code></pre> <pre><code>SELECT count(*) FROM songs WHERE artist = 'U2';\n</code></pre> <p>Frage 2</p> <p>Wie teuer w\u00fcrde es uns kommen, wenn wir alle Songs vom Artist 'Kiss' kaufen w\u00fcrden?</p> AntwortSQL <pre><code>Kosten_for_all_Kiss_Songs\n-------------------------\n34.65\n</code></pre> <pre><code>SELECT SUM(price) as \"Costs for all Kiss Songs\" FROM songs WHERE artist = 'Kiss';\n</code></pre> <p>Frage 3</p> <p>Wie lange dauert es, wenn wir uns alle Songs von den \"Red Hot Chili Peppers\" anh\u00f6ren?</p> AntwortSQL <pre><code>199 Minuten (Duration ist in msec angegeben)\n</code></pre> <pre><code>SELECT SUM(duration)/1000/60 FROM songs WHERE artist = 'Red Hot Chili Peppers';\n</code></pre> <p>Frage 4</p> <p>Wie viele verschiedene Artisten sind in der Datenbank vorhanden?</p> AntwortSQL <pre><code>204\n</code></pre> <pre><code>SELECT COUNT(DISTINCT artist) FROM songs;\n</code></pre> <p>Frage 5</p> <p>Welcher Song hat den h\u00f6chsten Preis pro Sekunde?</p> AntwortSQL <pre><code>\u00c9 Uma Partida De Futebol: 0.92436974789916/sec\n</code></pre> <pre><code>SELECT song, (price/duration*1000) as Price_per_sec FROM songs ORDER BY Price_per_sec desc LIMIT 1;\n</code></pre> <p>Frage 6</p> <p>Wie viele Alben hat der Artist 'Audioslave'?</p> AntwortSQL <pre><code>3\n</code></pre> <pre><code>SELECT COUNT(DISTINCT album) FROM songs WHERE artist = 'Audioslave';\n</code></pre> <p>Frage 7</p> <p>Wie viele Artisten gibt es, welche mindestens einen Song mit dem Genre 'Metal' haben?</p> AntwortSQL <pre><code>14\n</code></pre> <pre><code>SELECT COUNT(DISTINCT artist) FROM songs WHERE genre='Metal';\n</code></pre>"},{"location":"le02/UE02-1/","title":"SELECT Abfragen mit SQLite-Browser und chinook-DB","text":""},{"location":"le02/UE02-1/#sqlite-browser","title":"SQLite-Browser","text":"<p>Please refer to the SQLite-Download-Page</p>"},{"location":"le02/UE02-1/#chinook-db","title":"Chinook-DB","text":"<p>chinook.zip</p>"},{"location":"le02/UE02-2/","title":"UE02-2","text":"<p>Frage 1</p> <p>What is the title of the album with AlbumId 67? (Table: albums)</p> AntwortSQL <pre><code>Vault: Def Leppard's Greatest Hits\n</code></pre> <pre><code>SELECT Title FROM albums WHERE AlbumId=67;\n</code></pre> <p>Frage 2</p> <p>Find the name and length (in seconds) of all tracks that have lengthbetween 50 and 70 seconds. (Table: tracks)</p> AntwortSQL <pre><code>Intro 52\nWasted Reprise 53\nArc 65\nSoldier Side - Intro 63\n.\n.\n</code></pre> <pre><code>SELECT name, Milliseconds/1000 as seconds FROM tracks WHERE seconds BETWEEN 50 AND 70\n</code></pre> <p>Frage 3</p> <p>List all artists with the word \"black\" in their name. (Table: artists)</p> AntwortSQL <pre><code>Black Label Society\nBlack Sabbath\nBanda Black Rio\nThe Black Crowes\nBlack Eyed Peas\n</code></pre> <pre><code>SELECT name FROM artists WHERE name like \"%black%\"\n</code></pre> <p>Frage 4</p> <p>Provide a query showing a unique/distinct list of billing countries FROM the Invoice table.</p> AntwortSQL <pre><code>Germany\nNorway\nBelgium\nCanada\nUSA\nFrance\n</code></pre> <pre><code>SELECT DISTINCT BillingCountry FROM invoices;\n</code></pre> <p>Frage 5</p> <p>Display the city with highest SUM total Invoice. (Table: invoices)</p> AntwortSQL <pre><code>Prague  90.24\n</code></pre> <pre><code>SELECT BillingCity,round(SUM(Total),2) as TotalInvoice FROM invoices GROUP BY BillingCity ORDER BY TotalInvoice DESC limit 1;\n</code></pre> <p>Frage 6</p> <p>Produce a table that lists each country and the number of customers in that country.   You only need to include countries that have customers in descending order. (Highest count at the top)  (Table: customers)</p> AntwortSQL <pre><code>USA 13\nCanada  8\nFrance  5\nBrazil  5\nGermany 4\nUnited Kingdom  3\n.\n.\n.\n</code></pre> <pre><code>SELECT country,count(CustomerId) as counts FROM customers GROUP BY Country having counts &gt; 0 ORDER BY counts desc;\n</code></pre> <p>Frage 7</p> <p>How many Invoices were there in 2009 and 2011? (Table: invoices)</p> AntwortSQL <pre><code>166\n</code></pre> <pre><code>SELECT COUNT(InvoiceId) as \"Total Invoices\"\nFROM Invoices\nWHERE InvoiceDate between \"2009-01-01\" AND\"2011-01-01\";\n</code></pre> <p>Frage 8</p> <p>Provide a query showing only the Employees who are engaged in Sales. (Table: employees)</p> AntwortSQL <pre><code>NancyEdwards\nJanePeacock\nMargaretPark\nSteveJohnson\n</code></pre> <pre><code>SELECT FirstName||\" \"||LastName as \"Sales Employee\"\nFROM employees\nWHERE Title LIKE \"Sales%\";\n</code></pre> <p>Frage 9</p> <p>Provide a query showing Customers (just their full names, customer ID and country) who are not in the US.  (Table: customers)</p> AntwortSQL <pre><code>1 Lu\u00eds Gon\u00e7alves Brazil\n2 Leonie K\u00f6hler Germany\n3 Fran\u00e7ois Tremblay Canada\n4 Bj\u00f8rn Hansen Norway\n5 Franti\u0161ek Wichterlov\u00e1 Czech Republic\n6 Helena Hol\u00fd Czech Republic\n.\n.\n</code></pre> <pre><code>SELECT customerid, firstname, lastname, country\nFROM customers\nWHERE not country = 'USA';\n</code></pre> <p>Frage 10</p> <p>Provide a query only showing the Customers FROM Germany.  (Table: customers)</p> AntwortSQL <pre><code>Leonie K\u00f6hler\nHannah Schneider\nFynn Zimmermann\nNiklas Schr\u00f6der\n</code></pre> <pre><code>SELECT * FROM customers\nWHERE country = 'Germany';\n</code></pre> <p>Frage 11</p> <p>Provide a query showing only the Employees who are Sales Agents.  (Table: employees)</p> AntwortSQL <pre><code>Peacock Jane\nPark Margaret\nJohnson Steve\n</code></pre> <pre><code>SELECT * FROM employees\nWHERE employees.title = 'Sales Support Agent';\n</code></pre> <p>Frage 12</p> <p>Provide a query showing a unique list of billing countries FROM the Invoice table.  (Table: invoices)</p> AntwortSQL <pre><code>Germany\nNorway\nBelgium\nCanada\nUSA\nFrance\nIreland\nUnited Kingdom\nAustralia\nChile\nIndia\nBrazil\nPortugal\nNetherlands\nSpain\nSweden\nCzech Republic\nFinland\nDenmark\nItaly\nPoland\nAustria\nHungary\nArgentina\n</code></pre> <pre><code>SELECT distinct billingcountry FROM invoices;\n</code></pre>"},{"location":"le03/UE03-01-AB/","title":"UE03-01-A-Ranking Table","text":"<p>Aufgabe</p> <p>Mit SQLite-chinook.db:</p> <p>Erstellen Sie eine Tabelle, welche jedes Land auflistet mit der Anzahl der Kunden in diesem Land. Sie sollen nur L\u00e4nder auflisten, welche mehr als einen Kunden haben. Die Liste soll in absteigender Form dargestellt werden, d.h. das Land mit den meisten Kunden steht an erster Stelle.</p> <p>Verwenden Sie dazu die SQL-Befehle GROUP BY und HAVING. Die Informationen finden Sie in der Tabelle customers.</p> Antwort  <p> Output SQLite-Browser </p> L\u00f6sungsvorschlag <pre><code>SELECT country,count(CustomerId) AS customer_count FROM customers \nGROUP BY country HAVING customer_count &gt; 1 \nORDER BY customer_count DESC;\n</code></pre>"},{"location":"le03/UE03-01-AB/#group-by-und-aggregatfunktionen-in-sql","title":"GROUP BY und Aggregatfunktionen in SQL","text":"<p>Beachte !</p> <p>Experimentieren Sie mit dem Befehl GROUP BY !</p> <p>Sie stellen fest: GROUP BY liefert nur dann sinnvolle Ergebnisse, wenn die anzuzeigenden Attribute f\u00fcr einen GROUP BY-Output geeignet sind. </p> <p>Was heisst das? </p> <p>Eine GROUP-BY-Anweisung fasst Records zusammen. Hier also alle Kunden, welche zu einem bestimmten Land geh\u00f6ren (..GROUP BY country..). Das Land wird nur 1x angezeigt also muss auch das Attribut zu diesem Land mit einem \"Wert\" angezeigt werden k\u00f6nnen, in unserem Fall also die Anzahl der Kunden. Im Zusammenhang mit GROUP BY wird also immer auch eine Aggregatsfunktion verwendet. In diesem Fall hier der <code>COUNT()</code>-Befehl, welchen wir im SELECT ... definieren. </p> <p>Beachten Sie dazu auch die Frage 5 von  UE02-2: Hier ist die Aggregatsfunktion SUM().</p> <p>AGGREGATFUNKTIONEN IN SQL:</p> Aggregatfunktion in SQL Beschreibung <code>AVG()</code>  Mittelwert, ermittelt \u00fcber alle Zeilen <code>COUNT()</code>  Anzahl aller Zeilen <code>MAX()</code>      Maximalwert aller Zeilen <code>MIN()</code>      Minimalwert aller Zeilen <code>SUM()</code>      Summenwert, summiert \u00fcber alle Zeilen <p>BEISPIEL:</p> <p>SELECT Persnr, Name, 13 * Gehalt + 1000 * (Bonuslevel) AS Jahresgehalt FROM Personal;</p> <p>SELECT SUM(12 * Gehalt +1000 * (Bonuslevel)) AS Jahrespersonalkosten FROM Personal ;</p> <p>Studieren Sie zum GROUP BY-Befehl weitere Beispiele! ChatGPT liefert Ihnen dazu gute Beispiele...</p>"},{"location":"le03/UE03-01-AB/#ue03-01-b-view-definieren","title":"UE03-01-B-View definieren","text":"<p>View definieren</p> <p>Erstellen Sie eine View dieser Ranking-Tabelle von oben mit dem Namen customer_ranking ! \u00dcberpr\u00fcfen Sie das Resultat anschliessend mit</p> <p>SELECT * FROM customer_ranking;</p> L\u00f6sungsvorschlag <pre><code>CREATE VIEW customer_ranking AS\n   SELECT country,count(CustomerId) AS customer_count FROM customers \n      GROUP BY country HAVING customer_count &gt; 1 \n      ORDER BY customer_count DESC;\n</code></pre> <p>Der Test mit</p> <pre><code>SELECT * FROM customer_ranking;\n</code></pre> <p>muss dasselbe  Resultat liefern wie</p> <pre><code>SELECT country,count(CustomerId) AS customer_count FROM customers \n   GROUP BY country HAVING customer_count &gt; 1 \n   ORDER BY customer_count DESC;\n</code></pre> <p>Result:</p> <p> Output SQLite-Browser </p>"},{"location":"le03/UE03-2/","title":"UE03-02 - Datum und Zeit","text":"<p>SQLs mit Datum und Zeiten anwenden</p> <p>Mit SQLite-chinook_v2.db:</p> <p>Die Tabelle songs_bought besteht aus zwei Spalten: Name und Zeitpunkt des Verkaufs. Schreibe Sie nun die ben\u00f6tigten Queries, wenn folgendes Abfragen erreicht werden sollen:</p> <ol> <li>Wie viele Verk\u00e4ufe gab es im Jahre 2020?</li> <li>Beim Anzeigen der Verk\u00e4ufe soll die UTC-Zeit in schweizerische Lokalzeit angezeigt werden</li> <li>Alle Verk\u00e4ufe zwischen 21:00 und 23:59 sollen angezeigt werden</li> <li>Alle Verk\u00e4ufe zwischen 21:00 und 23:59 im Jahre 2020 sollen angezeigt werden</li> </ol> <p>Hinweis: Die Tabelle songs_bought wurde neu hinzugef\u00fcgt - verwende dazu die Version chinook_v2.db.</p> Antworten <ol> <li>Wie viele Verk\u00e4ufe gab es im Jahre 2020  Output SQLite-Browser </li> <li>Beim Anzeigen der Verk\u00e4ufe soll die UTC-Zeit in Lokalzeit-Schweiz angezeigt werden  Anzeige des Datums wie in der DB abgespeichert und nun konvertiert in Schweizerzeit.   Beachte die Zeitdifferenz von einer Stunde im Februar. Warum? Februar ist Winterzeit!</li> <li>Alle Verk\u00e4ufe zwischen 21:00 und 23:59, hier sortiert nach aufsteigender Zeit:  Alle Verk\u00e4ufe zwischen 21:00:00 und 23:59:00 </li> <li>Alle Verk\u00e4ufe zwischen 21:00 und 23:59 im Jahre 2020  dito, nur f\u00fcrs Jahr 2020, sortiert datum und zeit ASC </li> </ol> L\u00f6sungsvorschl\u00e4ge <ol> <li> <p>Wie viele Verk\u00e4ufe gab es im Jahre 2020 <pre><code>SELECT count(*) FROM songs_bought WHERE bought &gt; '2020-01-01 00:00:00' and bought &lt; '2021-01-01 00:00:00'\n</code></pre></p> </li> <li> <p>Beim Anzeigen der Verk\u00e4ufe soll die UTC-Zeit in schweizerische Lokalzeit angezeigt werden <pre><code>-- show time as saved in Database\nSELECT song, datetime(bought) AS time FROM songs_bought LIMIT 5;\n</code></pre> <pre><code>-- show time converted in localtime\nSELECT song, datetime(bought,'localtime') AS time FROM songs_bought LIMIT 5;\n</code></pre></p> </li> <li> <p>Alle Verk\u00e4ufe zwischen 21:00 und 23:59 <pre><code>SELECT song, time(bought) AS zeit, date(bought) AS datum FROM songs_bought\nWHERE zeit &gt; '21:00:00' AND zeit &lt;= '23:59:00'\nORDER BY zeit ASC;\n</code></pre></p> </li> <li> <p>Alle Verk\u00e4ufe zwischen 21:00 und 23:59 im Jahre 2020 <pre><code>SELECT song, time(bought) AS zeit, date(bought) AS datum FROM songs_bought\nWHERE zeit &gt; '21:00:00' AND zeit &lt;= '23:59:00' AND datum &gt; '2020-01-01' AND datum &lt; '2021-01-01'\nORDER BY datum ASC, zeit ASC;\n</code></pre> mit extrahiertem Jahr - nur als Anschauung, wie man einen Substring in einem Attribut verwenden kann: <pre><code>SELECT song, time(bought) AS zeit, date(bought) AS datum, substr(bought,1,4) AS YEAR FROM songs_bought\nWHERE zeit &gt; '21:00:00' AND zeit &lt;= '23:59:00' AND YEAR = '2020'\nORDER BY datum ASC, zeit ASC;\n</code></pre> Jahr extrahiert vom datum </p> </li> </ol>"},{"location":"le03/UE03-3/","title":"CSV in SQLite-DB","text":"<p>\u00dcberf\u00fchrung einer CSV-Datei in eine SQLite Datenbank</p> <p>\u00dcberf\u00fchren Sie die CSV Dateien NEW_PLZ1 und NEW_STR (Aufgabe UE01-1, Punkt 3) in eine SQLite Datenbank. Verwenden Sie dazu die Import-Funktion des DB Browsers for SQLite. Benennen Sie die Spalten gem\u00e4ss Beschreibung der Post. Sie k\u00f6nnen dazu die Funktion Modify Table im DB Browser verwenden.</p> SQLite-Import-Schritte f\u00fcr CSV-Dateien <p>Jede importierte CSV-Datei kreiert automatisch eine Tabelle in SQLite. Auf diese Tabellen werden wir dann SQL-Befehle anwenden.</p> <ol> <li>Kreieren Sie eine neue Datenbank und vergeben Sie einen beliebigen Namen  . </li> <li>Die Tabellen lassen wir durch den Import generieren, daher hier Cancel klicken  . <li>Wir sehen hier, dass noch keine Tabelle f\u00fcr die DB existiert. Den DB-Namen haben Sie im vorherigen Schritt vergeben.  . </li> <li>Import der CSV-Datei, welche daraus eine Tabelle erstellt  . </li> <li>W\u00e4hlen Sie NEW_PLZ1  . </li> <li>Empfohlene Einstellungen f\u00fcr den Import  . </li> <li>Wir wollen die Tabelle modifizieren, um die Spaltennamen anzupassen  . </li> <li>Mit Hilfe der Tabellenbeschreibung der Post vergeben wir die korrekten Spaltennamen. Der Datentyp der Spalte wird meist schon korrekt vorgeschlagen. Sie sehen auch im Fenster unten das CREATE TABLE - Statement, welches nach den \u00c4nderungen im Hintergrund ausgef\u00fchrt wird.  . </li> <li>Kontrolle der \u00c4nderungen via Button \"Database Structure\"  . </li> <li>Daten wurden geladen? Kontrolle mit Button \"Browse Data\". Anschliessend sind wir bereit f\u00fcr die CSV-Datei NEW_STR  . </li> <li>Importieren Sie nun die CSV-Datei NEW_STR und modifizieren Sie die Spaltennamen gem\u00e4ss Beschreibung der Post  . </li> <li>Nach dem Import der beiden CSV-Dateien und den Anpassungen der Spaltennamen sind wir bereit, die Daten mit SQL abzufragen  . </li>"},{"location":"le03/UE03-4/","title":"SQL Abfragen Strassenverzeichnis","text":"<p>SQL Abfragen zum Strassenverzeichnis der Post</p> <p>Beantworten Sie nun die Fragen aus UE01-1 mit Hilfe von SQL-Abfragen!</p> <ol> <li>Wie viele verschiedene Postleitzahlen gibt es in der Schweiz und im F\u00fcrstentum Lichtenstein?</li> <li>Wie viele Gemeinden tragen die Postleitzahl 4566, 4556 und 1580 und in welchen Kantonen befinden sich diese?</li> <li>Wie viele Gemeinden haben dieselbe PLZ wie ihre Wohngemeinde?</li> <li>In welchen Gemeinden mit Kanton existiert dieselbe Strasse ihrer Wohnadresse?</li> <li>In welchen Gemeinden gibt es ebenfalls eine \"Quellgasse\"?</li> <li>Wird die Quellgasse 12 (unser Geb\u00e4ude) mit Post bedient? Warum offenbar nicht?</li> </ol> L\u00f6sungsvorschl\u00e4ge <ol> <li> <p>Wie viele verschiedene Postleitzahlen gibt es in der Schweiz und im F\u00fcrstentum Lichtenstein? <pre><code>-- Resultat: 3483 verschiedene PLZ in CH und Li\nSELECT COUNT(DISTINCT POSTLEITZAHL) FROM NEW_PLZ1;\n</code></pre></p> </li> <li> <p>Wie viele Gemeinden tragen die Postleitzahl 4566, 4556 und 1580 und in welchen Kantonen befinden sich diese? <pre><code>-- 1 SQL-Statement f\u00fcr Liste mit allen Ortschaften sortiert nach PLZ und Ort\nSELECT POSTLEITZAHL, ORTBEZ27, KANTON FROM NEW_PLZ1\n   WHERE POSTLEITZAHL = '4566' OR POSTLEITZAHL = '4556' OR POSTLEITZAHL = '1580'\n   ORDER BY POSTLEITZAHL ASC, ORTBEZ27 ASC;\n</code></pre> . <pre><code>-- nebenbei: hier noch ein Beispiel, das zeigt, dass sich PLZ nicht an Kantonsgrenzen halten\nSELECT POSTLEITZAHL, ORTBEZ27, KANTON FROM NEW_PLZ1\n   WHERE POSTLEITZAHL = '1595';\n</code></pre></p> </li> <li> <p>Wie viele Gemeinden haben dieselbe PLZ wie ihre Wohngemeinde? <pre><code>-- gleich wie 2 mit Ihrer PLZ.\nSELECT ...\n</code></pre></p> </li> <li> <p>In welchen Gemeinden mit Kanton existiert dieselbe Strasse ihrer Wohnadresse? <pre><code>-- Variante einfach. Das Attribut ONRP ist die Verbindung zwischen PLZ- und Strassentabelle! Siehe Beschreibung der Post.\nSELECT STRBEZ2L, ORTBEZ, KANTON FROM NEW_PLZ1, NEW_STR \n   WHERE STRBEZ2L = 'Chemin des Pr\u00e9s' AND NEW_PLZ1.ONRP = NEW_STR.ONRP;\n</code></pre> <pre><code>-- Variante mit JOIN. Wird noch behandelt...\nSELECT STRBEZ2L, ORTBEZ, KANTON FROM NEW_PLZ1 \n   JOIN NEW_STR\n   ON  NEW_PLZ1.ONRP = NEW_STR.ONRP\n      WHERE STRBEZ2L = 'Chemin des Pr\u00e9s';\n</code></pre> . </p> </li> <li> <p>In welchen Gemeinden gibt es ebenfalls eine \"Quellgasse\"? <pre><code>-- mit JOIN\nSELECT STRBEZ2L, ORTBEZ, KANTON FROM NEW_PLZ1 \n   JOIN NEW_STR\n   ON  NEW_PLZ1.ONRP = NEW_STR.ONRP\n   WHERE STRBEZ2L = 'Quellgasse';\n</code></pre> oder <pre><code>-- ohne JOIN, Resultat ist identisch\nSELECT STRBEZ2L, ORTBEZ, KANTON FROM NEW_PLZ1, NEW_STR \n   WHERE STRBEZ2L = 'Quellgasse' AND NEW_PLZ1.ONRP = NEW_STR.ONRP;\n</code></pre> da Biel offiziell 2-sprachig ist, ist die Ortsbezeichnung Biel/Bienne  Einfach noch etwas zum Staunen: die Tabelle NEW_STR enth\u00e4lt 167887 und die NEW_PLZ1 5139 Eintr\u00e4ge. Abfragen, welche beide Tabellen ber\u00fccksichtigen, dauern wenige Millisekunden. (ca 30ms) </p> </li> <li> <p>Wird die Quellgasse 12 (unser Geb\u00e4ude) mit Post bedient? Warum offenbar nicht?</p> </li> </ol> <p>Diese Frage kann mit diesen 2 Tabellen nicht beantwortet werden, weil die Hausnummern nicht enthalten sind. </p> <p>Dazu ben\u00f6tigen wir die Tabelle NEW_GEB. Hier ist Die Hausnummer \u00fcber die STRID mit den Strassennamen verbunden. Siehe Beschreibung der Post.</p> <p> . </p> <p>Aber noch zur Frage \"Warum offenbar nicht?\": Die Quellgasse 12 wird von der Post tats\u00e4chlich nicht bedient. Sie finden auch keinen Briefkasten am Eingang. Alle Post f\u00fcr die BFH an der Quellgasse in Biel wird an die Hausnummer 21 geliefert und von dort BFH-intern verteilt.</p>"},{"location":"le04/le04-einfbindings/","title":"LE04 \u2013 Einf\u00fchrung Bindings","text":"<p>Mit Binding versteht man die \u00dcbergabe von Variablenwerten in SQL-Statements. Damit k\u00f6nnen User-Eingaben in SQL-Statements \u00fcbernommen werden.</p> <p>Erstes einfaches Beispiel: Die Usereingabe des Genres, listet alle dazugeh\u00f6renden Songs aus. Als Genre sind g\u00fcltig: Latin, Metal, Rock, Jazz, Bossa Nova, Blues, etc.</p> binding1.py<pre><code>import os \nimport sqlite3 \n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04/','chinook.db')) \n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor() \n\nbind_value = input(\"Genre: \")\nprint(bind_value)\n\nquery = \"SELECT * FROM songs WHERE genre = ?;\"\n# Query mit Cursor ausf\u00fchren \ncur.execute(query, (bind_value,)) # (1)\n# alternative Schreibweise f\u00fcr single element tupel: cur.execute(query, [bind_value])\n\n\n# Bearbeite jede Zeile des Result-Sets\nfor row in cur.fetchall():  \n    print(row)\n\n# Schliesse Cursor und Verbindung\ncur.close() \nconn.close() \n</code></pre> <ol> <li>Als Eingabe wird ein Single-Element-Tupel erwartet! Python requires a comma to create a single element tuple ! Alternativ: cur.execute(query, [bind_value])</li> </ol> <p>zum Erinnern!</p> <p>Vom Befehl <code>cursor.execute()</code> werden TUPEL erwartet! Ansonsten funktioniert das Binding nicht. Wenn man also nur einen Wert als Binding hat, muss man ein <code>Single Element Tupel</code> kreieren !</p> <p>Single Element Tupel schreibt man immer mit Komma! </p> <p>Ohne Komma erh\u00e4lt man einen String.</p> <p>BEACHTEN:</p> <pre><code>&gt;&gt;&gt; type(('eins',))\n&lt;class 'tuple'&gt;\n&gt;&gt;&gt; type(('eins'))\n&lt;class 'str'&gt;\n&gt;&gt;&gt;\n</code></pre>"},{"location":"le04/le04-einfcrud/","title":"LE04 \u2013 Einf\u00fchrung CRUD","text":"<p>Sie dazu die Slides Bindings und CRUD</p> <p>Beiliegend eine zus\u00e4tzliche Ressource f\u00fcr das Anlegen von Tabellen und Daten einf\u00fcgen: </p> <p>Tutorial Create Tables und Insert Values mit Python und SQLite</p>"},{"location":"le04/le04-einfpythonsqlite/","title":"LE04 \u2013 Einf\u00fchrungs SQL und Python mit SQLite-Datenbank","text":"<p>Mit Python kann auch auf eine Datenbank zugegriffen werden. Die Kommunikation mit der Datenbank erfolgt \u00fcber eine vereinheitlichte Schnittstelle. Als Abfragesprache verwenden wir auch hier SQL, welche wir in unserem Python-Code einbetten.</p> <p>Als Standarddatenbank f\u00fcr Python gilt SQLite. Es k\u00f6nnen aber auch andere DBs wie MySQL, PostgreSQL, MS SQL Server etc. verwendet werden.</p> <p>Voraussetzung f\u00fcr das Arbeiten mit der gew\u00fcnschten DB ist die Verwendung eines passenden Python-Moduls.</p> Datenbank Python-Modul SQLite sqlite3 MySQL pymysql PostgreSQL postgresql, Psycopg Microsoft SQL Server pyodbc, pymssql"},{"location":"le04/le04-einfpythonsqlite/#einfuhrungsbeispiel-mit-sqlite-und-chinookdb","title":"Einf\u00fchrungsbeispiel mit SQLite und chinook.db","text":"<p>Zeige alle Songs, welche zum Album Facelift geh\u00f6ren:</p> HelloDB.py<pre><code>import os # (8)\nimport sqlite3 # (1)\n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04/','chinook.db')) # (2)\n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor() # (3)\n\n# Query mit Cursor ausf\u00fchren \ncur.execute('SELECT * FROM songs WHERE album = \\'Facelift\\';') # (4)\n\n# Bearbeite jede Zeile des Result-Sets\nfor row in cur.fetchall():  # (5)\n    print(row)\n\n# Schliesse Cursor und Verbindung\ncur.close() # (6)\nconn.close() # (7)\n</code></pre> <ol> <li> Damit mit SQLite gearbeitet werden kann, muss das Modul <code>sqlite3</code> importiert werden</li> <li> Aufbau der Datenbankverbindung mit <code>connect</code>-Funktion. Diese gibt ein <code>Connection</code>-Objekt zur\u00fcck. Hier mit dem Namen <code>connection</code>. Wenn es die Datei <code>chinook.db</code> in diesem Verzeichnis nicht gibt, wird eine leere Datenbank mit diesem Namen erstellt.</li> <li> Um mit der verbundenen Datenbank zu arbeiten, werden Cursors (Positionszeiger) verwendet. Mit Cursors k\u00f6nnen wir Datens\u00e4tze ver\u00e4ndern oder abfragen. Eine Datenbankverbindung kann beliebig viel Cursors haben. Ein neuer Cursor wird mit der <code>cursor</code>-Methode des <code>Connection</code>-Objektes erzeugt.</li> <li> Das SQL-Statement wird mir der <code>execute</code>-Methode des <code>Cursor</code>-Objektes an die SQLite-Datenbank gesendet.</li> <li> Das Result-Set der Abfrage wird mit <code>cursor.fetchall</code> zur\u00fcckgegeben. Der R\u00fcckgabewert von <code>fetchall</code> ist eine Liste, die f\u00fcr jeden Datensatz ein Tupel mit den Werten der angeforderten Spalten enth\u00e4lt.</li> <li> Close the cursor</li> <li> Close the connection</li> <li> wird ben\u00f6tigt f\u00fcr <code>os.path.join()</code></li> </ol> <p>Wenn sie mit <code>SELECT * ...</code> abfragen, richtet sich die Reihenfolge der Spaltenwerte nach der Reihenfolge, in welcher die Spalten mit <code>CREATE</code> definiert wurden.</p> <p>Mit <code>fetchall</code> werden immer alle Ergebnisse einer Abfrage auf einmal aus der Datenbank geladen und dann gesammelt ausgegeben. Diese Methode eignet sich allerdings nur f\u00fcr relativ kleine Datenmengen, da erstens das Programm so lange warten muss, bis die Datenbank alle Ergebnisse ermittelt und zur\u00fcckgegeben hat, und zweitens das Resultat komplett als Liste im Speicher gehalten wird. F\u00fcr Operationen wie Bildschirmausgaben, die jeweils nur einen einzelnen Datensatz ben\u00f6tigen, ist dies bei sehr umfangreichen Ergebnissen eine Verschwendung von Arbeitsspeicher. Aus diesem Grund gibt es die M\u00f6glichkeit, die Daten zeilenweise, also immer in kleinen Portionen, abzufragen. Sie erreichen durch dieses Vorgehen, dass Sie nicht mehr auf die Berechnung der kompletten Ergebnismenge warten m\u00fcssen, sondern schon w\u00e4hrenddessen mit der Verarbeitung beginnen k\u00f6nnen. Ausserdem kann so der Arbeitsspeicher effizienter genutzt werden.</p> <p>Mit der Methode <code>fetchone</code> der <code>cursor</code>-Klasse fordern wir jeweils ein Ergebnis-Tupel an. Wurden bereits alle Datens\u00e4tze der letzten Abfrage ausgelesen, gibt <code>fetchone</code> den Wert <code>None</code> zur\u00fcck. Damit lassen sich auch grosse Datenmengen speichereffizient auslesen. Ersetzen Sie Zeilen 14 und 15 mit:</p> Variante mit fetchone<pre><code>while row:= cur.fetchone():\n    print(row)\n</code></pre>"},{"location":"le04/le04-einfpythonsqlite/#datentypen-bei-sqlite","title":"Datentypen bei SQLite","text":"<p>Die automatische Datentypumwandlung von Daten aus SQLite in Python-Datentypen richten sich nach folgender Tabelle:</p> SQLite-Datentyp als Quelle Python-Datentyp als Ziel NULL None INTEGER int REAL float TEXT str BLOB bytes <p>Frage</p> <p>Wie k\u00f6nnen wir in den R\u00fcck\u00fcbersetzungsprozess eingreifen, um Daten beim Auslesen aus der Datenbank unseren Vorstellungen entsprechend anzupassen? Diese Frage werden wir mit den Factory-Funktionen beantworten.</p>"},{"location":"le04/le04-einfpythonsqlite/#factory-funktionen-mit-python-und-sqlite","title":"Factory-Funktionen mit Python und SQLite","text":"<p>Es kann sein, dass die Daten in der Datenbank nicht in der Form vorliegen, wie gew\u00fcnscht. Deshalb kann man diese Rohdaten in der Datenbank in einer \"Factory\" mit Python aufbereiten. Wir schauen uns dazu zwei Factories an.</p>"},{"location":"le04/le04-einfpythonsqlite/#connectiontext_factory","title":"Connection.text_factory","text":"<p>Hier nehmen wir als Beispiel, dass wir die Daten alle in Grossbuchstaben erhalten wollen. Es sollen also alle Werte, welche in der Datenbank als TEXT abgespeichert sind in Grossbuchstaben umgewandelt werden.</p> <p>Jede von <code>sqlite3.connect</code> erzeugte Connection-Instanz hat ein Attribut <code>text_factory</code>, das eine Referenz auf eine Funktion enth\u00e4lt, die immer dann aufgerufen wird, wenn <code>TEXT</code>-Spalten ausgelesen werden. Im Ergebnis-Tupel der Datenbankabfrage steht dann der R\u00fcckgabewert dieser Funktion. Standardm\u00e4ssig ist das <code>text_factory</code>-Attribut auf die Built-in Function <code>str</code> gesetzt. Dies k\u00f6nnen Sie sich durch Python mit Zeilenkommandos anzeigen lassen:</p> <pre><code>&gt;&gt;&gt; import sqlite3\n&gt;&gt;&gt; conn=sqlite3.connect(\"chinnook.db\")\n&gt;&gt;&gt; conn.text_factory\n&lt;class 'str'&gt;\n</code></pre> <p>In unserer Beispielabfrage aller Songs eines Albums erstellen wir nun eine <code>text_factory</code>-Funktion, welche alle TEXT-Spalten-Werte in Grossbuchstaben umwandelt. Diese Funktion muss einen Parameter erwarten und den konvertierten Wert zur\u00fcckgeben. Der Parameter ist ein byte-String, der die Rohdaten aus der Datenbank mit UTF-8-codiert enth\u00e4lt. Die Funktion muss den ausgelesenen Wert erst in einen String umwandelen  und dann mit der <code>upper</code>-Methode alle Buchstaben zu Grossbuchstaben umwandeln.</p> text_factory.py<pre><code>import os\nimport sqlite3\n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04/','chinook.db'))\n\ndef my_text_factory_BIGLETTERS(value):\n    return value.decode(\"utf-8\", \"ignore\").upper()\n    # Alternativ: return str(value.upper(),encoding=\"utf-8\")\n\nconn.text_factory = my_text_factory_BIGLETTERS\n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor()\n\n# Query mit Cursor ausf\u00fchren \ncur.execute('SELECT * FROM songs WHERE album = \\'Facelift\\';')\n\n# Bearbeite jede Zeile des Result-Sets\nfor row in cur.fetchall():\n    print(row)\n\n# Schliesse Cursor und Verbindung\ncur.close()\nconn.close()\n</code></pre> <p>Damit Sie den Unterschied in der Ausgabe sch\u00f6n sehen und das urspr\u00fcngliche Ausgabeverhalten wieder herstellen k\u00f6nnen, setzen Sie in Zeile 11</p> <pre><code>conn.text_factory = str\n</code></pre>"},{"location":"le04/le04-einfpythonsqlite/#connectionrow_factory","title":"Connection.row_factory","text":""},{"location":"le04/le04-einfpythonsqlite/#row-factory-mit-eigener-funktion","title":"Row-Factory mit eigener Funktion","text":"<p>Ein \u00e4hnliches Attribut wie <code>text_factory</code> f\u00fcr <code>TEXT</code>-Spalten existiert auch f\u00fcr ganze Datens\u00e4tze. In dem Attribut <code>row_factory</code> kann eine Referenz auf eine Funktion gespeichert werden, die Zeilen f\u00fcr das Benutzerprogramm aufbereitet. Standardm\u00e4ssig wird die Funktion <code>tuple</code> benutzt. Wir wollen beispielhaft eine Funktion implementieren, die uns auf die Spaltenwerte eines Datensatzes \u00fcber die Namen der jeweiligen Spalten zugreifen l\u00e4sst. Das Ergebnis soll folgendermassen aussehen:</p> <pre><code>{'song': 'We Die Young', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 152084, 'price': 0.99}\n{'song': 'Man In The Box', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 286641, 'price': 0.99}\n{'song': 'Sea Of Sorrow', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 349831, 'price': 0.99}\n{'song': 'Bleed The Freak', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 241946, 'price': 0.99}\n{'song': \"I Can't Remember\", 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 222955, 'price': 0.99}\n{'song': 'Love, Hate, Love', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 387134, 'price': 0.99}\n{'song': \"It Ain't Like That\", 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 277577, 'price': 0.99}\n{'song': 'Sunshine', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 284969, 'price': 0.99}\n{'song': 'Put You Down', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 196231, 'price': 0.99}\n{'song': 'Confusion', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 344163, 'price': 0.99}\n{'song': 'I Know Somethin (Bout You)', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 261955, 'price': 0.99}\n{'song': 'Real Thing', 'album': 'Facelift', 'artist': 'Alice In Chains', 'genre': 'Rock', 'duration': 243879, 'price': 0.99}\n</code></pre> <p>Um dies zu erreichen, ben\u00f6tigen wir das Attribut <code>description</code> der <code>Cursor</code>-Klasse, das uns Informationen zu den Spaltennamen der letzten Abfrage liefert. Das Attribut <code>description</code> enth\u00e4lt dabei eine Sequenz, die f\u00fcr jede Spalte ein Tupel mit 7 Elementen bereitstellt, von denen uns aber nur das erste, n\u00e4mlich der Spaltenname, interessiert:</p> <pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; import sqlite3\n&gt;&gt;&gt; conn = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04/','chinook.db'))\n&gt;&gt;&gt; cur = conn.cursor()\n&gt;&gt;&gt; cur.execute('SELECT * FROM songs WHERE album = \\'Facelift\\';')\n&lt;sqlite3.Cursor object at 0x000002443B320AC0&gt;\n&gt;&gt;&gt; cur.description\n(('song', None, None, None, None, None, None), ('album', None, None, None, None, None, None), ('artist', None, None, None, None, None, None), ('genre', None, None, None, None, None, None), ('duration', None, None, None, None, None, None), ('price', None, None, None, None, None, None))\n&gt;&gt;&gt;\n</code></pre> <p>Mit dieser Information k\u00f6nnen wir nun die Spaltennamen in der Ausgabe aufbereiten,  indem wir eine Funktion als eigene Factory verwenden, hier <code>row_with_column_name()</code>.</p> row_factory.py<pre><code>import os\nimport sqlite3\n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04/','chinook.db'))\n\ndef row_with_column_name(cursor, zeile):\n    r = {}\n    for spaltennr, spalte in enumerate(cursor.description):\n        r[spalte[0]] = zeile[spaltennr]\n    return r\n\nconn.row_factory = row_with_column_name\ncur = conn.cursor()\n\n# Query mit Cursor ausf\u00fchren \ncur.execute('SELECT * FROM songs WHERE album = \\'Facelift\\';')\n\nfor row in cur.fetchall():\n    print(row)\n\n# Schliesse Cursor und Verbindung\ncur.close()\nconn.close()\n</code></pre> <p>Hinweis</p> <p><code>enumerate</code> erzeugt einen Iterator, der f\u00fcr jedes Element der \u00fcbergebenen  Sequenz ein Tupel zur\u00fcckgibt, das den Index des Elements in der Sequenz und seinen Wert enth\u00e4lt. Siehe</p>"},{"location":"le04/le04-einfpythonsqlite/#row-factory-mit-eingebauter-funktion-sqlite3row","title":"Row-Factory mit eingebauter Funktion sqlite3.Row","text":"<p>Die eingebaute Funktion <code>sqlite3.Row</code> erlaubt es auf Spalten zuzugreifen via Namen oder numerischen Index. Die sqlite3.Row-Factory-Funktion sollte einer eigenen Funktion vorgezogen werden, es sei denn, Sie wollen etwas ganz Spezielles erreichen.</p> import_pandas.py<pre><code>import os\nimport sqlite3\n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04','chinook.db'))\n\nconn.row_factory = sqlite3.Row # sqlite3.Row ist eine interne Factory\ncur = conn.cursor()\n\n# Query mit Cursor ausf\u00fchren \ncur.execute('SELECT * FROM songs WHERE album = \\'Facelift\\';')\n\nfor row in cur.fetchall():\n# Zugriff mit Spaltennamen oder via numerischen Index. Die Spalte artist hat index=2, daher doppelte Ausgabe.\n    print(row['artist'],row[2])\n    print(type(row))  # Ausgabe des Klasse\n\n# Schliesse Cursor und Verbindung\ncur.close()\nconn.close()\n</code></pre>"},{"location":"le04/le04-einfpythonsqlite/#import-data-aus-sqlite-in-pandas-dataframe","title":"Import Data aus SQLite in Pandas-Dataframe","text":"<p>Hier ein Beispiel, wie man Daten aus einer SQLite-Datenbank in ein Pandas-Dataframe <code>df</code> importiert. Beachten Sie hier auch die Anwendung, wie man SQL-Statements als Variable einsetzen kann. Auch ein Cursor ist dank der Pandas-Funktion <code>read_sql()</code> nicht notwendig.</p> <p>Die Bearbeitung des Datenframes kann dann auf gewohnte Art und Weise erfolgen.</p> row_factory.py<pre><code>import os\nimport sqlite3\nimport pandas as pd\n\n# Datenbank \u00f6ffnen und verbinden\nconnection = sqlite3.connect(os.path.join('C:/Users/tom/WORKLOCAL/PythonProj/UE04/','chinook.db'))\nsqlquery = \"SELECT * FROM employees;\"\ndf = pd.read_sql(sqlquery,con=connection) # (1)\nprint(df)\n# Schliesse Cursor und Verbindung \nconnection.close()\n</code></pre> <ol> <li> read_sql() ist eine Funktion (Methode) von Pandas, welche als Argument das SQL-Statement und die DB-Verbindung ben\u00f6tigt.</li> </ol>"},{"location":"le04/ue04-01/","title":"UE04-01 \u00dcbungen Dynamische Queries","text":"<p>Dynamische Queries mit Python und SQLite</p> <p>Der Auftrag ist es ein Script zu schreiben, welches aufgrund der fiktiven Eingaben eines Benutzers eine Selektierung macht. Der Benutzer macht eine beliebige Selektion von Alben (also mehrere auf einmal) und es soll sowohl die Laufzeiten pro Album aufgelistet werden, als auch das Total der Alben kombiniert. Datenbank: <code>chinook.db</code></p> <p>Das ganze soll mit Queries passieren, welche in Abh\u00e4ngigkeit folgender Liste generiert werden.</p> <p><code>album_list = ['Brave New World', 'Presence', 'ReLoad', 'Ten', 'Dark Side Of The Moon']</code></p> <p>Das Resultat k\u00f6nnte wie folgt aussehen: <pre><code>Select from following albums:\nNr: 0 Brave New World\nNr: 1 Presence\nNr: 2 ReLoad\nNr: 3 Ten\nNr: 4 Dark Side Of The Moon\nAlbum Nr? (or just RETURN to finish):2\nAlbum Nr? (or just RETURN to finish):4\nAlbum Nr? (or just RETURN to finish):0\nAlbum Nr? (or just RETURN to finish):1\nAlbum Nr? (or just RETURN to finish):3\nAlbum Nr? (or just RETURN to finish):\nYour Selection:\n===============\nNr  2 - ReLoad : 76 Minuten\nNr  4 - Dark Side Of The Moon : 42 Minuten\nNr  0 - Brave New World : 67 Minuten\nNr  1 - Presence : 44 Minuten\nNr  3 - Ten : 53 Minuten\nTOTAL:  282 Minuten\n</code></pre></p> L\u00f6sungsvorschlag <pre><code>\"\"\"\nFile:        ue04-01.py\nAuthor:      Thomas J\u00e4ggi\nDate:        16.10.2024\nDescription: List all selected Albums with duration of songs and calculate the total duration\n             of all songs of all selected albums.\n\"\"\"\n\nimport os # just used for Function os.path.join() \nimport sqlite3 # Module needed for connecting to SQLite-DB    \n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('/home/tom/work/bfh-btw2201/PythonProj/UE04/','chinook.db'))     \n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor()     \n\n# Initializing an empty list to store Numbers of selected Albums by the User\nuser_list = []    \n\n# List of selectable Albums. Can be extended, Program still works.\nalbum_list = ['Brave New World', 'Presence', 'ReLoad','Ten', 'Dark Side Of The Moon']    \n\n# Print selectable Albums on screen\nindex1 = 0\nprint(\"Select from following albums:\")\nwhile index1 &lt; len(album_list):\n    print(\"Nr:\",index1, album_list[index1])   \n    index1 += 1    \n\n# Prompting the user to enter numbers. RETURN=selection over\nwhile True:\n    num = input(\"Album Nr? (or just RETURN to finish):\")\n    if num == \"\":\n        break\n    user_list.append(int(num))   #append selected number into user_list    \n\n# Query to find the duration of all songs of a specified album.\n# Convert ms into Minutes!\nquery = \"SELECT SUM(duration)/1000/60 FROM songs WHERE album = ?;\"    \n\n# mintot = Variable to add Minutes to in order to calculate TOTAL Minutes of selected Albums\nmintot = 0\n# index2 is the index of the user-selected album-list\nindex2 = 0    \n\n# Iterate through album_list with index of user_list.\n# This method returns the selected album title.\n# Assign album title to variable bind_value und use it in query.\n# bind_variable must be a single element tuple. dont forget ,\nprint(\"Your Selection:\")\nprint(\"===============\")\nwhile index2 &lt; len(user_list):\n    bind_value = album_list[user_list[index2]]\n    cur.execute(query, (bind_value,))\n# here treat row by row and add minutes of each row (=song) to variable mintot.\n    for row in cur.fetchone():\n        # aggregate duration of songs into variable mintot \n        mintot = row + mintot\n        # Print Information in a formated way\n        print(\"Nr \",user_list[index2],\"-\",album_list[user_list[index2]],\":\", row, \"Minuten\")        \n\n    index2 += 1\n# Outside the loop through songs of selected albums we get the aggregated duration in minutes\n# in variable mintot\nprint(\"TOTAL: \", mintot, \"Minuten\")\n# Schliesse Cursor und Verbindung\ncur.close() \nconn.close() \n</code></pre>"},{"location":"le04/ue04-02/","title":"UE04-02 Suche nach Artist","text":"<p>UE04-02-Suche nach Artist</p> <p>Schreiben Sie ein Script, welches nach einem beliebigen Artisten fragt. Dazu k\u00f6nnen Sie die Funktion input() verwenden. Sobald Sie den Artist eingelesen haben, holen Sie die Anzahl von Songs zu diesem Artist.  ACHTUNG: Ein Song kann auf mehreren Alben erscheinen. Es soll die Anzahl unterschiedlicher Songs pro Band ausgegeben werden.  </p> <p>Versuchen Sie auch Eingaben flexibel zu handhaben. Wenn der User nur \"a\" eingibt, sollen alle Bandnamen, die mit \"a\" beginnen, angezeigt werden.</p> <p>Das Resultat k\u00f6nnte wie folgt aussehen:</p> <pre><code>Artist? ac\nAC/DC 18\nAcademy of St. Martin in the Fields &amp; Sir Neville Marriner 2\nAcademy of St. Martin in the Fields Chamber Ensemble &amp; Sir Neville Marriner 1\nAcademy of St. Martin in the Fields, John Birch, Sir Neville Marriner &amp; Sylvia McNair 1\nAcademy of St. Martin in the Fields, Sir Neville Marriner &amp; Thurston Dart 1\nAccept 4\n</code></pre> <pre><code>Artist? u2\nU2 124\n</code></pre> <pre><code>Artist? ba\nBackBeat 12\nBarry Wordsworth &amp; BBC Concert Orchestra 1\nBattlestar Galactica 20\nBattlestar Galactica (Classic) 24\n</code></pre> <pre><code>Artist? xyz\nLeider kein Resultat!\n</code></pre> L\u00f6sungsvorschlag <pre><code>\"\"\"\nFile: ue04-02.py\nAuthor: Thomas J\u00e4ggi\nDate: 16.10.2024\nDescription: List Number of different songs from selected band (=artist). Inputs for artists can be done as Substrings.\n             E.g.: Input of \"a\" results in all Bands beginning with \"a\", input \"ac\" results in all Bandnames beginning with \"ac\".\n\"\"\"\nimport os # just used for Function os.path.join() \nimport sqlite3 # Module needed for connecting to SQLite-DB    \n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('/home/tom/work/bfh-btw2201/PythonProj/UE04/','chinook.db'))     \n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor()     \n\n# Prompting the user to enter artist.    \n\nband = input(\"Artist? \")    \n\n# Query to find nr of different songs of selected band. Notice the LIKE statement.\n# The Input will be treated as Eingabe% in cur.execute() below. Meaning: \n# Beispiel: Input \"ac\" will get all artist berginning with ac, also Bands like  AC/DC, Accept, etc\n# Input like \"a\" will produce a list of all Bands beginning with A     \n\nquery = \"SELECT artist, count(DISTINCT song)  FROM songs WHERE artist LIKE ? GROUP BY artist;\"    \n\n# Hinweis: Um alle Bandnamen mit A.. mit der Anzahl songs abzufragen, w\u00fcrden Sie im SQLite Browser folgendes Statement verwenden:\n# SELECT artist, count(DISTINCT song) FROM songs WHERE artist LIKE 'a%' GROUP BY artist;\n# In Python wird das Binding-Argument mit ? markiert. Siehe query-Variable oben.\n# Im cursor.execute() wird das ? mit der formatierten Bind-Variable definert. In diesem Fall also die User-Eingabe gefolgt von %.\n# Nur so erreichen wir, dass der cursor.execute()-Befehl das SQL-Statement so aufbereitet, wie wir das im SQLite-Browser direkt\n# eingeben w\u00fcrden.\n# Das Konstrukt f\"{variable}%\" kreiert den Wert Variable+Prozentzeichen. Python-Thema: Formatierung von Ausdr\u00fccken in String-Literals.\n# ... und das ganze als Single-Element-Tupel (mit Komma)   \n# Siehe dazu:\n# https://docs.python.org/3/library/sqlite3.html#how-to-use-placeholders-to-bind-values-in-sql-queries\n#\n\n#qresult ist immer vom typ &lt;class 'sqlite3.Cursor'&gt;.\nqresult = cur.execute(query, (f\"{band}%\",))    \n\n# Das Resultat von fetchall() geht in die fetchresult-Liste. Typ ist &lt;class 'list'&gt;. \n# Wenn dieses leer ist, d.h.[], wurde kein Resultat gefunden.\n# \nfetchresult = qresult.fetchall()    \n\nfor row in fetchresult:\n    print(row[0],row[1])    \n\n# In this code, the condition if not fetchresult: checks if fetchresult is empty: [] \n# If it is [], the code will print \"Leider kein Resultat!\"\nif not fetchresult:\n    print(\"Leider kein Resultat!\")\n\ncur.close() \nconn.close()\n</code></pre>"},{"location":"le04/ue04-03/","title":"UE04-03 Alle Artisten","text":"<p>UE04-03-Alle Artisten</p> <p>Schreibe ein Script, dass in einem ersten Schritte alle einzelnen Artisten abfr\u00e4gt. Dies liefert eine Liste innerhalb von Python, womite das Total an Kosten f\u00fcr alle Songs des jeweiligen Artisten ausgerechnet werden kann. Zudem interessiert wie viele Songs der jeweilige Artist hat.</p> <p>Hinweis: Das escaping von Strings muss strikte ber\u00fcckstichtigt werden! Beispiel: Guns N' Roses</p> <p>Als Beispiel hier ein Auszug:</p> <p><pre><code>Select from following Artists:\nNr: 0 Guns N' Roses\nNr: 1 AC/DC\nNr: 2 Aerosmith\nNr: 3 Alanis Morissette\nNr: 4 Ant\u00f4nio Carlos Jobim\nNr: 5 The Black Crowes\nNr: 6 The Clash\nNr: 7 The Cult\nNr: 8 The Doors\nNr: 9 Tit\u00e3s\nNr: 10 The Rolling Stones\nNr: 11 The Police\nNr: 12 The Tea Party\nNr: 13 The Who\nNr: 14 Tim Maia\nNr: 15 Battlestar Galactica\nNr: 16 Apocalyptica\n</code></pre> User Input: <pre><code>Select Artist-Nr: (or just RETURN to finish):0\nSelect Artist-Nr: (or just RETURN to finish):1\nSelect Artist-Nr: (or just RETURN to finish):10\nSelect Artist-Nr: (or just RETURN to finish):4\nSelect Artist-Nr: (or just RETURN to finish):9\nSelect Artist-Nr: (or just RETURN to finish):\n</code></pre> Confirm Selection and print information: <pre><code>Your Selection:\n===============\nNr:  0 Guns N' Roses $ 41.58 42  Songs\nNr:  1 AC/DC $ 17.82 18  Songs\nNr:  10 The Rolling Stones $ 40.59 40  Songs\nNr:  4 Ant\u00f4nio Carlos Jobim $ 30.69 30  Songs\nNr:  9 Tit\u00e3s $ 37.62 38  Songs\n</code></pre></p> L\u00f6sungsvorschlag <pre><code>\"\"\"\nFile: ue04-03.py\nAuthor: Thomas J\u00e4ggi\nDate: 16.10.2024\nDescription: Schreibe ein Script, dass in einem ersten Schritte alle einzelnen Artisten abfr\u00e4gt. Dies liefert eine Liste innerhalb von Python, \nwomit das Total an Kosten f\u00fcr alle Songs des jeweiligen Artisten ausgerechnet werden kann. Zudem interessiert und wie viele Songs der jeweilige Artist hat.\nHinweis: Das escaping von Strings muss strikte ber\u00fcckstichtigt werden! Beispiel: Guns N' Roses\n\"\"\"\nimport os # just used for Function os.path.join() \nimport sqlite3 # Module needed for connecting to SQLite-DB    \n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('/home/tom/work/bfh-btw2201/PythonProj/UE04/','chinook.db'))     \n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor()     \n\n# Initializing an empty list to store Numbers of selected Artists by the User\nuser_list = []    \n\n# List of selectable Artists. Can be extended, Program still works. Escape bei Guns N' Roses\nartist_list = ['Guns N\\' Roses', 'AC/DC', 'Aerosmith', 'Alanis Morissette','Ant\u00f4nio Carlos Jobim',\n              'The Black Crowes','The Clash','The Cult','The Doors','Tit\u00e3s', 'The Rolling Stones',\n              'The Police', 'The Tea Party','The Who','Tim Maia','Battlestar Galactica', 'Apocalyptica']    \n\n# Print selectable Artists on screen\nindex1 = 0\nprint(\"Select from following Artists:\")\nwhile index1 &lt; len(artist_list):\n    print(\"Nr:\",index1, artist_list[index1])   \n    index1 += 1    \n\n# Prompting the user to enter number of artist. RETURN=selection over\nwhile True:\n    num = input(\"Select Artist-Nr: (or just RETURN to finish):\")\n    if num == \"\":\n        break\n    user_list.append(int(num))   #append selected number into user_list    \n\n# Query\nquery = \"SELECT artist, SUM(price), count(DISTINCT song) FROM songs WHERE artist = ? GROUP BY artist;\"    \n\n# index2 is the index of the user-selected artist-list\nindex2 = 0    \n\n# Iterate through artist_list with index of user_list.\n# This method returns the selected artist.\n# Assign artist to variable bind_value und use it in query.\n# bind_variable must be a single element tuple. dont forget ,\nprint(\"Your Selection:\")\nprint(\"===============\")\nwhile index2 &lt; len(user_list):\n    bind_value = artist_list[user_list[index2]]\n    cur.execute(query, (bind_value,))    \n\n    for row in cur.fetchall():\n        # print(row)\n        # print(type(row))\n        costs=round(row[1],2)  # Kosten Runden auf 2 Stallen nach dem Komma\n        #print(row[0],\"$\", row[1], row[2], \" Songs\")\n        print(\"Nr: \",user_list[index2],row[0],\"$\", costs, row[2], \" Songs\")\n\n    index2 += 1    \n\n# Schliesse Cursor und Verbindung\ncur.close() \nconn.close() \n</code></pre>"},{"location":"le04/ue04-04/","title":"UE04-04 \u00c4nderungen","text":"<p>UE04-04-\u00c4nderungen</p> <p>In der Datenbank <code>small.db</code> gibt es einige \u00c4nderungen vorzunehmen. Schreibe dazu ein Script, welches diese \u00c4nderungen automatisiert vornimmt. \u00c4nderungen k\u00f6nnen mit dem SQLiteBrowser verfiziert werden.</p> <p>Falls etwas schief l\u00e4uft und irreparable Sch\u00e4den angerichtet wurden, kann die Datenbank einfach erneut heruntergeladen werden.</p> <ol> <li>Der Artist <code>Metallica</code> f\u00fcgt einen weiteren Song zum Album <code>ReLoad</code> hinzu, namens <code>Fuel 2</code>. Der Song hat ausser dem Namen, die gleichen Informationen wie <code>Fuel</code>.</li> <li>Beim Album <code>Dark Side of the Moon</code> kosten alle Songs neuerdings <code>2.99</code>.</li> <li>Aus rechtlichen Gr\u00fcnden will <code>Pearl Jam</code> seine Musik nicht mehr anbieten, entsprechend sind alle Songs zu entfernen.</li> <li>Alle Songs die l\u00e4nger als 4 Minuten dauern, sollen per sofort 1.99 kosten.</li> <li>Alle Songs die k\u00fcrzer als 2 Minuten dauern, sollen nur noch 0.49 kosten.</li> </ol> <p>Download small.db</p>"},{"location":"le04/ue04-04/#losungsvorschlage","title":"L\u00f6sungsvorschl\u00e4ge","text":"<p>\u00c4nderungen mit puren SQL-Commands</p> \u00c4nderung 1\u00c4nderung 2\u00c4nderung 3\u00c4nderung 4\u00c4nderung 5 <pre><code>--1.Der Artist Metallica f\u00fcgt einen weiteren Song zum Album ReLoad hinzu, namens Fuel 2. \n--  Der Song hat ausser dem Namen, die gleichen Informationen wie Fuel.\n\n    -- Get Info about artist and songs\n    SELECT * FROM songs WHERE artist=\"Metallica\" AND song=\"Fuel\";\n\n    -- Generic Syntax of INSERT INTO:\n    --                   INSERT INTO table_name (column1, column2, column3, ...)\n    --                   VALUES (value1, value2, value3, ...);\n\n    INSERT INTO songs (song,album,artist,genre,duration,price) VALUES ('Fuel 2','ReLoad','Metallica','Metal',269557,0.99);\n\n    -- CHECK\n    SELECT * FROM songs WHERE artist=\"Metallica\" AND song LIKE \"Fuel%\";\n</code></pre> <pre><code>--2. Beim Album Dark Side of the Moon kosten alle Songs neuerdings 2.99.\n-- Get Infos about album\n   SELECT * FROM songs WHERE album = \"Dark Side Of The Moon\"\n\n-- Generic Syntax of UPDATE:\n--                   UPDATE table_name\n--                   SET column1 = value1, column2 = value2, ...\n--                   WHERE condition;\nUPDATE songs set price = 2.99 WHERE album = \"Dark Side Of The Moon\"\n\n-- CHECK:\nSELECT * FROM songs WHERE album = \"Dark Side Of The Moon\"\n</code></pre> <pre><code>--3. Aus rechtlichen Gr\u00fcnden will Pearl Jam seine Musik nicht mehr anbieten, entsprechend sind alle Songs zu entfernen.\n\n-- Get Info about Pearl Jam\nSELECT * FROM songs WHERE artist LIKE \"Pearl%\";  -- 11 Records found\n\n-- Generic Syntax DELETE FROM:\n--                DELETE FROM table_name\n--                WHERE condition;\nDELETE FROM songs WHERE artist = \"Pearl Jam\"\n\n-- CHECK:\nSELECT * FROM songs WHERE artist LIKE \"Pearl%\";  --0 Records found\n</code></pre> <pre><code>--4. Alle Songs die l\u00e4nger als 4 Minuten dauern, sollen per sofort 1.99 kosten.\n\n-- Get Infos . 4 Min = 240000ms\nSELECT * FROM songs WHERE duration &gt; 240000;   -- 34 Records\n\n-- Generic Syntax of UPDATE\n--                   UPDATE table_name\n--                   SET column1 = value1, column2 = value2, ...\n--                   WHERE condition;\nUPDATE songs SET price = 1.99 WHERE duration &gt; 240000;\n\n-- CHECK:\nSELECT * FROM songs WHERE duration &gt; 240000; --34 Rows have price 1.99\n</code></pre> <pre><code>--5. Alle Songs die k\u00fcrzer als  2 Minuten dauern, sollen nur noch 0.49 kosten.\n\n-- Check Records\nSELECT * FROM songs ORDER BY duration ASC; -- Es gibt keine songs k\u00fcrzer als 2 Minuten\n\nUPDATE songs SET price = 0.49 WHERE duration &lt; 120000;  --0 rows affected\n</code></pre> \u00c4nderungen automatisiert mit Skript <p>Eigene Skript-L\u00f6sung soll mit den Resultaten von oben kontrolliert werden. Melden sie sich bei Fragen!</p> <pre><code>\"\"\"\n--1.Der Artist Metallica f\u00fcgt einen weiteren Song zum Album ReLoad hinzu, namens Fuel 2. \n--  Der Song hat ausser dem Namen, die gleichen Informationen wie Fuel.\n\n        -- Get Info about artist and songs\n        SELECT * FROM songs WHERE artist=\"Metallica\" AND song=\"Fuel\";\n\n        -- Generic Syntax of INSERT INTO:\n        --                   INSERT INTO table_name (column1, column2, column3, ...)\n        --                   VALUES (value1, value2, value3, ...);\n\nINSERT INTO songs (song,album,artist,genre,duration,price) VALUES ('Fuel 2','ReLoad','Metallica','Metal',269557,0.99);\n\n        -- CHECK\n        SELECT * FROM songs WHERE artist=\"Metallica\" AND song LIKE \"Fuel%\";\n\n\n\n--2. Beim Album Dark Side of the Moon kosten alle Songs neuerdings 2.99.\n    -- Get Infos about album\n    SELECT * FROM songs WHERE album = \"Dark Side Of The Moon\"\n\n    -- Generic Syntax of UPDATE:\n    --                   UPDATE table_name\n    --                   SET column1 = value1, column2 = value2, ...\n    --                   WHERE condition;\nUPDATE songs set price = 2.99 WHERE album = \"Dark Side Of The Moon\"\n\n    -- CHECK:\n    SELECT * FROM songs WHERE album = \"Dark Side Of The Moon\"\n\n\n--3. Aus rechtlichen Gr\u00fcnden will Pearl Jam seine Musik nicht mehr anbieten, entsprechend sind alle Songs zu entfernen.\n\n    -- Get Info about Pearl Jam\n    SELECT * FROM songs WHERE artist LIKE \"Pearl%\";  -- 11 Records found\n\n    -- Generic Syntax DELETE FROM:\n    --                DELETE FROM table_name\n    --                WHERE condition;\nDELETE FROM songs WHERE artist = \"Pearl Jam\"\n\n    -- CHECK:\nSELECT * FROM songs WHERE artist LIKE \"Pearl%\";  --0 Records found\n\n--4. Alle Songs die l\u00e4nger als 4 Minuten dauern, sollen per sofort 1.99 kosten.\n\n    -- Get Infos . 4 Min = 240000ms\n    SELECT * FROM songs WHERE duration &gt; 240000;   -- 34 Records\n\n    -- Generic Syntax of UPDATE\n    --                   UPDATE table_name\n    --                   SET column1 = value1, column2 = value2, ...\n    --                   WHERE condition;\nUPDATE songs SET price = 1.99 WHERE duration &gt; 240000;\n\n    -- CHECK:\n    SELECT * FROM songs WHERE duration &gt; 240000; --34 Rows have price 1.99\n\n\n\n--5. Alle Songs die k\u00fcrzer als  Minuten dauern, sollen nur noch 0.49 kosten.\n\n    -- Check Records\n    SELECT * FROM songs ORDER BY duration ASC; -- Es gibt keine songs k\u00fcrzer als 2 Minuten\n\n    UPDATE songs SET price = 0.49 WHERE duration &lt; 120000;  --0 rows affected\n\"\"\"\nimport os # just used for Function os.path.join() \nimport sqlite3 # Module needed for connecting to SQLite-DB\n\n# Datenbank \u00f6ffnen und verbinden\nconn = sqlite3.connect(os.path.join('/home/tom/work/bfh-btw2201/PythonProj/UE04/','small.db')) \n\n# Cursor kreieren. Dieser wird im Zusammenhang mit einem Query verwendet\ncur = conn.cursor() \n.\n.\n.\n.\n.\n</code></pre> Well Done!  <p></p>"},{"location":"le05/","title":"Themen LE05","text":"<ol> <li>Containerisierung</li> <li>MySQL mit Docker installieren</li> <li>Administrations-Tools kennen von MySQL</li> <li>Python-Programmierung mit MySQL und der Verwendung der Library SQLAlchemy</li> <li>Relationenmodell</li> </ol>"},{"location":"le05/instMySQL/","title":"DOCKER-Installationsanleitung MySQL mit phpmyadmin","text":"<p>F\u00fchren Sie folgende Schritte aus:</p>"},{"location":"le05/instMySQL/#start-docker-desktop-auf-ihrem-notebook","title":"Start Docker-Desktop auf Ihrem Notebook","text":"<p>Stellen Sie sicher, dass Sie eingeloggt sind:</p> Docker-Desktop gestartet und angemeldet <p>F\u00fchren Sie ein update des Docker-Desktops aus, falls das notwendig ist. Die aktuelle Version zum Zeitpunkt dieses Dokumentes ist 4.34.3 (Windows)</p>"},{"location":"le05/instMySQL/#verzeichniss-erstellen-fur-ihre-docker-files","title":"Verzeichniss erstellen f\u00fcr Ihre Docker-Files","text":"<pre><code>mkdir docker-compose-files\ncd .\\docker-compose-files\\\nmkdir mysql\ncd mysql\n</code></pre>"},{"location":"le05/instMySQL/#erstellung-des-docker-compose-files","title":"Erstellung des docker-compose-Files","text":"<p>unter Windows:</p> <p><code>notepad docker-compose.yml</code> oder unter MacOS mit Text-Editor.</p> <p>Wir werden mit diesem compose-file 2 Container erstellen:</p> <ul> <li>Container mit dem RDBMS  MySQL, Version 8.0.40. Der Name des Containers ist <code>mysql</code> </li> <li>Container mit phpmyadmin. Dies ist ein webbasiertes Management-Interface, um MySQL zu administrieren. Name des Containers: <code>phpmyadmin-container</code></li> </ul> <p>Aktionen,ausgel\u00f6st durch docker-compose.yml</p> <p>Versuchen Sie die Aussagen unten im docker-compose.yml zu verifizieren. </p> <ul> <li>MySQL l\u00e4uft als Docker-Container mit dem Namen \u00abmysql\u00bb.</li> <li>Als MySQL-Version wird 8.0.40 gew\u00e4hlt</li> <li>phpmyadmin, als webbasiertes Verwaltungstool, l\u00e4uft als Web-Dienst auf Port 80 und verbindet sich auf die DB mit dem User <code>root</code> und dem Passwort <code>btw2201btw2201</code>. Die Angabe unter Ports <code>80:80</code> heisst, dass der externe Port 80 auf den Container-Port 80 verbunden wird.</li> <li>Als Version f\u00fcr phpmyadmin wird <code>latest</code> gew\u00e4hlt </li> <li>MySQL verwendet den Standard-Port 3306. Der externe Port lautet ebenfalls 3306.</li> <li>Der Hostname der MySQL-Instanz l\u00e4uft aus Sicht Ihrer Installation unter <code>localhost</code> </li> <li>Die IP-Adresse lautet 127.0.0.1</li> <li>Das root-Passwort lautet: btw2201btw2201</li> <li>Ein User <code>student</code> wird angelegt mit Passwort <code>btw2201btw2201</code></li> <li>Eine Datenbank mit dem Namen <code>MYDB</code> wird angelegt</li> <li>Beide Container werden immer automatisch gestartet</li> <li>Die Daten von MySQL werden auf dem Host unter mysql-data gespeichert.</li> <li>Das virtuelle Container-Verzeichnis /var/lib/mysql zeigt auf das physische Volume mysql-data der Docker-Installation.</li> </ul> <p>Inhalt von </p> docker-compose.yml<pre><code>services:\n  mysql:\n    image: mysql:8.0.40\n    container_name: mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: btw2201btw2201\n      MYSQL_DATABASE: MYDB\n      MYSQL_USER: student\n      MYSQL_PASSWORD: btw2201btw2201\n    ports:\n      - \"3306:3306\"\n    volumes:\n      - mysql_data:/var/lib/mysql\n    networks:\n      - mysql-phpmyadmin\n\n  phpmyadmin:\n    image: phpmyadmin/phpmyadmin:latest\n    container_name: phpmyadmin-container\n    restart: always\n    depends_on:\n      - mysql\n    ports:\n      - \"80:80\"\n    environment:\n      PMA_HOST: mysql\n      PMA_PORT: 3306\n      MYSQL_ROOT_PASSWORD: btw2201btw2201\n    networks:\n      - mysql-phpmyadmin\n\nvolumes:\n  mysql_data:\nnetworks:\n  mysql-phpmyadmin:\n</code></pre>"},{"location":"le05/instMySQL/#docker-container-starten","title":"Docker-Container starten","text":"<p>Im Verzeichnis, wo das docker-compose.yml liegt, starten Sie die Container mit</p> <p><code>docker compose up -d</code></p> <p>Kontrolle, ob Container laufen mit</p> <p><code>docker ps -a</code></p> Container running? <p>Auch im Docker-Desktop sind die laufenden Container sichtbar:</p> Container running?"},{"location":"le05/instMySQL/#varianten-der-mysql-administration","title":"Varianten der MySQL-Administration","text":"<p>Es gibt nun 3 M\u00f6glichkeiten auf die Datenbank zuzugreifen und zu administrieren</p> <ol> <li>Via Web-Anwendung phpmyadmin. Diese l\u00e4uft auf http://localhost:80</li> <li>Via MySQL Workbench</li> <li>Zugriff mit mysql-client im Container. Dieser Client wird mit Konsolenbefehlen bedient.</li> </ol> <p>Versuchen Sie die Zugriffe in dieser Reihenfolge gem\u00e4ss folgenden Anleitungen</p>"},{"location":"le05/instMySQL/#variante-1-phpmyadmin","title":"Variante 1: phpMyAdmin","text":"<p>Mit Browser Zugriff auf <code>http://localhost:80</code></p> <p>Login mit user <code>root</code>, Passwort: <code>btw2201btw2201</code>. Dies Credentials haben wir im <code>docker-compose.yml</code> definiert.</p> <p>Geben Sie dem User <code>student</code> alle Privilegien:</p> phpMyAdmin Interface f\u00fcr Anpassung des User-Privilegien Erfolgsmeldung nach Go. Das angewendete SQL ist sichtbar. <p>Nun als <code>root</code> abmelden und sich mit <code>student</code> wieder neu anmelden:</p> Anmeldung als User student. <p>Sie haben nun mit diesem Datenbank-User \u00abstudent\u00bb alle Privilegien, um die Datenbank zu administrieren:</p> Ansicht `User accounts` in phpMyAdmin <p>phpMyAdmin eignet sich gut, um MySQL-Datenbanken zu administrieren. Es k\u00f6nnen damit auch SQL-Befehle f\u00fcr Datenanalysen gemacht werden.</p> <p>Dokumentation phpMyAdmin</p>"},{"location":"le05/instMySQL/#variante-2-mysql-workbench","title":"Variante 2: MySQL Workbench","text":"<p>MySQL-Workbench- Download</p> <p>Erstellen Sie eine Verbindung auf die lokale MySQL-Installation mit</p> Erstellung einer Verbindung` in phpMyAdmin <p>Verbindungsdetails:</p> Verbindungsdetails f\u00fcr die lokale MySQL-Installation <p>das Passwort wird dann abgefragt und ein Eintrag bei den Verbindungen wird gemacht.</p> <p>Starten Sie die Verbindung und machen Sie sich mit der Oberfl\u00e4che vertraut. Weitere Erkl\u00e4rungen erfolgen in der Vorlesung.</p> <p>Dokumentation MySQL Workbench</p> MySQL Workbench Admin-Screen"},{"location":"le05/instMySQL/#variante-3-mysql-client-in-der-mysql-container-konsole","title":"Variante 3: mysql-client in der MySQL-Container-Konsole","text":"<p>Die Verbindung erfolgt aus der Powershell in Windows 11 oder dem Terminal in MacOS auf den Docker-Container mit MySQL:</p> <p><code>docker exec -it mysql mysql -u student -p</code></p> <p>Passwort: <code>btw2201btw2201</code></p> <p>Mit dem SQL-Befehl <code>show databases;</code> k\u00f6nnen Sie die vorhandenen Datenbanken abfragen. </p> <p>Die M\u00f6glichkeiten, um die Datenbak zu administrieren und zu bearbeiten sind mit dieser Variante genau gleich, wie mit den anderen Varianten. Es ist ein Command-Line-Interface (CLI) ohne jegliche grafische M\u00f6glichkeiten.</p> mysql-Client als CLI"},{"location":"le05/ue05-01/","title":"UE05-01-MySQL und erste Datenbank","text":"<p>In Aufgabe UE03-03 haben wir CSV-Dateien in SQLite importiert. Bei diesem Vorgang liessen wir die Tabellen automatisch erstellen und haben dann die Spaltennamen manuell angepasst gem\u00e4ss Beschreibung des Datenbankschemas. </p> <p>Nun wollen wir die Datenbank in MySQL nachbauen. Entsprechend sollst du eine Datenbank <code>StrassenCH</code> erstellen, die f\u00fcr den Moment zwei Tabellen enth\u00e4lt: <code>new_PLZ1</code> und <code>new_STR</code>.</p> <p>Erstelle die Datenbank und die Tabellen mit den 3 Varianten:</p> <ol> <li>phpMyAdmin  DB-Name: StrasseCHv1</li> <li>MySQL Workbench DB-Name: StrasseCHv2</li> <li>mysql-CLI im mysql-Container DB-Name: StrasseCHv3</li> </ol> <p>Spielen Sie mit den 3 Tools. Sie k\u00f6nnen auch versuchen DBs und Tabellen wieder zu l\u00f6schen. Am Ende der \u00dcbung sollten Sie eine DB mit dem Namen <code>StrassenCH</code> erstellt haben. Diese werden wir in UE05-02 ben\u00f6tigen.</p> <p>Folgende MySQL-Befehle k\u00f6nnen bei der Variante 3 helfen:</p> <p><code>USE database_name; --- Wechseln der momentan aktiven Datenbank</code></p> <p><code>CREATE DATABASE database_name; --- Erstellt eine neue/leere Datenbank</code></p> <p><code>SHOW DATABASES; --- Zeigt die aktuell vorhandenen Datenbanken an</code></p> <p><code>SHOW TABLES; --- Zeigt die aktuell vorhandenen Tabellen an</code></p> <p>Bei Varianten 1 und 2 kann alles grafisch gemacht werden.</p>"},{"location":"le05/ue05-02/","title":"UE05-02 Datentransfer von SQLite-DB nach MySQL mit Python","text":"<p>UE05-02-Datentransfer von SQLite-DB nach MySQL mit Python und der sqlalchemy-Library</p> <p>Laden Sie die Daten der Tabellen <code>new_PLZ1</code> und <code>new_STR</code> aus der SQLite-Datenbank in die MySQL-Datenbank. Verwenden Sie dazu die Library <code>sqlalchemy</code>. Informieren Sie sich zu dieser Library !   Link</p> <p>Tipp: Bef\u00fcllen Sie in einem ersten Schritt ein Pandas-Dataframe und anschliessend schreiben Sie die Daten in MySQL.</p> <p>Die Installation der ben\u00f6tigten Libraries:</p> <p><code>pip install pymysql pandas sqlalchemy mysql-connector-python</code></p> <p>Laden der ben\u00f6tigten Libraries:</p> <pre><code>import pymysql\nimport pandas\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.sql import text\n</code></pre> <p>Hinweis</p> <p>Siehe auch die markierten Code-Zeilen 74+75 im L\u00f6sungsvorschlag und beachten Sie diese elegante und einfache M\u00f6glichkeit, mit <code>Pandas</code> und <code>sqlalchemy</code> Tabellen mit Daten in einer Datenbank zu kreieren.</p> <p><code>sqlalchemy</code> \u00fcnterst\u00fctzt die Datenbanken <code>SQLite</code>, <code>Postgresql</code>, <code>MySQL</code> &amp; <code>MariaDB</code>, <code>Oracle</code>, and <code>MS-SQL</code>. </p> <p> Database-Engineers using SQLAlchemy-Library.. </p> L\u00f6sungsvorschlag mit Pandas und SQLAlchemy <p>Achtung bei Zeilen 30-33 ! Verwenden Sie Ihre spezifischen Angaben.</p> <pre><code>\"\"\"\nFile: ue05-02.py\nAuthor: Thomas J\u00e4ggi\nDate: 20.10.2024\nDescription: Transfer Data from SQLite to MySQL using Python-Library\n             sqlalchemy for tables NEW_PLZ1 and NEW_STR\nNeeded Libraries mit pip: python3 -m pip install ....\npip install PyMySQL pandas sqlalchemy mysql-connector-python\noder wenn Sie mit anaconda als Library-Manager arbeiten: (dauert manchmal sehr lange, pip ist schneller.)\nconda install anaconda::sqlalchemy\nconda install anaconda::mysql-connector-python\nconda install anaconda::pandas\nconda install conda-forge::pymysql\n\"\"\"\nimport os\nimport sqlite3\nimport pymysql as mysql\nimport pandas as pd\n# SQLAlchemy is a popular Python SQL toolkit and Object-Relational Mapping (ORM) library. \n# Essentially, it bridges the gap between Python programming and SQL databases.\nfrom sqlalchemy import create_engine\n# The create_engine function in SQLAlchemy is used to establish a connection to a database. \n# Think of it as the gateway between your Python application and the database.    \n\nfrom sqlalchemy.sql import text\n# needed for wrapping SQL-statements into text(). See Lines 78,83,88,93.    \n\n# Create a connection string using SQLAlchemy. \n# Replace the placeholder values with your actual database credentials.\nusername = 'tom'\npassword = 'mysecretmySQLpassword'\nhostname = 'hercule.lan.tj'\ndatabase = 'pytest'    # in MySQL auch Schema genannt.\n\n# Create a connection string for MySQL\nconnection_string = f'mysql+mysqlconnector://{username}:{password}@{hostname}/{database}'    \n\n# Create an engine\nengine = create_engine(connection_string)    \n\n# Now you can use the engine to connect to the database\ncon_mysql = engine.connect()    \n\n# SQLite Datenbank \u00f6ffnen und verbinden\nconnsqlite = sqlite3.connect(os.path.join('/home/tom/work/bfh-btw2201/Exercise-DB/','StrassenverzPost.db'))\n# Cursor definieren f\u00fcr SQLite\ncursqlite = connsqlite.cursor()     \n\n# Tabellen NEW_PLZ1 und NEW_STR in Dataframes abf\u00fcllen\n# ================\nsqlquery1 = \"SELECT * FROM NEW_PLZ1;\"\n# Read all data from SQLite into Dataframe df\ndf_new_plz1 = pd.read_sql(sqlquery1,con=connsqlite) #\nprint(df_new_plz1.head(10))    \n\nsqlquery2 = \"SELECT * FROM NEW_STR;\"\n# Read all data from SQLite into Dataframe df\ndf_new_str = pd.read_sql(sqlquery2,con=connsqlite) #\nprint(df_new_str.head(10))    \n# ================    \n\n# Schliesse Cursor und Verbindung zu SQLite\ncursqlite.close()\nconnsqlite.close()    \n\n# Write the DataFrames to the MySQL tables in Schema defined in the Connection \"connection_string\"\n# Generic Statement:\n# df.to_sql('your_table', con=engine, if_exists='replace', index=False)\n# your_table: The name of the table you want to write to.\n# con=engine: The connection to your MySQL database.\n# if_exists='replace': Replace the table if it already exists. You can also use\n#                      'append' to add to an existing table without deleting it.\n# index=False: Do not write DataFrame index as a column.\ndf_new_plz1.to_sql('NEW_PLZ1', con=con_mysql, if_exists='replace', index=False) #(1)\ndf_new_str.to_sql('NEW_STR', con=con_mysql, if_exists='replace', index=False)    \n\n# Is there data in Table NEW_STR ?\nresult1 = con_mysql.execute(text(\"SELECT * FROM NEW_STR LIMIT 5\")) #(2)\nfor row in result1:\n    print(row)\n\n# Is there data in Table NEW_PLZ1 ?\nresult3 = con_mysql.execute(text(\"SELECT * FROM NEW_PLZ1 LIMIT 5\")) #(3)\nfor row in result3:\n    print(row)\n\n# How many rows in Table NEW_PLZ1 ?\nresult4 = con_mysql.execute(text(\"SELECT count(*) FROM NEW_PLZ1\")) #(4)\nfor row in result4:\n    print(\"rows transfered into NEW_PLZ1:\",row[0])    \n\n# How many rows in Table NEW_STR ?\nresult2 = con_mysql.execute(text(\"SELECT count(*) FROM NEW_STR\")) #(5)\nfor row in result2:\n    print(\"rows transfered into NEW_STR :\", row[0])    \n\n# Close the mySQL-connection\ncon_mysql.close()\n</code></pre> <ol> <li>  siehe pandas.DataFrame.to_sql</li> <li> needs \"from sqlalchemy.sql import text\"</li> <li> needs \"from sqlalchemy.sql import text\"</li> <li> needs \"from sqlalchemy.sql import text\"</li> <li> needs \"from sqlalchemy.sql import text\"</li> </ol> <p>Bemerkt?</p> <p>Die Verwendung der sqlalchemy-Engine erstellt die Tabellen, wenn diese noch nicht bestehen. Die Datenbank hingegen (=Schema), die muss bereits erstellt sein. </p>"},{"location":"le05/ue05-03/","title":"UE05-03-Aufgaben zum Relationenmodell","text":""},{"location":"le05/ue05-03/#aufgabe-1","title":"Aufgabe 1","text":"<p>Erkl\u00e4ren Sie folgende Begriffe: Tupel, Attribut, Relation, Domain (Gebiet), Grad, Kardinalit\u00e4t.</p>"},{"location":"le05/ue05-03/#aufgabe-2","title":"Aufgabe 2","text":"<p>Warum ist die Wahl problematisch, den Schl\u00fcsselkandidaten Symbol in der Relation chemischeElemente als Prim\u00e4rschl\u00fcssel zu w\u00e4hlen?</p> Relation chemischeElemente Antwort <p>Dagegen spricht: Ein chemisches Element wird identifiziert durch die Protonenzahl, nicht durch  seinen Namen oder sein Symbol. Ein eventuell neu entdecktes Element k\u00f6nnte erst dann eingetragen werden,  wenn es einen Namen hat. Zus\u00e4tzlich lassen sich Zahlen als Prim\u00e4rschl\u00fcssel besser handhaben als Zeichen.</p>"},{"location":"le05/ue05-03/#aufgabe-3","title":"Aufgabe 3","text":"<p>Geben Sie den Prim\u00e4rschl\u00fcssel der nachfolgenden Relation an.</p> <p>RELATION VerkaeuferProdukt:</p> VerkNr VerkName PLZ VerkAdresse Produktname Umsatz V1 Meier 8075 Z\u00fcrich Waschmaschine 11000 V1 Meier 8075 Z\u00fcrich Herd 5000 V1 Meier 8075 Z\u00fcrich K\u00fchlschrank 1000 V2 Schneider 1580 Broye Herd 4000 V2 Schneider 1580 Broye K\u00fchlschrank 33000 V3 M\u00fcller 3000 Bern Staubsauger 1000 Antwort <p>VerkNr und Produktname zusammen, also (VerkNr, Produktname)</p>"},{"location":"le05/ue05-03/#aufgabe-4","title":"Aufgabe 4","text":"<p>Welche Attribute m\u00fcssen beim Eintrag eines neuen Records (=Tupel) immer mindestens angegeben werden, wenn die Integrit\u00e4tsregeln beachtet werden sollen?</p> Antwort <p>Der Prim\u00e4rschl\u00fcssel muss nach der ersten Integrit\u00e4tsregel zwingend angegeben werden. Hinzu kommen alle Attribute mit der Vorgabe Not Null.</p>"},{"location":"le05/ue05-03/#aufgabe-5","title":"Aufgabe 5","text":"<p>Finden Sie die Prim\u00e4r- und Fremdschl\u00fcssel aller Relationen der Beispieldatenbank Strassenverzeichnis Schweiz und Li.</p> <p>Stellen Sie sich vor, wir h\u00e4tten in der Datenbank alle PK und FK der Tabellen definiert.</p> <p>Stellen sie sich vor, wir m\u00f6chten dann eine Tabelle l\u00f6schen.</p> <p>Frage:</p> <ul> <li>Unter welcher Voraussetzung geht das?</li> <li>Wann geht das nicht?</li> </ul> <p>Beantworten Sie die Frage unter Ber\u00fccksichtigung der Integrit\u00e4tsregeln. Ein RDBMS wie MySQL beachtet diese Regeln strikt und gibt Fehlermeldungen aus, wenn diese verletzt werden.</p>"},{"location":"le05/ue05-03/#aufgabe-6","title":"Aufgabe 6","text":"<p>Kurzer Ausflug in die SQL-Welt mit dem Ziel:</p> <p>Die Referenzielle Integrit\u00e4t sichtbar machen !</p> <p>Erstellen Sie folgende Tabelle in MySQL, z.Bsp im Schema <code>MYDB</code> oder erstellen Sie eine neue DB <pre><code>CREATE TABLE Employees (\n    EmployeeID INT PRIMARY KEY,\n    FirstName VARCHAR(50),\n    LastName VARCHAR(50),\n    ManagerID INT,\n    FOREIGN KEY (ManagerID) REFERENCES Employees(EmployeeID)\n);\n</code></pre></p> <p>Laden Sie folgende Daten in die Tabelle:</p> <pre><code>INSERT INTO Employees (EmployeeID, FirstName, LastName, ManagerID) VALUES\n(1, 'Stefan', 'Gr\u00f6sser', NULL),   \n(2, 'Thomas', 'J\u00e4ggi', 1),        \n(3, 'Beat', 'Jans', 2),     \n(4, 'Markus', 'Studer', 2);  \n</code></pre> <p>Nun versuchen Sie den Mitarbeiter mit ID=2 oder ID=1 zu l\u00f6schen:</p> <p><pre><code>DELETE FROM Employees WHERE EmployeeID = 2;\n</code></pre> Warum diese Fehlermeldung?</p> <p>Nun l\u00f6schen des Mitarbeiters mit ID=3 <pre><code>DELETE FROM Employees WHERE EmployeeID = 3;\n</code></pre> Warum gibt es hier keinen Fehler? Das Tupel wird gel\u00f6scht!</p> <p>Diese Verhalten nennt man Referenzielle Integrit\u00e4t ! </p> <p>Beschreiben Sie dieses Verhalten in eigenen Worten anhand von diesem Beispiel!</p> Antwort <p>Mit dem Beispiel haben sie dann auch die Frage 5 beantwortet.</p> <p>Referentielle Integrit\u00e4t und die Reaktion auf die Verletzung der FK-Regeln</p> <p>Lesen Sie den beiliegenden pdf-Artikel zur referentiellen Integrit\u00e4t RI. Notieren sie sich die SQL-Funktionen und deren Wirkung auf die Behandlung der RI-Regeln.</p> ON DELETE .. ON UPDATE ... ON DELETE CASCADE ON UPDATE  CASCADE ON DELETE SET NULL ON UPDATE  SET NULL ON DELETE SET DEFAULT ON UPDATE  SET DEFAULT <p>ReferenzielleIntegritaet.pdf</p>"},{"location":"le05/useful-docker-commands/","title":"Useful Docker commands","text":"<p>These commands can be used in the terminal. Docker-Desktop provides features for the same in the GUI.</p>"},{"location":"le05/useful-docker-commands/#verify-that-container-is-running","title":"Verify that Container is running","text":"<p><pre><code>docker ps -a\n</code></pre> Lists containers stopped and running</p> <p>oder auch</p> <pre><code>docker container ls\n</code></pre>"},{"location":"le05/useful-docker-commands/#show-docker-logs-of-a-container","title":"Show Docker logs of a Container","text":"<pre><code>docker logs &lt;container name or id&gt;\n</code></pre>"},{"location":"le05/useful-docker-commands/#stopstartrestart-container","title":"Stop/Start/Restart Container","text":"<p><pre><code>docker stop &lt;container name or id&gt;\n</code></pre> <pre><code>docker start &lt;container name or id&gt;\n</code></pre> <pre><code>docker restart &lt;container name or id&gt;\n</code></pre></p>"},{"location":"le05/useful-docker-commands/#delete-a-container","title":"Delete a Container","text":"<pre><code>docker rm &lt;container name or id&gt;\n</code></pre>"},{"location":"le05/useful-docker-commands/#list-container-images","title":"List Container-Images","text":"<pre><code>docker image ls\n</code></pre>"},{"location":"le05/useful-docker-commands/#delete-container-image","title":"Delete Container-Image","text":"<pre><code>docker rmi &lt;image name or id&gt;\n</code></pre>"},{"location":"le05/useful-docker-commands/#remove-all-containers","title":"Remove all Containers","text":"<pre><code>sudo docker rm -f $(sudo docker ps -a -q)\n</code></pre> <p>or</p> <pre><code>docker container prune\n</code></pre>"},{"location":"le05/useful-docker-commands/#remove-all-images","title":"Remove all images","text":"<pre><code>sudo docker image remove -f $(sudo docker images -a -q)\n</code></pre>"},{"location":"le05/useful-docker-commands/#enter-filesystem-as-root-of-created-container","title":"Enter Filesystem as root of created Container","text":"<pre><code>docker exec -it &lt;container name or id&gt; sh\n</code></pre>"},{"location":"le05/useful-docker-commands/#enter-filesystem-as-user-xxx-of-created-container","title":"Enter Filesystem as User XXX of created Container","text":"<pre><code>docker exec -it  -u XXX &lt;container name or id&gt; sh\n</code></pre>"},{"location":"le06/","title":"Themen LE06","text":"<ol> <li>Funktionale und Transitive Abh\u00e4ngigkeiten</li> <li>Normalformen 1NF, 2NF, 3NF</li> <li>Datenanalyse, ERM, Kardinalit\u00e4t</li> <li>Erstellung einer Schuldatenbank mit Beispieldaten unter Anwendung von SQL-Befehlen</li> <li>SQL-Abfragen der Schuldatenbank</li> </ol>"},{"location":"le06/ue06-01/","title":"UE06-01-Funktionale und Transitive Abh\u00e4ngigkeit","text":"<p>Alle folgenden Aufgaben basieren auf der Beispieltabelle Abteilungsmitarbeiter.</p> <p>Abteilungsmitarbeiter</p> ID_Mitarbeiter Mitarbeitername ID_Abteilung Bezeichnung Bereich 1 Huber 1 Planung Nutzfahrzeuge 2 M\u00fcller 1 Planung Nutzfahrzeuge 3 Ernst 2 Fertigung PKW 4 Klein 3 Fertigung Nutzfahrzeuge"},{"location":"le06/ue06-01/#zum-einstimmen-fur-die-folgenden-aufgaben","title":"Zum Einstimmen f\u00fcr die folgenden Aufgaben","text":"<p>Aufgabe 1</p> <ol> <li>Wie lautet die Definition der 3NF?</li> <li>Wie lautet die \u00dcberf\u00fchrungsregel der 3NF?</li> <li>Was versteht man unter funktionaler Abh\u00e4ngigkeit?</li> <li>Was versteht man unter transitiver Abh\u00e4ngigkeit?</li> </ol> 1. Definition 3NF <p>Die 3NF garantiert, dass es in einer Tabelle keine Merkmale gibt, die bereits von einem Nichtschl\u00fcsselmerkmal abh\u00e4ngig sind.</p> <p>Definition:</p> <p>Eine Tabelle ist in 3NF, wenn sie die 2NF erf\u00fcllt und kein Nichtschl\u00fcsselmerkmal vom Schl\u00fcssel transitiv abh\u00e4ngig ist!</p> 2. \u00dcberf\u00fchrungsregel der 3NF <p>Eine Tabelle, die der 1NF und der 2NF gen\u00fcgt, aber nicht die 3NF erf\u00fcllt, muss in Teiltabellen zerlegt werden. Dabei m\u00fcssen alle vom Schl\u00fcssel transitiv abh\u00e4ngigen Nichtschl\u00fcsselmerkmale zusammen mit den Nichtschl\u00fcsselmerkmalen, von denen sie funktional abh\u00e4ngig sind, zu eigenen Tabellen zusammengefasst werden.</p> <p>3 Schritte:</p> <ol> <li>Bestimme alle vom Schl\u00fcssel transitiv abh\u00e4ngigen Nichtschl\u00fcsselmerkmale.</li> <li>Bilde aus diesen transitiv abh\u00e4ngigen Nichtschl\u00fcsselmerkmalen und den Nichtschl\u00fcsselmerkmalen, von denen sie funktional abh\u00e4ngig sind, eigene Tabellen.</li> <li>Entferne aus der urspr\u00fcnglichen Tabelle alle transitiv abh\u00e4ngigen Nichtschl\u00fcsselmerkmale.</li> </ol> 3. Funktionale Abh\u00e4ngigkeit <p>Ein Merkmal A ist funktional abh\u00e4ngig von einem Merkmal S, wenn zu jedem m\u00f6glichen Wert von S genau ein Wert aus A existiert.</p> <p>Schreibweise: S \u2192 A</p> <p>Es wird auch noch die volle  funktionale Abh\u00e4ngigkeit unterschieden. Diese lautet:</p> <p>Ein Merkmal A ist voll funktional abh\u00e4ngig von einem aus S1 und S2 zusammengesetzten Schl\u00fcssel, wenn A funktional abh\u00e4ngig vom Gesamtschl\u00fcssel, nicht aber von seinem Teilschl\u00fcssel ist.</p> <p>Schreibweise: (S1,S2) \\(\\Rightarrow\\) A</p> <p>Eine Tabelle ist in 2NF, wenn sie die 1NF erf\u00fcllt und wenn alle Nichtschl\u00fcsselattribute vom Schl\u00fcssel voll funktional abh\u00e4ngig sind. Aus jeder Kombination von Niichtschl\u00fcsselmerkmal und Teilschl\u00fcsseln, funktionale Abh\u00e4ngigkeit vorausgesetzt, wir eine eigene Tabelle gebildet. Einfach ist es, wenn der Schl\u00fcssel nicht zusammengesetzt ist, d.h. nur aus einem Schl\u00fcsselattribut besteht, also (S) \\(\\Rightarrow\\) A.</p> 4. Transitive Abh\u00e4ngigkeit <p>Allgemein Transitivit\u00e4t:</p> <p>Wenn man aus \"S bestimmt A\" und \"A bestimmt B\" folgern kann, dass zwangsl\u00e4ufig auch \"S bestimmt B\" gilt, dann ist Transitivit\u00e4t gegeben.</p> <p>Schreibweise:</p> <p>S \\(\\rightarrow\\) A \\(\\rightarrow\\) B</p> <p>S \\(\\nleftarrow\\) A</p> <p>Definition:</p> <p>Ein Merkmal B ist transitiv von einem Merkmal S abh\u00e4ngig, wenn es ein Merkmal A gibt, so dass gilt:</p> <ul> <li>B ist funktional abh\u00e4ngig von A:  A \\(\\rightarrow\\) B</li> <li>A ist funktional abh\u00e4ngig von S:  S \\(\\rightarrow\\) A</li> <li>S ist nicht funktional abh\u00e4ngig von A:   A \\(\\nrightarrow\\) S</li> </ul> <p>Eine Tabelle ist erst in der 3NF, wenn KEINE Transitivit\u00e4t vorhanden ist und die 2NF erf\u00fcllt ist.</p> <p>oder</p> <p>Eine Tabelle ist in 3NF, wenn es kein Nichtschl\u00fcsselattribut gibt, welches vom Schl\u00fcssel transitiv abh\u00e4ngig ist. Zus\u00e4tzlich muss die 2NF erf\u00fcllt sein.</p>"},{"location":"le06/ue06-01/#1-funktionale-abhangigkeit","title":"1. Funktionale Abh\u00e4ngigkeit","text":"<p>Aufgabe 2</p> <p>\u00dcberpr\u00fcfen Sie die funktionale Abh\u00e4ngigkeit: (ja/nein)    </p> <ol> <li>ID_Mitarbeiter \\(\\rightarrow\\) Mitarbeitername</li> <li>ID_Mitarbeiter \\(\\rightarrow\\) ID_Abteilung</li> <li>ID_Mitarbeiter \\(\\rightarrow\\) Bezeichnung</li> <li>ID_Mitarbeiter \\(\\rightarrow\\) Bereich</li> <li>Name \\(\\rightarrow\\) ID_Mitarbeiter</li> <li>ID_Abteilung \u2192 Bezeichnung</li> <li>Bezeichnung \u2192 ID_Abteilung</li> <li>ID_Abteilung \u2192 ID_Mitarbeiter</li> <li>Bereich \\(\\rightarrow\\) Bezeichnung</li> </ol> <p>\u00dcberpr\u00fcfen Sie die funktionale Abh\u00e4ngigkeit: (ja/nein)    </p> Antworten 1.2.3.4.5.6.7.8.9. <p>ID_Mitarbeiter \\(\\rightarrow\\) Mitarbeitername: ja</p> <p>ID_Mitarbeiter \\(\\rightarrow\\) ID_Abteilung: ja</p> <p>ID_Mitarbeiter \\(\\rightarrow\\) Bezeichnung:ja</p> <p>ID_Mitarbeiter \\(\\rightarrow\\) Bereich: ja</p> <p>Name \\(\\rightarrow\\) ID_Mitarbeiter: nein</p> <p>ID_Abteilung \\(\\rightarrow\\) Bezeichnung: ja</p> <p>Bezeichnung \\(\\rightarrow\\) ID_Abteilung: nein</p> <p>ID_Abteilung \\(\\rightarrow\\) ID_Mitarbeiter: nein</p> <p>Bereich \\(\\rightarrow\\) Bezeichnung: nein</p>"},{"location":"le06/ue06-01/#2-transitive-abhangigkeit","title":"2. Transitive Abh\u00e4ngigkeit","text":"<p>Aufgabe 3</p> <ol> <li>Zeichnen Sie das Pfeildiagramm zur Definition transitive Abh\u00e4ngigkeit.    </li> <li>(ID_Mitarbeiter, Bezeichnung, Bereich). Transitive Abh\u00e4ngigkeit ja oder nein?</li> <li>(ID_Abteilung, Bereich, Bezeichnung). Transitive Abh\u00e4ngigkeit ja oder nein?</li> <li>(ID_Mitarbeiter, ID_Abteilung, Mitarbeitername). Transitive Abh\u00e4ngigkeit ja oder nein?</li> <li>(Mitarbeitername, ID_Abteilung, Bezeichnung). Transitive Abh\u00e4ngigkeit ja oder nein?</li> <li>Welche Merkmale sind transitiv abh\u00e4ngig von ID_Mitarbeiter? Begr\u00fcndung? Transitive Abh\u00e4ngigkeit ja oder nein?</li> </ol> Antworten 1.2.3.4.5.6.  <p>S \\(\\rightarrow\\) A \\(\\rightarrow\\) B und</p> <p>S \\(\\nleftarrow\\) A</p> <p>(ID_Mitarbeiter, Bezeichnung, Bereich): nein</p> <p>(ID_Abteilung, Bereich, Bezeichnung): nein</p> <p>(ID_Mitarbeiter, ID_Abteilung, Mitarbeitername):nein</p> <p>(Mitarbeitername, ID_Abteilung, Bezeichnung): nein</p> <p>Welche Merkmale sind transitiv abh\u00e4ngig von ID_Mitarbeiter? (Begr\u00fcndung!):</p> <p>Die beiden Merkmale 'Bereich' und 'Bezeichnung' sind \u00fcber das Merkmal 'ID_Abteilung' transitiv abh\u00e4ngig vom Schl\u00fcsselmerkmal 'ID_Mitarbeiter'. </p> <p>Begr\u00fcndung:</p> <p>Es gilt: ID_Mitarbeiter \\(\\rightarrow\\) ID_Abteilung \\(\\rightarrow\\) Bezeichnung</p> <p>Es gilt nicht: ID_Abteilung \\(\\rightarrow\\) ID_Mitarbeiter</p> <p>und</p> <p>Es gilt: ID_Mitarbeiter \\(\\rightarrow\\) ID_Abteilung \\(\\rightarrow\\) Bereich</p> <p>Es gilt nicht: ID_Abteilung \\(\\rightarrow\\) ID_Mitarbeiter</p>"},{"location":"le06/ue06-01/#3-schlussfolgerungen","title":"3. Schlussfolgerungen","text":"<p>Aufgabe 4</p> <p>\u00dcberpr\u00fcfen und begr\u00fcnden sie folgende Aussagen:    </p> <ol> <li>Die transitive Abh\u00e4ngigkeit kann nur in einer Tabelle auftreten, die mindestens einen Fremdschl\u00fcssel enth\u00e4lt.</li> <li>Tabellen, die einen zusammengesetzten Schl\u00fcssel haben, k\u00f6nnen keine transitiven Abh\u00e4ngigkeiten enthalten.    </li> </ol> Antworten 1.2. <p>Die transitive Abh\u00e4ngigkeit kann nur in einer Tabelle auftreten, die mindestens einen  Fremdschl\u00fcssel enth\u00e4lt.</p> <p>Nein. Das Merkmal 'ID_Abteilung' der Tabelle 'Abteilungsmitarbeiter' ist nur dann ein Fremdschl\u00fcssel, wenn es in einer anderen Tabelle als Schl\u00fcssel vorkommt, z. B. in der Tabelle 'Abteilung'. Da es im obigen Beispiel eine Tabelle 'Abteilung' nicht gibt, ist 'ID_Abteilung' in der Tabelle 'Abteilungsmitarbeiter' auch kein Fremdschl\u00fcssel. Trotzdem treten transitive Abh\u00e4ngigkeiten in der Tabelle auf.</p> <p>Tabellen, die einen zusammengesetzten Schl\u00fcssel haben, k\u00f6nnen keine transitiven Abh\u00e4ngigkeiten enthalten.</p> <p>Nein. Es kann sehr wohl Merkmale geben, die transitiv abh\u00e4ngig sind von einem zusammengesetzten Schl\u00fcssel.</p>"},{"location":"le06/ue06-01/#4-uberfuhrung-in-die-3nf","title":"4. \u00dcberf\u00fchrung in die 3NF","text":"<p>Aufgabe 5</p> <p>\u00dcberf\u00fchren Sie die Tabelle Abteilungsmitarbeiter in die 3NF.</p> Antworten <p>Ursprungstabelle:</p> Abteilungsmitarbeiter ID_Mitarbeiter Mitarbeitername ID_Abteilung Bezeichnung Bereich 1 Huber 1 Planung Nutzfahrzeuge 2 M\u00fcller 1 Planung Nutzfahrzeuge 3 Ernst 2 Fertigung PKW 4 Klein 3 Fertigung Nutzfahrzeuge <p>Tabellen in 3NF:</p> Abteilungsmitarbeiter ID_Mitarbeiter Mitarbeitername ID_Abteilung (=FK) 1 Huber 1 2 M\u00fcller 1 3 Ernst 2 4 Klein 3 Abteilung ID_Abteilung  Bezeichnung Bereich 1 Planung Nutzfahrzeuge 2 Fertigung PKW 3 Fertigung Nutzfahrzeuge <p>Wenn man diese Tabelle \"Abteilung\" betrachtet, stellen wir fest, dass es zwischen der Abteilungsbezeichnung und den Abteilungsbereichen eine m:m Beziehung gibt:</p> <p>Eine Abteilung kann mehrere Bereichbezeichnungen haben und eine Bereichsbezeichnung kann in mehreren Abteilungen vorkommen.</p> <p>Dies w\u00fcrde uns zu einer Verbindungstabelle \"Abteilungsbezeichnung_Abteilungsbereich zwingen. In diesem kleinen Beispiel wollen wir aber darauf verzichten. Wenn die Datenbank aber hunderte von Abteilungen und Abteilungsbezeichnungen h\u00e4tte, m\u00fcsste man auch die Tabelle \"Abteilung\" in eine 3NF \u00fcberf\u00fchren, um sicher zu stellen, dass wir das Risiko von Anomalien ausschliessen. </p> <p>Ein weiteres Argument, die 3NF hier anzuwenden ist die Tatsache, dass die Information zu Abteilung und Bereich statisch ist. Es ist nicht anzunehmen, dass in dieser Tabelle viele \u00c4nderungen vorkommen. Das Risiko von Anomalien bleibt daher minimal.</p> <p>Merke:</p> <p>Das Verhindern von ANOMALIEN ist das Kriterium, um \u00fcberhaupt Tabellen in eine Normalform zu \u00fcberf\u00fchren!</p>"},{"location":"le06/ue06-01/#5-tabellen-erstellen-in-mysql","title":"5. Tabellen erstellen in MySQL","text":"<p>Aufgabe 6</p> <p>Nachdem Sie die Tabelle in 3NF \u00fcberf\u00fchrt haben, erstellen Sie die Datenbank (Schema) und die Tabellen in MySQL. Verwenden Sie dazu SQL-Befehle. Als Werkzeug k\u00f6nnen Sie eines der drei vorgestellten Tools verwenden: phpMyAdmin, MySQL Workbench oder mysql-CLI. Empfehlung: MySQL Workbench.</p> Physische DB erstellen mit SQL's <pre><code>-- Datenbank (Schema) in MySQL erstellen. Der DB-Name kann beliebig gew\u00e4hlt werden.\nCREATE DATABASE Mitarbeiter;\n\n-- Fokus auf die erstellte DB setzen. Damit werden die folgenden SQL-Befehle auf diese DB angewendet.\n-- Andernfalls kann es sein, dass MySQL nich weiss, auf welche DB die SQL-Befehle angewendet werden sollen und darum eine Fehlermeldung erscheint.\n\nUSE Mitarbeiter;\n\n\n-- Tabelle Abteilung erstellen. Diese zuerst erstllen, weil die Mitarbeitertabelle auf diese referenzieren wird\n\n-- Alle SQLs funktionieren in einigen Umgebungen auch ohne Backticks `\nCREATE TABLE `Abteilung` (\n  `ID_Abteilung` int NOT NULL,\n  `Bezeichnung` varchar(45) DEFAULT NULL,\n  `Bereich` varchar(45) DEFAULT NULL,\n  PRIMARY KEY (`ID_Abteilung`)\n);\n\n-- auch das sollte ok sein, andernfalls verwenden Sie ` wie oben\n\nCREATE TABLE Abteilung (\nID_Abteilung int NOT NULL,\nBezeichnung varchar(45) DEFAULT NULL,\nBereich varchar(45) DEFAULT NULL,\nPRIMARY KEY (ID_Abteilung)\n);\n\n-- Tabelle Abteilungsmitarbeiter erstellen mit AbteilungsID als FK  auf Abteilung\nCREATE TABLE `Abteilungsmitarbeiter` (\n  `ID_Mitarbeiter` int NOT NULL,\n  `Mitarbeitername` varchar(45) DEFAULT NULL,\n  `ID_Abteilung` int DEFAULT NULL,\n  PRIMARY KEY (`ID_Mitarbeiter`),\n  FOREIGN KEY (`ID_Abteilung`) REFERENCES `Abteilung` (`ID_Abteilung`)\n);\n\n\n-- Einige Records einf\u00fcgen in Tabelle Abteilung\n\nINSERT INTO `Mitarbeiter`.`Abteilung` (`ID_Abteilung`, `Bezeichnung`, `Bereich`) VALUES ('1', 'Planung', 'Nutzfahrzeuge');\nINSERT INTO `Mitarbeiter`.`Abteilung` (`ID_Abteilung`, `Bezeichnung`, `Bereich`) VALUES ('2', 'Fertigung', 'PKW');\nINSERT INTO `Mitarbeiter`.`Abteilung` (`ID_Abteilung`, `Bezeichnung`, `Bereich`) VALUES ('3', 'Fertigung', 'Nutzfahrzeuge');\n\n-- einige Records einf\u00fcgen in Tabelle Abteilungsmitarbeiter\nINSERT INTO `Mitarbeiter`.`Abteilungsmitarbeiter` (`ID_Mitarbeiter`, `Mitarbeitername`, `ID_Abteilung`) VALUES ('1', 'Huber', '1');\nINSERT INTO `Mitarbeiter`.`Abteilungsmitarbeiter` (`ID_Mitarbeiter`, `Mitarbeitername`, `ID_Abteilung`) VALUES ('2', 'M\u00fcller', '1');\nINSERT INTO `Mitarbeiter`.`Abteilungsmitarbeiter` (`ID_Mitarbeiter`, `Mitarbeitername`, `ID_Abteilung`) VALUES ('3', 'Ernst', '2');\nINSERT INTO `Mitarbeiter`.`Abteilungsmitarbeiter` (`ID_Mitarbeiter`, `Mitarbeitername`, `ID_Abteilung`) VALUES ('4', 'Klein', '3');\n\n\n-- Check: Abfrage aller Abteilungsmitarbeiter\nSELECT * FROM Abteilungsmitarbeiter;\n\n-- Check: Abfrage aller Abteilungen\nSELECT * FROM Abteilung;\n\n\n-- Check: Abfrage beider Tabellen mit Verbindungskriterium ID_Abteilung\nSELECT * FROM Abteilungsmitarbeiter AS am, Abteilung AS a\n       WHERE am.ID_Abteilung = a.ID_Abteilung;\n\n-- Ursprungstabelle wird mit Abfrage \u00fcber beide Tabellen sichtbar, jedoch jetzt in 3NF\nSELECT am.ID_Mitarbeiter, am.Mitarbeitername, a.Bezeichnung, a.Bereich FROM Abteilungsmitarbeiter AS am, Abteilung AS a\n       WHERE am.ID_Abteilung = a.ID_Abteilung;\n</code></pre> Fun at work!"},{"location":"le06/ue06-02/","title":"UE06-02","text":""},{"location":"le06/ue06-02/#aufgabe-1-datenbeziehungen-kardinalitat","title":"Aufgabe 1: Datenbeziehungen / Kardinalit\u00e4t","text":"<p>Aufgabe 1</p> <p>Bestimmen Sie das richtige Beziehungstypenpaar f\u00fcr die Entit\u00e4tsmenge 1 und die Entit\u00e4tsmenge 2. Achten Sie auf die Beziehung in der letzten Spalte.</p> <p>Beispiel</p> Entit\u00e4t 1 Entit\u00e4t 2 Beziehung Kardinalit\u00e4t Mensch Brille tragen 1:1 <p>Aufgabe 1</p> Nr Entit\u00e4t 1 Entit\u00e4t 2 Beziehung Kardinalit\u00e4t 1 Auto Rad dazugeh\u00f6ren ? 2 Taste Tastatur dazugeh\u00f6ren ? 3 Studenten Modul besuchen ? 4 Attribut Tabelle dazugeh\u00f6ren ? 5 Tier Zoo dazugeh\u00f6ren ? 6 Fussballmatch Besucher besuchen ? 7 Kunstwerke Museum ausgestellt ? 8 Studenten Studenten Freundschaften ? 9 Tierart Zoo findet sich ? Antworten 1-Auto-Rad2-Taste-Tastatur3-Studenten-Modul4-Attribut-Tabelle5-Tier-Zoo6-Fussballmatch-Besucher7-Kunstwerk-Museum8-Student-Student9-Tierart-Zoo <p>1:m    </p> <p>m:1</p> <p>m:m    </p> <p>m:1    </p> <p>m:1   </p> <p>1:m</p> <p>m:1    </p> <p>m:m    </p> <p>m:m   </p>"},{"location":"le06/ue06-02/#aufgabe-2-erm-interpretieren","title":"Aufgabe 2: ERM interpretieren","text":"<p>Aufgabe 2</p> <p>Betrachten Sie das folgende ERM und beurteilen Sie die Richtigkeit der nachstehenden Aussagen.</p> <p> ERM zur Filmausleihe </p> <ol> <li>Jeder Film geh\u00f6rt zu einem Kunden  [ ] wahr  [ ] falsch</li> <li>Ein Kunde kann mehrere Filme ausleihen?  [ ] wahr  [ ] falsch</li> <li>Ein Kunde kann mehrere Filme pro Ausleihereignis ausleihen?  [ ] wahr  [ ] falsch</li> <li>Es gibt Filme, die nicht ausgeliehen werden k\u00f6nnen?  [ ] wahr  [ ] falsch</li> <li>Es gibt Kunden, die keine Filme ausgeliehen haben?  [ ] wahr  [ ] falsch</li> <li>Kann ein Kunde mehrere Filme w\u00e4hrend zwei Wochen ausleihen?  [ ] wahr  [ ] falsch</li> <li>Die Beziehung Film \u2013 Ausleihe hat den Beziehungstyp 1-mc.  [ ] wahr  [ ] falsch</li> <li>Die Beziehung Ausleihe \u2013 Kunde hat den Beziehungstyp m-1.  [ ] wahr  [ ] falsch</li> </ol> Antworten 1.2.3.4.5.6.7.8. <p>falsch. Ein Film kann sich in keiner Ausleihe befinden. Film - Ausleihe 1:mc. mc heist: kein, ein oder mehrere.      </p> <p>wahr. Dann wenn der Kunde mehrere Ausleihen hat. Ein Kunde kann mehrere Ausleihen haben.</p> <p>falsch. Film Ausleihe ist 1-mc. Das heisst: 1 Film kann sich in 0,1 oder mehreren Ausleihen befinden. 1 Ausleihe hat 1 Film. Ob das sinnvoll ist, ist eine andere Frage. Intuitiv w\u00fcrde man wohl annehmen, dass eine Ausleihe auch mehrere Filme beinhalten kann. Die Darstellung im ERM besagt jedoch, dass eine Ausleihe genau ein Film beinhaltet.     </p> <p>wahr. Ja, wenn diese in keiner Ausleihe vorhanden sind. Beziehung Film - Ausleihe ist 1-mc. c besagt, dass ein Film auch in 0 Ausleihen vorhanden sein kann.    </p> <p>falsch. Beziehung Ausleihe-Kunde ist m-1. Das heisst 1 Ausleihe geh\u00f6rt zu einem Kunden und 1 Kunde hat m Ausleihen. Wenn ein Kunde keine Ausleihen haben darf, dann m\u00fcsste die Beziehung mc-1 sein. Diese Beziehung heisst also, dass nur ein Kunde ein Kunde ist, wenn er eine Ausleihe hat.   </p> <p>wahr. Die Zeitperiode ist hier irrelevant, da im ERM gar nicht ber\u00fccksichtigt. Relevant ist nur die Tatsache, dass ein Kunde mehrere Ausleihen haben kann.</p> <p>wahr. Der Kreis bei Ausleihe sagt, dass auch 0 eingeschlossen ist. Also: 0, 1 oder mehrere Ausleihen k\u00f6nnen einen Film beinhalten. Oder: 1 Film ist in keiner, einer oder mehreren Ausleihen vorzufinden.    </p> <p>wahr. Das Symbol bei Ausleihe besagt 1 oder mehrere. F\u00fcr \"mehrere\" wird 1 auch immer eingeschlossen.</p>"},{"location":"le06/ue06-02/#aufgabe-3-normalformen-aufgabe-als-repetition","title":"Aufgabe 3: Normalformen - Aufgabe als Repetition","text":"<p>Aufgabe 3</p> <p>Welche Normalformen sind hier verletzt? Erstellen Sie daaraus Tabellen in 3NF!</p> idStudent Student idModul Modulname Note 98712 John Scofield 2200 Programmieren A 98712 John Scofield 2201 Datenbanken C 123456 Heidi Scholl 2201 Datenbanken E 123456 Heidi Scholl 2257 Networking B 123456 Heidi Scholl 2210 Security D 52478 Adrian Sieber 2201 Datenbanken A <p>Antworten</p> <p>Erste \u00dcberlegung:</p> <p>Was sind die Schl\u00fcsselmerkmale? </p> <p>id_Student und idFachModul definieren einen Record eindeutig.</p> <p>Also:</p> idStudent Student idModul Modulname Note <p>Pr\u00fcfung der NF:</p> <p>1NF ist verletzt, da die Wertebereiche des Merkmals Student nicht atomar sind. Hier besteht das Risiko von Anomalien, wenn ein Name unterschiedlich geschrieben wird. </p> <p>2NF ist verletzt, weil 1NF verletzt ist.</p> <p>Wenn wir nun 1NF korrigieren wollen, ist es naheliegend die Merkmale Student_Name und Student_Vorname zu verwenden. Damit haben wir 1NF erf\u00fcllt.</p> <p>Ist dann 2NF erf\u00fcllt?</p> <p>Die 2NF ist nicht erf\u00fcllt, da nicht alle Nichtschl\u00fcsselmerkmale voll funktional abh\u00e4ngig sind vom zusammengesetzten Schl\u00fcssel der Tabelle: Student ist nur funktional abh\u00e4ngig von id_Student, Modulname ist funktional abh\u00e4ngig nur von idFachModul. </p> <p>Das Merkmal \"Note\" ist ein Beziehungsmerkmal zwischen Student und Modul. </p> <p>Grafisch kann man die Beziehung so darstellen:</p> <p> ER-Modell Student-Modul-Note (erstellt mit draw.io) </p> <p>Es handelt sich hier um eine m-m-Beziehung zwischen Modul und Student: 1 Student belegt mehrere Module und 1 Modul kann von mehreren Studenten belegt werden.</p> <p>Wie wir wissen, bedingt eine m-m Beziehung eine Beziehungstabelle.</p> <p>Nach den \u00dcBERF\u00dcHRUNGSREGELN  entstehen aus der Ursprungstabelle somit 3 Tabellen:</p> <ul> <li>Student (idStudent, Name, Vorname)</li> <li>Modul (idModul, Modulname)</li> <li>Student_Modul (id_Student, idModul, Note)</li> </ul> <p>Wir haben hier KEINE transitive Abh\u00e4ngigkeiten, daher befinden sich diese Tabellen in 3NF.</p> <p>BEACHTEN SIE HIER DIE SCHREIBWEISE VON TABELLEN: </p> <p>tabellenname(merkmal1,merkmal2,...)</p> <p>Unterstrichene Merkmale (Attribute) bezeichnen Schl\u00fcsselmerkmale.</p>"},{"location":"le06/ue06-02/#aufgabe-4-erstellung-physisches-datenmodell","title":"Aufgabe 4: Erstellung physisches Datenmodell","text":"<p>Aufgabe 4</p> <p>Erstellen Sie das physische Datenmodell in 3NF.</p> <p>Antwort</p> <p>Die L\u00f6sung dazu ist analog \u00dcbung UE06-01, Aufgabe 5. \u00dcben Sie sich im Handling mit MySQL und f\u00fchren Sie die SQL-Befehle zum Erstellen und Bef\u00fcllen von Tabellen, inkl. dem Definieren von PK und FK.</p> <p>Als Basis dazu verwenden Sie das logische ER-Modell, welches sich auch so beschreiben l\u00e4sst, wie in vorhergehender Aufgabe dargestellt:</p> <ul> <li>Student (idStudent, Name, Vorname)</li> <li>Modul (idModul, Modulname)</li> <li>Student_Modul (id_Student, idModul, Note)</li> </ul>"},{"location":"le06/ue06-03/","title":"UE06-03-Schuldatenbank","text":""},{"location":"le06/ue06-03/#3nf-herstellen","title":"3NF herstellen","text":"<p>Aufgabe 1</p> <p>An einer Schule soll es in Zukunft eine zentralisierte Datenablage geben. Folgende Anforderungen gelten f\u00fcr die zu erstellende Datenbank:</p> <ul> <li>Ein Sch\u00fcler hat jeweils einen Vor- und Nachnamen</li> <li>Die Lehrperson hat einen Vor- und Nachnamen und unterrichtet mindestens ein Fach, oder mehrere F\u00e4cher</li> <li>Eine Klasse hat einen Namen und beliebig viele Sch\u00fcler</li> <li>Pro Fach kann ein Sch\u00fcler jeweils eine numerische Note von 1-6 erzielen (eine Kommanstelle Pr\u00e4zision reicht) </li> </ul> <p>Versuche die n\u00f6tigen Tabellen in der 3ten Normalform (3NF) zu entwickeln. Gehe dabei methodisch vor: siehe Folien auf Moodle.</p>"},{"location":"le06/ue06-03/#konzeptionelles-und-logisches-erm-erstellen","title":"Konzeptionelles und logisches ERM erstellen","text":"<p>Aufgabe 2</p> <p>Erstelle aus den Angaben von Aufgabe 1 ein konzeptionelles und ein logisches ERM.  Diese Aufgabe kannst du auch l\u00f6sen, wenn die vorhergehende Aufgabe noch nicht gel\u00f6st wurde. Gehe auch hier methodisch vor und nimm die PPT-Folien als Hilfe.</p>"},{"location":"le06/ue06-03/#physische-datenbank-aus-logischem-erm-erstellen","title":"Physische Datenbank aus logischem ERM erstellen","text":"<p>Aufgabe 3</p> <p>In einem weiteren Schritt sollst Du die Datenbank mittels SQL umsetzen. F\u00fclle die Datenbank mit einer gen\u00fcgend grossen Anzahl Testdaten und versuche mehrere Tabellen gleichzeitig abzufragen. Diesen Schritt nennt man auch physisches Datenbankmodell. Hierbei soll dir das logische ERM als Basis dienen.</p> <p>Erstelle Queries f\u00fcr folgende Fragestellungen:</p> <ul> <li>Was sind die Namen der Lehrer die eine bestimmte Klasse unterrichten?</li> <li>Welchen Notendurchschnitt hat ein bestimmter Sch\u00fcler?</li> <li>Welches Fach hat am meisten Sch\u00fcler?</li> </ul> L\u00f6sungsvorschlag <p>Vorgehen konzeptionelle ERM:</p> <ol> <li>Das konzeptionelle ERM erstellen wir, indem wir die Entit\u00e4ten (Entit\u00e4tsmengen) als erses bestimmen. Entit\u00e4ten besitzen immer mehrere gleichartige Elemente, wie z. Beispiel Lehrer, Sch\u00fcler, Klassen, etc. Meist sind es Nomen in einer textuellen Beschreibung einer fachlichen Anforderung!</li> <li>Bestimmung der Beziehungen zwischen den Entit\u00e4ten. Beschreiben Sie die Beziehung.</li> <li>Attribute (Merkmale) der Entit\u00e4ten k\u00f6nnen wir auch gleich notieren und im ERM darstellen</li> <li>Bestimmung der Kardinalit\u00e4ten (Anzahlangaben) zwischen den Entit\u00e4ten. Gehen Sie dabei so vor, wie im PPT beschrieben: 1:m, m:m, etc</li> </ol> <p>Dies f\u00fchrt uns zu folgendem ERM:</p> <p> konzeptionelles ERM zur Aufgabenstellung </p> <p>Beachte die Kardinalit\u00e4ten: </p> <p>Die Umsetzung von m-m-Beziehungen bedingt eine Zusatztabelle, die sog. Verbindungstabelle.</p> <p>Wir haben hier zwei m-m-Beziehungen: * Schueler-Fach: 1 Sch\u00fcler belegt mehrere F\u00e4cher und ein Fach wird von mehreren Sch\u00fclern belegt * Fach-Lehrer: 1 Fach kann von mehreren Lehrern unterrichtet werden und 1 Lehrer kann mehrere F\u00e4cher unterrichten.</p> <p>Das heisst, wir m\u00fcssen zus\u00e4tzlich 2 Verbinduchstabellen erstellen: * Schueler_Fach * Fach_Lehrer</p> <p>Die Beziehung Schueler-Fach hat zudem ein Merkmal \"Note\". Dieses Merkmal m\u00fcssen wir in der Verbindungstabelle Schueler_Fach ber\u00fccksichtigen.</p> <p>Vorgehen logisches ERM:</p> <p>Das logische ERM ist die Basis f\u00fcr das physische ERM. Es sollte daher komplett sein und auch die Verbindungstabellen ber\u00fccksichtigen. Komplett heisst: alle Merkmale, Verbindiungstabellen, Schl\u00fcssel (PK und FK) inkl. Tabellen- und Attributnamen sind gekl\u00e4rt.</p> <p>Logisches ERM als textualer Beschreibung:</p> <ul> <li>Schueler(idSchueler, NameSchueler, VornameSchueler, idKlasse (FK))<ul> <li>\\(\\rightarrow\\) der ForeignKey ist immer in der Tabelle, welche mit m bezeichnet ist! Siehe konzeptionelles ERM oben.</li> </ul> </li> <li>Klasse(idKlasse, KlasseName)</li> <li>Fach(idFach, Fachname)</li> <li>Schueler_Fach(idSchueler,idFach,Note)<ul> <li>\\(\\rightarrow\\) Verbindungstabelle mit Merkmal Note </li> </ul> </li> <li>Lehrer(idLehrer, NameLehrer, VornameLehrer)</li> <li>Fach_Lehrer(idFach, idLehrer)</li> </ul> <p>Logisches ERM in Tabellenform mit Beispieldaten</p> <p> </p> Datenbank mit Tabellen erstellen<pre><code>CREATE DATABASE SchulDB;\n\nUSE SchulDB;\n\nCREATE TABLE Klasse (\n  idKlasse int NOT NULL,\n  KlasseName varchar(45) DEFAULT NULL,\n  PRIMARY KEY (idKlasse)\n);\n\n\nCREATE TABLE Schueler (\n  idSchueler int NOT NULL,\n  NameSchueler varchar(45) DEFAULT NULL,\n  VornameSchueler varchar(45) DEFAULT NULL,\n  idKlasse int  NULL,\n  PRIMARY KEY (idSchueler),\n  FOREIGN KEY (idKlasse) REFERENCES Klasse (idKlasse)\n);\n\nCREATE TABLE  Fach  (\n   idFach int NOT NULL,\n   Fachname  varchar(45) DEFAULT NULL,\n  PRIMARY KEY ( idFach )\n);\n\nCREATE TABLE Schueler_Fach (\n  idSchueler int NOT NULL,\n  idFach int NOT NULL,\n  Note DECIMAL(10,1) NULL,\n  PRIMARY KEY (idSchueler,idFach),\n  FOREIGN KEY (idSchueler) REFERENCES Schueler (idSchueler),\n  FOREIGN KEY (idFach) REFERENCES Fach (idFach)\n);\n\n\n\n\nCREATE TABLE Lehrer (\n  idLehrer int NOT NULL,\n  NameLehrer varchar(45) DEFAULT NULL,\n  VornameLehrer varchar(45) DEFAULT NULL,\n  PRIMARY KEY (idLehrer)\n);\n\nCREATE TABLE Fach_Lehrer (\n  idFach INT NOT NULL,\n  idLehrer INT NOT NULL,\n  PRIMARY KEY (idFach, idLehrer),\n  FOREIGN KEY (idFach) REFERENCES Fach (idFach),\n  FOREIGN KEY (idLehrer) REFERENCES Lehrer (idLehrer)\n);\n</code></pre> Tabellen mit Beispieldaten f\u00fcllen<pre><code>-- Tabelle Fach\nINSERT INTO `SchulDB`.`Fach` (`idFach`, `Fachname`) VALUES ('1', 'Datenbanken');\nINSERT INTO `SchulDB`.`Fach` (`idFach`, `Fachname`) VALUES ('2', 'CloudComputing');\n\n-- Tabelle Klasse\nINSERT INTO `SchulDB`.`Klasse` (`idKlasse`, `KlasseName`) VALUES ('1', 'KlasseA');\nINSERT INTO `SchulDB`.`Klasse` (`idKlasse`, `KlasseName`) VALUES ('2', 'KlasseB');\nINSERT INTO `SchulDB`.`Klasse` (`idKlasse`, `KlasseName`) VALUES ('3', 'KlasseC');\n\n-- Tabelle Schueler\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('1', 'M\u00fcller', 'Hans', '1');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('2', 'Ineichen', 'Kurt', '2');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('3', 'Gerber', 'Robin', '3');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('4', 'Lanz', 'Paul', '1');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('5', 'Aebischer', 'Thomas', '3');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('6', 'Burki', 'Beat', '2');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('7', 'Sterchi', 'Mireille', '3');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('8', 'Frieden', 'Ana\u00efs', '3');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('9', 'Berger', 'Florence', '1');\nINSERT INTO `SchulDB`.`Schueler` (`idSchueler`, `NameSchueler`, `VornameSchueler`, `idKlasse`) VALUES ('10', 'Illy', 'Sabine', '2');\n\n-- Tabelle Lehrer\nINSERT INTO `SchulDB`.`Lehrer` (`idLehrer`, `NameLehrer`, `VornameLehrer`) VALUES ('1', 'Luginb\u00fchl', 'Tim');\nINSERT INTO `SchulDB`.`Lehrer` (`idLehrer`, `NameLehrer`, `VornameLehrer`) VALUES ('2', 'J\u00e4ggi', 'Thomas');\n\n-- Tabelle Schueler_Fach\n\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('1', '1', '4.8');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('2', '1', '5.1');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('3', '2', '6');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('4', '2', '5.0');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('5', '1', '4.2');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('6', '2', '3.6');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('7', '2', '2.7');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('8', '1', '4.8');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('9', '2', '5.5');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('10', '2', '5.4');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('1', '2', '4.3');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('2', '2', '4.1');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('3', '1', '4.0');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('4', '1', '4.6');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('5', '2', '5.2');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('6', '1', '4.6');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('7', '1', '5.7');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('8', '2', '5.8');\nINSERT INTO `SchulDB`.`Schueler_Fach` (`idSchueler`, `idFach`, `Note`) VALUES ('9', '1', '4.5');\n\n\n-- Tabelle Fach_Lehrer\nINSERT INTO `SchulDB`.`Fach_Lehrer` (`idFach`, `idLehrer`) VALUES ('1', '2');\nINSERT INTO `SchulDB`.`Fach_Lehrer` (`idFach`, `idLehrer`) VALUES ('2', '1');\n</code></pre> Query: Was sind die Namen der Lehrer die eine bestimmte Klasse unterrichten?<pre><code>-- Was sind die Namen der Lehrer die eine bestimmte Klasse unterrichten?\n-- alle Daten abfragen\nSELECT * FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\n WHERE Schueler.idKlasse = Klasse.idKlasse\n AND Schueler_Fach.idSchueler = Schueler.idSchueler\n AND Fach.idFach =  Schueler_Fach.idFach\n AND Fach_Lehrer.idFach = Schueler_Fach.idFach\n AND Fach_Lehrer.idLehrer = Lehrer.idLehrer\n\n -- alle Sch\u00fcler, welche in KlasseA unterrichtet werden\n SELECT * FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\n WHERE Schueler.idKlasse = Klasse.idKlasse\n AND Schueler_Fach.idSchueler = Schueler.idSchueler\n AND Fach.idFach =  Schueler_Fach.idFach\n AND Fach_Lehrer.idFach = Schueler_Fach.idFach\n AND Fach_Lehrer.idLehrer = Lehrer.idLehrer\n AND Klasse.KlasseName = \"KlasseA\"\n\n -- alle Lehrer, welche in KlasseA unterrichten. FINALES ERGEBNIS\n SELECT DISTINCT Lehrer.NameLehrer, Lehrer.VornameLehrer FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\n WHERE Schueler.idKlasse = Klasse.idKlasse\n AND Schueler_Fach.idSchueler = Schueler.idSchueler\n AND Fach.idFach =  Schueler_Fach.idFach\n AND Fach_Lehrer.idFach = Schueler_Fach.idFach\n AND Fach_Lehrer.idLehrer = Lehrer.idLehrer\n AND Klasse.KlasseName = \"KlasseA\"\n</code></pre> Query: Welchen Notendurchschnitt hat ein bestimmter Sch\u00fcler?<pre><code>SELECT * FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\nWHERE Schueler.idKlasse = Klasse.idKlasse\nAND Schueler_Fach.idSchueler = Schueler.idSchueler\nAND Fach.idFach =  Schueler_Fach.idFach\nAND Fach_Lehrer.idFach = Schueler_Fach.idFach\nAND Fach_Lehrer.idLehrer = Lehrer.idLehrer\nAND Schueler.NameSchueler = \"Sterchi\"\n\n-- Resultat f\u00fcr Sterchi: Noten 2.7 und 5.7. --&gt; Durchschnitt ist 4.2\n\n-- Welchen Notendurchschnitt hat Sterchi? FINALES ERGEBNIS:\nSELECT Schueler.NameSchueler, AVG(Note) AS AVG_Modules FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\nWHERE Schueler.idKlasse = Klasse.idKlasse\nAND Schueler_Fach.idSchueler = Schueler.idSchueler\nAND Fach.idFach =  Schueler_Fach.idFach\nAND Fach_Lehrer.idFach = Schueler_Fach.idFach\nAND Fach_Lehrer.idLehrer = Lehrer.idLehrer\nAND Schueler.NameSchueler = \"Sterchi\"\n-- Resultat f\u00fcr Sterch: 4.2  Stimmt!\n</code></pre> Query: Welches Fach hat am meisten Sch\u00fcler?<pre><code>SELECT * FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\n WHERE Schueler.idKlasse = Klasse.idKlasse\n AND Schueler_Fach.idSchueler = Schueler.idSchueler\n AND Fach.idFach =  Schueler_Fach.idFach\n AND Fach_Lehrer.idFach = Schueler_Fach.idFach\n AND Fach_Lehrer.idLehrer = Lehrer.idLehrer\n ORDER BY Fach.idFach\n\n -- GROUP BY Fach mit COUNT() mit Sortierung DESC und Anzeige des h\u00f6chsten Wertes mit LIMIT 1\n SELECT Fach.Fachname, COUNT(Fach.idFach) AS Anzahl_Schueler FROM Schueler, Klasse, Schueler_Fach,Fach,Fach_Lehrer,Lehrer\n WHERE Schueler.idKlasse = Klasse.idKlasse\n AND Schueler_Fach.idSchueler = Schueler.idSchueler\n AND Fach.idFach =  Schueler_Fach.idFach\n AND Fach_Lehrer.idFach = Schueler_Fach.idFach\n AND Fach_Lehrer.idLehrer = Lehrer.idLehrer\n GROUP BY Fach.idFach\n ORDER BY Anzahl_Schueler DESC\n -- LIMIT 1\n</code></pre>"},{"location":"le07/","title":"LE07 - Repetitionslektion vor Pr\u00fcfung","text":""},{"location":"le07/ue07-01/","title":"UE07-01 \u00dcbungen","text":""},{"location":"le07/ue07-01/#1-fehlerhafte-tabellen","title":"1. Fehlerhafte Tabellen","text":"<p>In einem Unternehmen sollen der Name, die Adresse, das Alter und der direkte Vorgesetzte des jeweiligen Mitarbeiters in einer Tabelle gespeichert werden.</p> Name Adresse Alter Vorgesetzter Scofield, John Kirchweg 4, 2500 Biel 44 Hankok, Herbie Hof 7, 8000 Z\u00fcrich 21 Scofiled, John Starr, Ringo Rosenstrasse 1, 4566 Halten 24 Scofield, John John,Elton am Platz 3, 7000 Luzern 31 <p>Aus dieser Tabelle ist beispielsweise erkennbar, dass 'Scofield John' der direkte Vorgesetze der beiden Mitarbeiter 'Hankok Herbie' und 'Starr Ringo' ist.</p> <ol> <li> <p>Warum ist diese Tabelle f\u00fcr die Umsetzung auf einem relationalen Datenbanksystem ungeeignet?</p> </li> <li> <p>Zu welchen Komplikationen kann es bei der Verwendung dieser Tabelle kommen?</p> </li> </ol> Antworten 1.2. <ul> <li>Weil die Tabelle kein eindeutiges Schl\u00fcsselmerkmal (und keinen Schl\u00fcssel) besitzt.</li> <li>Weil einige Tabellenspalten mehrere unterschiedliche Informationen enthalten. (z. B. Name anstelle von Vorname und Nachname oder Adresse f\u00fcr Strasse, PLZ, Ort)</li> <li>Die Tabelle enth\u00e4lt Informationen, die st\u00e4ndigen \u00c4nderungen unterworfen sind. (z. B. Alter anstelle von Geburtsdatum)</li> </ul> <ul> <li>Ein Datensatz kann nicht (oder nur unzureichend) eindeutig identifiziert werden. </li> <li>Es ist nicht immer m\u00f6glich zwischen Vorname und Nachname eindeutig zu unterscheiden, da in der Spalte 'Name' eine saubere und eindeutige Trennung zwischen Vor- und Nachname nicht enthalten ist.</li> <li>Nach jedem Geburtstag eines Mitarbeiters muss das entsprechende Alter angepasst werden.</li> </ul>"},{"location":"le07/ue07-01/#2-schlussel-und-fremdschlussel","title":"2. Schl\u00fcssel  und Fremdschl\u00fcssel","text":"<ol> <li>Welche beiden Eigenschaften muss ein Schl\u00fcsselmerkmal erf\u00fcllen?</li> <li>Welcher Zusammenhang besteht zwischen einem Fremdschl\u00fcssel und einem zugeh\u00f6rigen Schl\u00fcsselmerkmal?</li> <li>Begr\u00fcnden Sie, ob die Werte eines Fremdschl\u00fcssels eindeutig sein m\u00fcssen oder nicht?</li> <li> <p>Beschreiben Sie die Bedeutung der Merkmale einer Beziehungstabelle.         a. Welche Merkmale sind Fremdschl\u00fcssel?</p> <p>b. Welche Merkmale geh\u00f6ren zum Schl\u00fcssel?</p> </li> </ol> Antworten 1.2.3.4. <p>Ein Schl\u00fcssel muss folgende beiden Bedingungen erf\u00fcllen:</p> <ul> <li>Eindeutigkeit: Ein Schl\u00fcssel identifiziert eindeutig die Datens\u00e4tze der Tabelle.</li> <li>Minimal: Wird ein Schl\u00fcssel aus mehreren Merkmalen kombiniert, d\u00fcrfen nicht mehr Merkmale als unbedingt notwendig an der Kombination beteiligt werden.</li> </ul> <p>Als Fremdschl\u00fcssel einer Tabelle wird ein Merkmal oder eine Kombination von Merkmalen bezeichnet, die in einer anderen Tabelle als Schl\u00fcssel vorkommen. Mithilfe von Fremdschl\u00fcsseln werden die Beziehungen zwischen Tabellen realisiert.</p> <ul> <li>Die Werte m\u00fcssen nicht eindeutig sein!</li> <li>Beispiel: Betrachten Sie die Tabellen 'Klasse' und 'Sch\u00fcler'. Um die Beziehung 'ist Mitsch\u00fcler' umzusetzen, muss das Merkmal 'ID_Klasse' der Tabelle 'Sch\u00fcler' als Fremdschl\u00fcssel hinzugef\u00fcgt werden. Da aber mehrere Sch\u00fcler in dieselbe Klasse gehen, wird auch derselbe ID-Wert im Fremdschl\u00fcsselmerkmal 'ID_Klasse' mehrfach auftauchen.</li> </ul> <p>Zusammengesetzter Schl\u00fcssel:</p> <p>Der Schl\u00fcssel der Beziehungstabelle wird aus der Kombination der Fremdschl\u00fcssel gebildet. Die einzelnen Merkmale, aus denen sich der Schl\u00fcssel zusammensetzt, werden auch als Teilschl\u00fcssel bezeichnet.</p>"},{"location":"le07/ue07-01/#3-er-modell-miethaus","title":"3. ER-Modell Miethaus","text":"<p>Erstellen Sie das ER-Modell zu der folgenden Datenbankbeschreibung mit Anforderungsliste. Der Vermieter eines Mietshauses m\u00f6chte eine Datenbank erstellen lassen, in der er Informationen zu seinem Mietshaus ablegen kann.</p> <p>Anforderungsliste:</p> <ol> <li>Welche Wohnungen gibt es in dem Mietshaus?</li> <li>Zu der Wohnung sollen die Gr\u00f6sse in m\u00b2, die Anzahl der Personen, f\u00fcr die die Wohnung ausgelegt ist und die Angabe der Etage (EG, 1. OG, 2. OG) gespeichert werden.</li> <li>Welche Personen leben in dem Mietshaus?</li> <li>Welche Personen wohnen in den jeweiligen Wohnungen?</li> <li>Es k\u00f6nnen mehrere Personen in einer Wohnung wohnen.</li> <li>Eine Person kann nicht gleichzeitig in mehreren Wohnungen wohnen.</li> <li>Wohnungen werden von Personen gemietet.</li> <li>In jeder Wohnung gibt es eine Person, die als Ansprechpartner bzw. als \u201eoffizieller\u201c Mieter auftritt.</li> <li>Eine Person kann nicht gleichzeitig Mieter von mehreren Wohnungen sein.</li> <li>Zu den Personen sollen der Vorname, der Nachname und das Geburtsdatum erfasst werden.</li> </ol> <p>Augaben:</p> <ol> <li>Welche Entit\u00e4ten entnehmen Sie der Anforderungsliste?</li> <li>Welche Beziehungen entnehmen Sie der Anforderungsliste?</li> <li>Erstellen Sie ein konzeptionelles ER-Modell mit Kardinalit\u00e4ten und Merkmalenin Form einer Skizze!</li> <li> <p>Erstellen Sie mithilfe der \u00dcberf\u00fchrungsregeln die zugeh\u00f6rigen Tabellen mit folgenden Datens\u00e4tzen:</p> <p>Erdgeschoss:</p> <p>Die Wohnung im Erdgeschoss hat 120 m\u00b2 und ist f\u00fcr maximal 4 Personen ausgelegt. Sie wurde von Heinz Neumann (geb. 15.04.1967) f\u00fcr sich und seine Familie, Gerda Neumann (geb. 08.10.69) und die Kinder Tim (geb. 02.04.1995) und Hanna (geb. 05.08.1997) ab dem 01.01.95 gemietet.</p> <p>Erstes Obergeschoss:</p> <p>Das erste Obergeschoss (1. OG) hat 70 m\u00b2 und kann maximal von 3 Personen genutzt werden. Es wird von dem Ehepaar Werner (geb. 24.5.1937) und Monika Specht (geb. 06.08.1940) seit dem 01.05.80 bewohnt. Weil den Spechts die Familie Neumann im EG zu laut ist, hat Werner Specht (Mieter) die Wohnung zum 01.01.2012 gek\u00fcndigt.</p> <p>Zweites Obergeschoss:</p> <p>Die Wohnung im zweiten Obergeschoss (2. OG) hat 55 m\u00b2 und bietet Platz f\u00fcr maximal 2 Personen. Sie steht zur Zeit leer.</p> </li> </ol> Antworten 1.2.3.4. <p>Entit\u00e4ten: Wohnung, Person</p> <p>Beziehungen: wohnen, mieten</p> <p> konzeptionelles ER-Modell </p> <p>Es werden nur 2 Tabellen ben\u00f6tigt (2 Entit\u00e4ten; keine m:m-Beziehung, daher keine Verbindungstabelle.).</p> <p> Der Fremdschl\u00fcssel 'ID_Person' realisiert die Beziehung 'mietet' </p> <p> Der Fremdschl\u00fcssel 'ID_Wohnung' realisiert die Beziehung 'wohnen' </p>"},{"location":"le07/ue07-01/#4-er-modell-grossere-schuldatenbank","title":"4. ER-Modell gr\u00f6ssere Schuldatenbank","text":"<p>F\u00fcr eine Berufsschule soll eine Datenbank erstellt werden. Mit der Schulleitung und dem Fachpersonal, das sp\u00e4ter mit der Datenbank arbeiten soll, sind erste Gespr\u00e4che gef\u00fchrt worden, aus denen die folgende Anforderungsliste entstanden ist. In dieser Aufgabe wird eine Anforderungsliste vorgegeben, die in Bezug auf die Anzahlangaben absichtlich nicht vollst\u00e4ndig ist.</p> <p>Beim L\u00f6sen der Aufgabe gehen Sie bitte wie folgt vor:</p> <ol> <li>Erstellen Sie das ER-Modell zur gegebenen Anforderungsliste.</li> <li>Erg\u00e4nzen Sie die Anzahlangaben sinnvoll.</li> <li>Erzeugen Sie die Tabellen mithilfe des ER-Modells.</li> </ol> <p>Anforderungsliste:</p> <ol> <li>Welche Sch\u00fcler gibt es an der Schule?</li> <li>Der Name, der Vorname, die Adresse sowie das Geburtsdatum und der bisher erreichte schulische Abschluss m\u00fcssen als Informationen f\u00fcr einen Sch\u00fcler gespeichert werden.</li> <li>Welche Klassen gibt es an der Schule?</li> <li>Welche \u00c4mter werden an der Schule ausge\u00fcbt?</li> <li>Unter einem Amt versteht man zus\u00e4tzliche Aufgaben wie Klassenlehrer, Mentor, etc.</li> <li>F\u00fcr \u00c4mter und auch f\u00fcr Klassen gen\u00fcgt die Angabe einer Bezeichnung als Information.</li> <li>Welche Sch\u00fcler sind Mitsch\u00fcler welcher Klasse?</li> <li>Eine Klasse kann in mehrere feste Laborgruppen aufgeteilt werden. </li> <li>Die Information welcher Laborgruppe ein Sch\u00fcler zugeteilt worden ist, soll ebenfalls in der Datenbank hinterlegt werden. Die Laborgruppe kann somt als einfaches Merkmal eines Sch\u00fclers einer Klasse definiert werden.</li> <li>Welcher Sch\u00fcler ist Klassensprecher einer Klasse?</li> <li>Welche F\u00e4cher werden an der Schule unterrichtet?</li> <li>F\u00fcr das Fach muss eine Fachbezeichnung und das Fachk\u00fcrzel in der Datenbank gespeichert werden.</li> <li>Welche Lehrer gibt es an der Schule?</li> <li>Als Angaben zu einem Lehrer sind der Vor- und Nachname, seine Privatadresse, das Lehrerk\u00fcrzel, sein Alter und das Dienstalter notwendig.</li> <li>Welcher Lehrer ist Klassenlehrer einer Klasse?</li> <li>Seit wann ist ein Lehrer Klassenlehrer einer Klasse?</li> <li>Ein Lehrer kann Klassenlehrer von mehreren Klassen sein. Aber eine Klasse hat immer nur einen Klassenlehrer.</li> <li>Welcher Lehrer lehrt welche F\u00e4cher?</li> <li>Welcher Lehrer unterrichtet in welcher Klasse welches Fach?</li> <li>Ein Fach wird in einer Klasse nur von einem Lehrer unterrichtet.</li> <li>Welcher Lehrer pr\u00fcft welchen Sch\u00fcler in welchem Fach?</li> <li>Ein Lehrer kann einen Sch\u00fcler evtl. in mehreren F\u00e4chern pr\u00fcfen.</li> <li>Ein Sch\u00fcler wird in einem Fach nur von einem Lehrer gepr\u00fcft.</li> <li>Ein Lehrer pr\u00fcft in einem Fach mehrere Sch\u00fcler.</li> <li>Mit welchen Noten pr\u00fcft der Lehrer einen Sch\u00fcler in einem Fach und an welchem Datum? Mit welcher Gewichtung (z. B. 3 = Arbeit, 1 = Test, \u2026) geht das Pr\u00fcfergebnis in die Endnote ein? Wie hat der Lehrer den Sch\u00fcler in dem Fach gepr\u00fcft (Art der Pr\u00fcfung: Arbeit, Test, ...)?</li> </ol> <p>Anmerkung: Mit Ausnahme der \u00c4mter, bei denen explizit gefordert wurde, dass nachvollziehbar bleibt, wann welcher Lehrer welches Amt \u00fcbernommen hat, sind keine Historiendaten in der Datenbank erforderlich. Beispielsweise ist nicht wichtig, wer in der Vergangenheit schon mal Klassenlehrer einer bestimmten Klasse war. Die Datenbank repr\u00e4sentiert abgesehen von den Besetzungen der \u00c4mter lediglich den aktuellen Zustand im laufenden Schuljahr.</p> <p>Eine Ortstabelle muss nicht erstellt werden. Die Adresse wird bei den Entit\u00e4ten gespeichert.</p> Antworten ER-Modell mit Kardinalit\u00e4tenTabellen <p> konzeptionelles ER-Modell </p> <p>Es werden 11 Tabellen ben\u00f6tigt (7 Entit\u00e4ten; 3 m:m-Beziehungen, eine Mehrfachbeziehung).  logisches ERM - Tabellen </p>"},{"location":"le08/","title":"LE08-Table-JOINs and Transactions","text":""},{"location":"le08/#demodatenbank-books-ppt","title":"Demodatenbank books (PPT)","text":"<p>Auf die beiliegende Demodatenbank wird in den PPT's auf Moodle referenziert. F\u00fcr die \u00dcbungen werden wir eine andere Datenbank verwenden.</p> <p>Zum Erstellen der DB \u00f6ffnen Sie in der  MySQL Workbench ein neues SQL-Tab und f\u00fchren Sie bibliothek.sql aus. Anschliessend <code>Refresh All</code> ausf\u00fchren, damit die DB sichtbar wird.</p> <p>Download bibliothek.sql</p> ERM der Bibliothek-Datenbank"},{"location":"le08/#demodatenbank-schuldb2-fur-ubungsaufgaben-ue08-xx","title":"Demodatenbank schuldb2 f\u00fcr \u00dcbungsaufgaben UE08-xx","text":"<p>Download schuldb2.sql</p> <p>Download ERM-schuldb2</p> ERM der schuldb2-Datenbank"},{"location":"le08/#literatur","title":"Literatur","text":""},{"location":"le08/#join-beispiele","title":"JOIN-Beispiele","text":"<p>Download JOINs.pdf</p>"},{"location":"le08/#subqueries-unterabfragen","title":"Subqueries - Unterabfragen","text":"<p>Download Unterabfragen-SubQueries.pdf</p>"},{"location":"le08/#transaktionen","title":"Transaktionen","text":"<p>Download Transaktionen.pdf</p>"},{"location":"le08/#ubungsdatenbank-zu-den-literaturhinweisen","title":"\u00dcbungsdatenbank zu den Literaturhinweisen","text":"<p>Download books.sql</p>"},{"location":"le08/ue08-01/","title":"UE08-01 \u00dcbungen zum INNER JOIN (EQUI-JOIN)","text":"<p>verwendete Datenbank: schuldb2</p>"},{"location":"le08/ue08-01/#frage-1","title":"Frage 1","text":"<p>Question</p> <p>Welche Aufgaben/\u00c4mter bekleiden die Lehrer? Lassen Sie sich den Nachnamen und das K\u00fcrzel des Lehrers und die Bezeichnung des Amtes ausgeben.</p> AntwortSQL <p> </p> <pre><code>SELECT L.Nachname, L.K\u00fcrzel, A.Bezeichnung\nFROM Lehrer AS L, Amt AS A, Aus\u00fcbung AS Au\nWHERE L.ID_Lehrer = Au.ID_Lehrer\nAND Au.ID_Amt = A.ID_Amt;\n</code></pre>"},{"location":"le08/ue08-01/#frage-2","title":"Frage 2","text":"<p>Question</p> <p>Lassen Sie sich den Klassenlehrer und den Klassensprecher der Klassen ausgeben.  Dabei sind der Nachname des Klassenlehrers sowie dessen K\u00fcrzel, der Nachname des Klassensprechers und die Bezeichnung der Klasse wichtig.</p> AntwortSQL <p> </p> <pre><code>SELECT L.Nachname, L.K\u00fcrzel, S.Nachname, K.Bezeichnung\nFROM Lehrer AS L, Sch\u00fcler AS S, Klasse AS K\nWHERE L.ID_Lehrer = K.ID_Lehrer\nAND S.ID_Sch\u00fcler = K.ID_Sch\u00fcler;\n</code></pre>"},{"location":"le08/ue08-01/#frage-3","title":"Frage 3","text":"<p>Question</p> <p>Ermitteln Sie, welcher Mitarbeiter eine Pr\u00fcfung in welchem Fach gestellt hat. Lassen Sie sich dazu das K\u00fcrzel des Faches und des Mitarbeiters sowie das Datum und die Gewichtung der Pr\u00fcfung anzeigen.</p> AntwortSQL <p> </p> <pre><code>SELECT F.K\u00fcrzel, L.K\u00fcrzel, P.Datum, P.Gewichtung\nFROM Fach AS F, Lehrer AS L, Pr\u00fcfung AS P\nWHERE L.ID_Lehrer = P.ID_Lehrer\nAND F.ID_Fach = P.ID_Fach;\n</code></pre>"},{"location":"le08/ue08-01/#frage-4","title":"Frage 4","text":"<p>Question</p> <p>Bestimmen Sie, welcher Lehrer in welcher Klasse welches Fach unterrichtet. Lassen Sie sich das Lehrer- und Fachk\u00fcrzel und die Klassenbezeichnung anzeigen.</p> AntwortSQL <p> </p> <pre><code>SELECT L.K\u00fcrzel, K.Bezeichnung, F.K\u00fcrzel\nFROM Lehrer AS L, Klasse AS K, Fach AS F, unterrichten AS U\nWHERE F.ID_Fach = U.ID_Fach\nAND K.ID_Klasse = U.ID_Klasse\nAND L.ID_Lehrer = U.ID_Lehrer;\n</code></pre>"},{"location":"le08/ue08-01/#frage-5","title":"Frage 5","text":"<p>Question</p> <p>Welcher Lehrer lehrt (nicht unterrichtet!) welches Fach? Lassen Sie sich den Nachnamen und das K\u00fcrzel des Lehrers sowie die Bezeichnung des Faches anzeigen.</p> AntwortSQL <p> </p> <pre><code>SELECT L.Nachname, L.K\u00fcrzel, F.Bezeichnung \nFROM Lehrer AS L, lehrt AS Le, Fach AS F \nWHERE L.ID_Lehrer = Le.ID_Lehrer\nAND F.ID_Fach = Le.ID_Fach;\n</code></pre>"},{"location":"le08/ue08-01/#frage-6-self-join","title":"Frage 6 - Self Join","text":"<p>Question</p> <p>Ermitteln Sie die K\u00fcrzel aller Lehrer, die ein Fach lehren, das Voraussetzung f\u00fcr ein anderes Fach ist. Lassen Sie sich das Lehrerk\u00fcrzel, aber auch die beiden Fachbezeichnungen ausgeben. (Zwei Referenzen auf die Tabelle 'Fach' sind notwendig!)</p> AntwortSQL <p> </p> <pre><code>SELECT L.K\u00fcrzel AS K\u00fcrzel, vor.Bezeichnung, nach.Bezeichnung \nFROM Lehrer AS L, lehrt AS le, Fach AS vor, Fach AS nach, vorraussetzen AS v \nWHERE L.ID_Lehrer = le.ID_Lehrer\nAND vor.ID_Fach = le.ID_Fach \nAND vor.ID_Fach = v.ID_Fach_ist_Vorraussetzung\nAND nach.ID_Fach = v.ID_Fach_hat_Vorraussetzung;\n</code></pre>"},{"location":"le08/ue08-02/","title":"UE08-02 \u00dcbungen zum LEFT- und RIGHT- JOIN","text":"<p>verwendete Datenbank: schuldb2</p>"},{"location":"le08/ue08-02/#frage-1","title":"Frage 1","text":"<p>Question</p> <p>Erstellen Sie eine vollst\u00e4ndige Sch\u00fclerliste mit Klassenzuteilung. Dabei sollen ALLE Sch\u00fcler in der Liste enthalten sein, auch diejenigen, die zurzeit noch keiner Klasse zugeordnet sind. </p> AntwortSQL <p> </p> <p>Es werden 19 Datens\u00e4tze angezeigt, inklusive des Sch\u00fclers 'Kurp, Udo'.</p> <pre><code>SELECT S.Nachname, K.Bezeichnung \nFROM Sch\u00fcler AS S LEFT JOIN Klasse AS K \nON S.ID_Klasse = K.ID_Klasse;\n</code></pre>"},{"location":"le08/ue08-02/#frage-2","title":"Frage 2","text":"<p>Question</p> <p>Ermitteln Sie, welcher Lehrer welches Fach lehrt. Bei dieser Abfrage sollen ALLE F\u00e4cher angezeigt werden, auch solche, f\u00fcr die zurzeit kein Lehrer eine Lehrbef\u00e4higung besitzt. Lassen Sie sich die Lehrer- und die entsprechenden Fachk\u00fcrzel ausgeben. </p> AntwortSQL <p>Es werden 25 Datens\u00e4tze angezeigt, inklusive der F\u00e4cher 'L' (Latein), 'GR' (Griechisch), und 'JAVA'.</p> <pre><code>SELECT L.K\u00fcrzel, F.K\u00fcrzel \nFROM Fach AS F\nLEFT JOIN lehrt AS Le ON F.ID_Fach = Le.ID_Fach\nLEFT JOIN Lehrer AS L ON L.ID_Lehrer = Le.ID_Lehrer;\n</code></pre>"},{"location":"le08/ue08-02/#frage-3","title":"Frage 3","text":"<p>Question</p> <p>Ermitteln Sie, welcher Lehrer welches Fach lehrt. Dabei sollen ALLE Lehrer angezeigt werden, auch solche, f\u00fcr die zurzeit noch keine Lehrbef\u00e4higung in der Datenbank eingetragen worden ist. </p> AntwortSQL <p>Es werden 23 Datens\u00e4tze angezeigt, inklusive des Lehrers 'SCK' (Schneider, Klaus)</p> <pre><code>SELECT L.K\u00fcrzel, F.K\u00fcrzel\nFROM Fach AS F\nRIGHT JOIN lehrt AS Le ON F.ID_Fach = Le.ID_Fach\nRIGHT JOIN Lehrer AS L ON L.ID_Lehrer = Le.ID_Lehrer;\n</code></pre>"},{"location":"le08/ue08-03/","title":"UE08-03 \u00dcbungen zu Unterabfragen","text":"<p>verwendete Datenbank: schuldb2</p>"},{"location":"le08/ue08-03/#frage-1","title":"Frage 1","text":"<p>Question</p> <p>Bestimmen Sie den Nachnamen und den Vornamen des \u00e4ltesten Sch\u00fclers.</p> AntwortSQL ohne VariableSQL mit Variable <p> </p> <pre><code>SELECT Nachname, Vorname \nFROM Sch\u00fcler\nWHERE Geburtsdatum = \n      ( SELECT MIN(Geburtsdatum)\n        FROM Sch\u00fcler\n        WHERE Geburtsdatum != 0000-00-00 );\n</code></pre> <pre><code>SELECT @geburtsdatum := MIN(Geburtsdatum)\nFROM Sch\u00fcler \nWHERE Geburtsdatum != 0000-00-00;\nSELECT Vorname, Nachname\nFROM Sch\u00fcler \nWHERE Geburtsdatum = @geburtsdatum;\n</code></pre>"},{"location":"le08/ue08-03/#frage-2","title":"Frage 2","text":"<p>Question</p> <p>Bestimmen Sie die Nachnamen aller Lehrer, die das Fach lehren (nicht unbedingt unterrichten!), das nach Fachk\u00fcrzel alphabetisch sortiert an letzter Stelle steht. Lassen Sie sich zus\u00e4tzlich das Fachk\u00fcrzel anzeigen.</p> AntwortSQL ohne VariableSQL mit Variable <p> </p> <pre><code>SELECT L.Nachname, L.Vorname, F.K\u00fcrzel\nFROM Lehrer AS L, lehrt AS Le, Fach AS F\nWHERE L.ID_Lehrer = Le.ID_Lehrer\nAND Le.ID_Fach = F.ID_Fach\nAND F.K\u00fcrzel =\n( SELECT MAX( K\u00fcrzel )\n  FROM Fach );\n</code></pre> <pre><code>SELECT @k\u00fcrzel := MAX( K\u00fcrzel )\nFROM Fach;\nSELECT L.Nachname, L.Vorname, F.K\u00fcrzel\nFROM Lehrer AS L, lehrt AS Le, Fach AS F\nWHERE L.ID_Lehrer = Le.ID_Lehrer\n  AND Le.ID_Fach = F.ID_Fach\n  AND F.K\u00fcrzel = @k\u00fcrzel;\n</code></pre>"},{"location":"le08/ue08-03/#frage-3","title":"Frage 3","text":"<p>Question</p> <p>Ermitteln Sie die Nachnamen und Vornamen der Sch\u00fcler, die \u00e4lter sind als das Durchschnittsalter aller  Sch\u00fcler der Klasse 'IF2C'.</p> AntwortSQL ohne VariableSQL mit Variable <p> </p> <pre><code>SELECT Nachname, Vorname\nFROM Sch\u00fcler\nWHERE Geburtsdatum != '0000-00-00'\n  AND Geburtsdatum &lt;\n      ( SELECT AVG( S.Geburtsdatum )\n        FROM Sch\u00fcler S, Klasse K\n        WHERE S.ID_Klasse = K.ID_Klasse\n          AND K.Bezeichnung = \"IF2C\"\n          AND Geburtsdatum != '0000-00-00' );\n</code></pre> <pre><code>SELECT @geburtsdatum := AVG( S.Geburtsdatum )\nFROM Sch\u00fcler S, Klasse K\nWHERE S.ID_Klasse = K.ID_Klasse\n  AND K.Bezeichnung = \"IF2C\"\n  AND Geburtsdatum != '0000-00-00';\n  SELECT Nachname, Vorname\n  FROM Sch\u00fcler\n  WHERE Geburtsdatum != '0000-00-00'\n    AND Geburtsdatum &lt; @geburtsdatum;\n</code></pre>"},{"location":"le08/ue08-03/#frage-4","title":"Frage 4","text":"<p>Question</p> <p>Bestimmen Sie die Vor- und Nachnamen und die Geburtsdaten aller Sch\u00fcler, die j\u00fcnger sind als alle Sch\u00fcler der Klasse 'IF2A'.</p> <ul> <li>Verwenden Sie einen 'Join', um auf die Klassenbezeichnung 'IF2A' abzupr\u00fcfen.</li> <li>Formulieren Sie die Abfrage mit der ALL- oder ANY- Anweisung.</li> <li>Wie kann diese SQL-Anfrage ohne ALL- oder ANY-Anweisung realisiert werden?</li> </ul> AntwortSQLSQL Alternative ohne All- oder ANY-Anweisung <p> </p> <pre><code>SELECT Vorname, Nachname, Geburtsdatum \nFROM Sch\u00fcler\nWHERE Geburtsdatum &gt; All\n      (SELECT S.Geburtsdatum\n       FROM Sch\u00fcler S, Klasse K \n       WHERE K.ID_Klasse = S.ID_Klasse \n         AND K.Bezeichnung = \"IF2A\");\n</code></pre> <pre><code>SELECT Vorname, Nachname, Geburtsdatum \nFROM Sch\u00fcler\nWHERE Geburtsdatum &gt; \n      (SELECT MAX(S.Geburtsdatum)\n       FROM Sch\u00fcler S, Klasse K \n       WHERE K.ID_Klasse = S.ID_Klasse \n         AND K.Bezeichnung = \"IF2A\");\n</code></pre>"},{"location":"le08/ue08-03/#frage-5","title":"Frage 5","text":"<p>Question</p> <p>Welcher Lehrernachname kommt auch als Nachname bei den Sch\u00fclern vor?</p> <ul> <li>Formulieren Sie die Abfrage mit der IN-Anweisung.</li> <li>Kann die Abfrage auch ohne die IN-Anweisung umgesetzt werden?</li> </ul> AntwortSQL <p> </p> <p><pre><code>SELECT Nachname \nFROM Lehrer \nWHERE Nachname IN\n( SELECT Nachname FROM Sch\u00fcler );\n</code></pre> Alternative Umsetzung mit <code>= ANY</code> m\u00f6glich</p>"},{"location":"le08/ue08-03/#frage-6","title":"Frage 6","text":"<p>Question</p> <p>Ermitteln Sie die Vor- und Nachnamen aller Lehrer, die ein Amt aus\u00fcben oder schon mal ausge\u00fcbt haben, das mit dem Buchstaben 'S' beginnt.</p> <ul> <li>Formulieren Sie die Abfrage mit der IN-Anweisung</li> <li>Formulieren Sie die Abfrage mit der ALL- oder der ANY- Anweisung.</li> <li>Wie kann die Abfrage ohne geschachtelte SQL-Abfrage umgesetzt werden?</li> </ul> AntwortSQL mit IN-StatementSQL-Alternative mit ANYSQL-Alternative ohne Unterabfrage <p> </p> <pre><code>SELECT L.Vorname, L.Nachname, A.Bezeichnung\nFROM Lehrer L, Amt A, Aus\u00fcbung Au\nWHERE L.ID_Lehrer = Au.ID_Lehrer\n  AND Au.ID_Amt = A.ID_Amt\n  AND A.ID_Amt IN\n      ( SELECT ID_Amt\n        FROM Amt\n        WHERE Bezeichnung LIKE \"S%\" );\n</code></pre> <pre><code>SELECT L.Vorname, L.Nachname, A.Bezeichnung\nFROM Lehrer L, Amt A, Aus\u00fcbung Au\nWHERE L.ID_Lehrer = Au.ID_Lehrer\n  AND Au.ID_Amt = A.ID_Amt\n  AND A.ID_Amt = ANY\n      ( SELECT ID_Amt\n        FROM Amt\n        WHERE Bezeichnung LIKE \"S%\" );\n</code></pre> <pre><code>SELECT L.Vorname, L.Nachname, A.Bezeichnung\nFROM Lehrer L, Amt A, Aus\u00fcbung Au\nWHERE L.ID_Lehrer = Au.ID_Lehrer\n  AND Au.ID_Amt = A.ID_Amt\n  AND A.Bezeichnung LIKE \"S%\";\n</code></pre>"},{"location":"le08/ue08-04/","title":"UE08-04 \u00dcbungen zu INSERT-SELECT-Abfragen (nice to know...)","text":"<p>verwendete Datenbank: schuldb2</p> <p>SQL-Abfragen k\u00f6nnen als Teil oder als Basis f\u00fcr weitere SQL-Anweisungen verwendet werden.</p> <ul> <li>Abfragen in Form von Unterabfragen k\u00f6nnen zu neuen Abfragen zusammengef\u00fchrt werden.</li> <li>Sie k\u00f6nnen auch als Basis f\u00fcr INSERT-SELECT-Abfragen oder</li> <li>f\u00fcr VIEW-Definitionen verwendet werden.</li> </ul> <p>Oft werden Unterabfragen im WHERE-Teil von SQL-Abfragen verwendet. Beachten Sie folgende Beispiele.</p>"},{"location":"le08/ue08-04/#frage-1","title":"Frage 1","text":"<p>Question</p> <p>Als weiteres Amt soll f\u00fcr jedes Unterrichtsfach ein Lehrer als Hauptansprechpartner benannt werden. Beispielsweise muss ein Sprecher der Fachschaft <code>Mathematik</code> ernannt werden. Deshalb lassen Sie mithilfe der INSERT-SELECT-Abfrage alle Fachbezeichnungen der Tabelle <code>Fach</code> automatisiert in die Tabelle <code>Amt</code> als Wert des Merkmals <code>Bezeichnung</code> eintragen.</p> AntwortSQL <p>Nach dem Ausf\u00fchren der Abfrage hat die Tabelle <code>Amt</code> 22 Datens\u00e4tze.</p> <p>ACHTUNG: funktioniert nur, wenn der Primary Key der Tabelle <code>Amt</code> automatisch generiert wird:</p> <p> \u00c4nderung mit <code>Alter Table</code> </p> <pre><code>INSERT INTO Amt( Bezeichnung )\nSELECT Bezeichnung FROM Fach;\n\nSELECT count(*) FROM Amt;\n</code></pre> <p> </p>"},{"location":"le09/","title":"LE09-Transaktion und INDEX","text":"<p>Wir bearbeiten \u00dcbungen zu INDIZES und Transaktionen</p>"},{"location":"le09/index-perf-messung/","title":"INDEX Perfomance Messung","text":"Speed mit INDEX! <p>Performancesteigerung mit Faktor 8000 !!!</p> <p>Du kannst mit beiliegendem Beispiel eine Performancesteigerung dank Index selber durchf\u00fchren. F\u00fchre beide SQLs, zuerst  <code>01-IDX_Messung_CreateDB_LoadDATA.sql</code> und dann <code>02-IDX_Messung_bank_bankneu.sql</code> in der MySQL-Workbench aus. Das 1. Skript kann einfach als Ganzes gestartet werden. Das 2. SQL-Skript ist kommentiert, beinhaltet auch neue Befehle f\u00fcr die Messung von Ausf\u00fchrungszeiten von SQL-Befehlen und soll Schritt f\u00fcr Schritt durchgef\u00fchrt werden.</p> <p>Im Beispiel:  SELECT Befehl dauert OHNE INDEX 43 Sekunden. Mit INDEX: 5.4 Millisekunden ! --&gt; Faktor 8000 ! </p> <p>Download 01-IDX_Messung_CreateDB_LoadDATA.sql</p> <p>Download 02-IDX_Messung_bank_bankneu.sql</p> <p>Aufgabe</p> <p>Welchen Performance-Faktor erreichen Sie bei 10'000'000 Records? Dazu im 2. SQL das <code>LIMIT</code> setzen. Ansonsten werden 380 Mio Records generiert. Meine Hardware hat es geschluckt. Ihre? Wenn Sie Zweifel haben, beginnen Sie zuerst mit 10 Mio. Das sollte bei allen funktionieren.</p>"},{"location":"le09/ue09-01/","title":"UE09-01 INDIZES","text":"B*-Baum <p>grossartige Sammlung von Insiderwissen zum Verhalten von Indizes bei g\u00e4ngigen DBMS.</p>"},{"location":"le09/ue09-01/#aufgabe","title":"Aufgabe","text":"<p>verwendete Datenbank f\u00fcr Aufgabe: schuldb2</p> <p>Aufgabe</p> <p>Bei der Datenanalyse (schuldb2) hat sich herausgestellt, dass viele SQL-Abfragen suchend auf das Merkmal 'Nachname' der Tabelle 'Sch\u00fcler' zugreifen.</p> <ul> <li>Erstelle deshalb einen Sekund\u00e4rindex auf dieses Merkmal. Verwende als Datenstruktur f\u00fcr diesen Index den B*-Baum.</li> <li>Kontrolliere den Index mit einem SQL-Befehl. Was f\u00e4llt Dir auf?</li> </ul> AntwortSQL INDEX kreierenSQL INDEX kontrollieren <pre><code>USE schuldb2;\n\nCREATE INDEX nachname_idx \nUSING BTREE\nON Sch\u00fcler(Nachname);\n</code></pre> <p><pre><code>USE schuldb2;\n\nSHOW INDEX FROM Sch\u00fcler;\n</code></pre> Beachte: f\u00fcr Schl\u00fcssel (PK und FK) wurde bereits ein Index erstellt. Dies wird vom RDBMS automatisch gemacht.</p> <p> Kontrolle Index </p>"},{"location":"le09/ue09-02/","title":"UE09-02 TRANSAKTIONEN","text":"START TRANSACTION.."},{"location":"le09/ue09-02/#aufgabe-1","title":"Aufgabe 1","text":"<p>verwendete Datenbank f\u00fcr Aufgabe: schuldb2</p> <p>Aufgabe</p> <p>Der Mathematiklehrer 'Udo Kurp' wurde versehentlich in die Sch\u00fclertabelle eingetragen. </p> <p> Fehler.. </p> <p>Um diesen Fehler zu korrigieren, muss der Datensatz zum einen aus der Sch\u00fclertabelle gel\u00f6scht und zum anderen in die Lehrertabelle aufgenommen werden. Um ganz sicher zu gehen, dass diese beiden Aktionen entweder komplett oder gar nicht ausgef\u00fchrt werden, gehen   Sie wie folgt vor:</p> <ol> <li> <p>Transaktion starten: Starten Sie zun\u00e4chst eine Transaktion.</p> </li> <li> <p>SQL-Abfragen: L\u00f6schen Sie nun den Datensatz aus der Tabelle 'Sch\u00fcler'. Kontrollieren Sie mit einem SQL, ob der Lehrer auch gel\u00f6scht ist. F\u00fcgen Sie anschliessend den Datensatz in die Tabelle 'Lehrer' ein. Kontrollieren Sie mit einem SQL, ob er in der Tabelle sichtbar ist.</p> </li> <li> <p>Transaktion beenden: Nehmen Sie nun die Transaktion ZUR\u00dcCK! Kontrollieren Sie, ob der Ausgangszustand wieder hergestellt worden ist. Ist also der Datensatz 'Udo Kurp' wieder in der Tabelle 'Sch\u00fcler' und aus der Lehrertabelle entfernt.</p> </li> </ol> <p>WICHTIG zu beachten: </p> <p>Die Tabellen m\u00fcssen von der Engine <code>InnoDB</code> verwaltet werden, damit die Transaktion korrekt abl\u00e4uft. Kontrolliere, ob dies der Fall ist mit dem Befehl: <code>SHOW TABLE STATUS;</code></p> <p>Falls die Tabellen nicht von der Engine <code>InnoDB</code> verwaltet werden, muss folgende \u00c4nderung gemacht werden: </p> <p><code>ALTER TABLE Lehrer ENGINE=InnoDB;</code></p> <p><code>ALTER TABLE Sch\u00fcler ENGINE=InnoDB;</code></p> <p>Danke C.. !   </p> Antwort1. Transaktion starten2. Datensatz l\u00f6schen und kontrollieren2. Datensatz in Lehrertabelle einf\u00fcgen und kontrollieren3. TRANSAKTION zur\u00fccknehmen3. Kontrolle Ausgangszustand <p>nicht gleich spicken! </p> <pre><code>USE schuldb2;\n\nSTART TRANSACTION;\n</code></pre> <pre><code>DELETE FROM Sch\u00fcler WHERE Nachname = 'Kurp' AND Vorname = 'Udo';\nSELECT * FROM schuldb2.Sch\u00fcler where Nachname = 'Kurp'\n</code></pre> <pre><code>INSERT INTO Lehrer( Vorname, Nachname ) VALUES ('Udo', 'Kurp');\nSELECT * FROM schuldb2.Lehrer where Nachname = 'Kurp'\n</code></pre> <pre><code>ROLLBACK;\n</code></pre> <pre><code>SELECT * FROM schuldb2.Sch\u00fcler where Nachname = 'Kurp'\nSELECT * FROM schuldb2.Lehrer where Nachname = 'Kurp'\n</code></pre>"},{"location":"le09/ue09-02/#aufgabe-2","title":"Aufgabe 2","text":"<p>verwendete Datenbank f\u00fcr Aufgabe: schuldb2</p> <p>Aufgabe</p> <p>Wie vorhergehende Aufgabe, mit dem Unterschied, dass Sie nun die Transaktion nicht zur\u00fccknehmen.</p> <ol> <li>Transaktion beenden: Beenden Sie nun die Transaktion, indem Sie sie definitv schreiben und somit die \u00c4nderung dauerhaft \u00fcbernehmen.</li> </ol> AntwortTransaktion starten und beendenKontrolle <p>nicht gleich spicken!  </p> <pre><code>USE schuldb2;\n\nSTART TRANSACTION;\nDELETE FROM Sch\u00fcler WHERE Nachname = 'Kurp' AND Vorname = 'Udo';\nINSERT INTO Lehrer( Vorname, Nachname ) VALUES ('Udo', 'Kurp');\nCOMMIT;\n</code></pre> <pre><code>SELECT * FROM Lehrer WHERE Nachname = 'Kurp';\n</code></pre>"},{"location":"le09/ue09-02/#variablen-in-sql","title":"Variablen in SQL","text":"<p>Bevor wir zur n\u00e4chsten Aufgabe schreiten, lernen Sie bitte folgendes SQL-Feature:</p> <p>Variablen</p> <p>In SQL k\u00f6nnen auch Variablen definiert werden. Mit SQL-Variablen lassen sich komplexe Abfragen entzerren und vereinfachen. Eine Variable beginnt mit dem Zeichen <code>@</code>, gefolgt vom eigentlichen Variablennamen. Der Variable kann mit dem Zuweisungsoperator <code>:=</code> oder \u00fcber das Schl\u00fcsselwort <code>SET</code> ein Wert zugewiesen werden. </p> <p>Im folgenden Beispiel wird der Variable <code>@gehalt</code> der Wert des Durchschnittsgehalt in der Tabelle \"Mitarbeiter\" zugewiesen. Auf die Variable kann innerhalb einer Folge von SQL-Statements stets wieder zugegriffen werden.</p> <pre><code>SET @gehalt = (SELECT AVG(Gehalt) FROM Mitarbeiter);\n</code></pre> <p>und</p> <pre><code>SELECT @gehalt := AVG(Gehalt) FROM Mitarbeiter;\n</code></pre> <p>Beide Statements sind gleichwertig!</p> <p>Wenn nun also folgende Frage beantwortet werden soll:</p> <p>Welcher Mitarbeiter verdient weniger als das Durchschnittsgehalt aller Mitarbeiter?</p> <p>k\u00f6nnte dies auch so beantwortet werden:</p> <pre><code>SELECT @gehalt := AVG(Gehalt) FROM Mitarbeiter;\n\nSELECT Nachname, Vorname FROM Mitarbeiter\n  WHERE Gehalt &lt; @gehalt;\n</code></pre> <p>oder </p> <pre><code>SET @gehalt = (SELECT AVG(Gehalt) FROM Mitarbeiter);\n\nSELECT Nachname, Vorname FROM Mitarbeiter\n  WHERE Gehalt &lt; @gehalt;\n</code></pre>"},{"location":"le09/ue09-02/#aufgabe-3","title":"Aufgabe 3","text":"<p>erstellen Sie in einer dummy-Datenbank folgende Konto-Tabelle:</p> <pre><code>CREATE TABLE Konto (\n  idKonto INT NOT NULL AUTO_INCREMENT,\n  IBAN VARCHAR(45) NULL,\n  Betrag FLOAT NULL,\n  PRIMARY KEY (idKonto));\n</code></pre> <p>Beachte: AUTO_INCREMENT</p> <p><code>AUTO_INCREMENT</code> weist die Datenbank an, einen automatischen Z\u00e4hler f\u00fcr das Attribut <code>idKonto</code> einzurichten. Der erste Record erh\u00e4lt damit die Nr 1, der zweite Record die Nummer 2 u.s.w. Damit muss sich der DB-Entwickler nicht mehr um den Wert des Primary Keys k\u00fcmmern; dieser wird automatisch generiert, wenn ein Record eingef\u00fcgt wird.</p> <p>In der MySQL-Workbench kann ein <code>AUTO_INCREMENT</code> auch grafisch definiert werden, indem <code>AI</code> angeklickt wird.</p> <p> MySQL-Workbench: AI = AUTO_INCREMENT </p> <p>F\u00fcgen Sie zwei Records hinzu:</p> <p><pre><code>INSERT INTO Konto (IBAN, Betrag) VALUES ('CH11 1111 1111 1111 1111 11', '2500');\nINSERT INTO Konto (IBAN, Betrag) VALUES ('CH22 2222 2222 2222 2222 22', '1000');\n</code></pre> Sie k\u00f6nnen nun beobachten, dass die Werte f\u00fcr den Primary Key <code>idKonto</code> automatisch generiert wurden.</p> <p>Nun zur</p> <p>Aufgabe</p> <p>\u00dcberweise das Geld von Konto <code>CH11 1111 1111 1111 1111 11</code> auf das Konto <code>CH22 2222 2222 2222 2222 22</code>. Verwende dazu die Variablen <code>@geld1</code> f\u00fcr den Betrag aus <code>CH11 1111 1111 1111 1111 11</code> und <code>@geld2</code> f\u00fcr den Betrag aus <code>CH22 2222 2222 2222 2222 22</code>.</p> <p>Die \u00dcberweisung soll als SICHERE Transaktion erfolgen!</p> <p>Danach soll die Transaktion \u00fcberpr\u00fcft werden. Wurde das Geld transferiert und sind die Kontost\u00e4nde korrekt?</p> AntwortTransaktion starten und beendenKontrolle <p>nicht gleich spicken! \ud83d\ude4e\u200d\u2642\ufe0f </p> <pre><code>USE xxx;\n\nSTART TRANSACTION;\n</code></pre> <pre><code>SELECT @geld1 := Betrag FROM Konto\n  WHERE IBAN = 'CH11 1111 1111 1111 1111 11';\n</code></pre> <pre><code>SELECT @geld2 := Betrag FROM Konto\n  WHERE IBAN = 'CH22 2222 2222 2222 2222 22';\n</code></pre> <pre><code>UPDATE Konto \n  SET Betrag = Betrag - @geld1\n  WHERE IBAN = 'CH11 1111 1111 1111 1111 11';\n</code></pre> <pre><code>UPDATE Konto \nSET Betrag = @geld1 + @geld2\nWHERE IBAN = 'CH22 2222 2222 2222 2222 22';\n</code></pre> <pre><code>COMMIT;\n</code></pre> <pre><code>SELECT * FROM Konto;\n</code></pre> <p> das war eine sichere Transaktion! </p>"},{"location":"le09/ue09-03/","title":"UE09-03 Transaktion mit Python und MySQL","text":"<p>Aufgabe</p> <p>Schreibe ein Python-Skript, welches eine Transaktion von CHF 100 von IBAN-Konto CH22 2222 2222 2222 2222 22 auf CH11 1111 1111 1111 1111 11 SICHER ausf\u00fchrt. Siehe dazu die Aufgabe 3 in ue09-02.</p> <p>Ein generisches Skript als Hilfestellung kann verwendet werden. Nehmen Sie dieses in ihrer Zusammenfasung auf.</p> ResultatHilfestellung <p> vor der Transaktion </p> <p> nach der Transaktion </p> <p>Kontrolle, ob library installiert ist</p> bei Verwendung von pip<pre><code>pip show mysql-connector-python\n</code></pre> <p>oder</p> <p>bei Verwendung von Anaconda<pre><code>conda list mysql-connector-python\n</code></pre> ansonsten</p> bei Verwendung von pip<pre><code>pip install mysql-connector-python\n</code></pre> <p>oder </p> bei Verwendung von Anaconda<pre><code>conda install mysql-connector-python\n</code></pre> <pre><code>import mysql.connector\n\nimport pymysql as mysql\n\n\n# Verbindung zur MySQL-Datenbank herstellen\nconn = mysql.connector.connect(\nhost=\"localhost\",\nuser=\"dein_benutzername\",\npassword=\"dein_passwort\",\ndatabase=\"deine_datenbank\"\n)\n\ncursor = conn.cursor()\n\ndef perform_transaction(conn, cursor):\n    try:\n        # Starte die Transaktion\n        conn.start_transaction()\n\n        # F\u00fchre das erste Update durch\n        cursor.execute(\".... \")\n\n        # F\u00fchre das zweite Update durch\n        cursor.execute(\".... \")\n\n        # Best\u00e4tige die Transaktion\n        conn.commit()\n        print(\"Transaktion erfolgreich abgeschlossen.\")\n    except Exception as e:\n        # Rolle die Transaktion zur\u00fcck bei Fehlern\n        conn.rollback()\n        print(\"Fehler bei der Transaktion. \u00c4nderungen wurden zur\u00fcckgesetzt.\")\n        print(e)\n\n# Beispiel f\u00fcr die Durchf\u00fchrung der Transaktion\nperform_transaction(conn, cursor)\n\nconn.close()\n</code></pre>"},{"location":"le09/ue09-04/","title":"UE09-04 MySQL und Python","text":""},{"location":"le09/ue09-04/#chinook-datenbank-in-mysql-erstellen","title":"Chinook-Datenbank in MySQL erstellen","text":"<p>Nachfolgend hast Du eine Ansammlung von SQL-Queries welche die Chinook-Datenbank ausmachen. Wie Du Dich sicher erinnern magst, hast Du mit dieser Datenbank Deine ersten Erfahrungen gesammelt in SQLite.</p> <p>Versuche nun, die Datenbank chinook in MySQL zu erstellen.</p> <p>Dazu verwendest Du ein Python Skript, welches ein beigelegtes SQL-Skript gegen deine lokale MYSQL-Datenbank ausf\u00fchrt.</p> <p>Versuche diese Chinook-DB in MySQL zu erstellen und lass dir anschliessend das ERM von der MySQL-Workbench generieren.</p> <p>Download Chinook_MySql_AutoIncrementPKs.sql</p> Python-Skript<pre><code>import os\nimport io\nimport pymysql\n\ndef run_query(connection, query):\n    with connection.cursor() as cursor:\n        cursor.execute(query)\n    connection.commit()\n\n\nconnection = pymysql.connect(\n    host=\"localhost\",\n    user=\"ihr_username\",\n    password=\"ihr_passwort\",\n    port=3306,\n    charset='utf8'\n)\n\nlines = []\nwith open(os.path.join('.', 'Chinook_MySql_AutoIncrementPKs.sql'), encoding='utf-8-sig') as fp:\n    lines = fp.read().split('\\n')\n\ncurrent_query = ''\nfor i in range(len(lines)):\n    current_query += lines[i]\n    if len(current_query) &gt; 0 and current_query[-1] == ';':\n        run_query(connection, current_query)\n        current_query = \"\"\n    if i%round(len(lines)/100) == 0:\n        print('progress: ' + str(round(i/len(lines)*100)) + '%')\n</code></pre>"},{"location":"le09/ue09-04/#erm-der-chinook-datenbank","title":"ERM der chinook-Datenbank","text":"<p>Lass dieses ERM in deiner Installation mit MySQL-Workbench erstellen! Verwende dazu die Funktion \"Reverse Engineer\".</p> ERM generiert aus MYSQL-Workbench f\u00fcr chinook-DB"},{"location":"le09/ue09-05/","title":"Verbindungsvarianten Python - MySQL","text":"<p>Es gibt zwei Libraries, mit welchen wir eine Verbindung mit einer MySQL-Datenbank realisieren k\u00f6nnen:</p> <ul> <li>mysql-connector-python</li> <li>pymysql</li> </ul> <p>In den vorhergehenden Beispielen haben wir beide bereits verwendet. Nimm im Zweifelsfall das offizielle Paket, d.h. <code>mysql-connector-python</code>.</p>"},{"location":"le09/ue09-05/#mysql-connector-python","title":"mysql-connector-python","text":"<p>Dazu muss die Library mysql-connector-python geladen sein:</p> <p><code>pip install mysql-connector-python</code> oder <code>conda install mysql-connector-python</code></p> Verbindungsbeispiel mit mysql-connector-python<pre><code>import mysql.connector\n\n# Verbindung zur MySQL-Datenbank herstellen\nconn = mysql.connector.connect(\n    host=\"localhost\",\n    user=\"dein_benutzername\",\n    password=\"dein_passwort\",\n    database=\"deine_datenbank\"\n)\n\n# Erstellen eines Cursors\ncursor = conn.cursor()\n\n# Eine einfache Abfrage ausf\u00fchren\ncursor.execute(\"SELECT DATABASE()\")\n\n# Ergebnis abrufen und ausdrucken\nresult = cursor.fetchone()\nprint(f\"Verbunden mit der Datenbank: {result[0]}\")\n\n# Verbindung schlie\u00dfen\ncursor.close()\nconn.close()\n</code></pre> <p>Eigenschaften dieser Variante:</p> <ul> <li>Offizielle Unterst\u00fctzung: mysql-connector-python wird direkt von Oracle entwickelt und gepflegt, was bedeutet dass es regelm\u00e4ssig aktualisiert wird und Unterst\u00fctzung f\u00fcr die neuesten MySQL-Versionen bietet.</li> <li>Da es ein offizieller Connector ist, ist es sehr gut kompatibel mit verschiedenen Versionen von MySQL.</li> <li>In manchen Szenarien kann es etwas langsamer sein als PyMySQL, insbesondere bei sehr grossen Datenmengen oder komplexen Abfragen.</li> </ul>"},{"location":"le09/ue09-05/#pymysql","title":"PyMySQL","text":"<p>Dazu muss die Library pymysql geladen sein:</p> <p><code>pip install pymysql</code> oder <code>conda install pymysql</code> `</p> Verbindungsbeispiel mit pymysql<pre><code>import pymysql\n\n# Verbindung zur MySQL-Datenbank herstellen\nconn = pymysql.connect(\n    host=\"localhost\",\n    user=\"dein_benutzername\",\n    password=\"dein_passwort\",\n    database=\"deine_datenbank\"\n)\n\n# Erstellen eines Cursors\ncursor = conn.cursor()\n\n# Eine einfache Abfrage ausf\u00fchren\ncursor.execute(\"SELECT DATABASE()\")\n\n# Ergebnis abrufen und ausdrucken\nresult = cursor.fetchone()\nprint(f\"Verbunden mit der Datenbank: {result[0]}\")\n\n# Verbindung schlie\u00dfen\ncursor.close()\nconn.close()\n</code></pre> <p>Eigenschaften dieser Variante:</p> <ul> <li>Da PyMySQL komplett in Python geschrieben ist, kann es einfacher zu installieren und auf verschiedenen Plattformen zu verwenden sein.</li> <li>In bestimmten Anwendungsf\u00e4llen kann PyMySQL schneller sein als mysql-connector-python.</li> <li>PyMySQL ist ein Open-Source-Projekt und wird nicht offiziell von Oracle unterst\u00fctzt, was bedeutet, dass es m\u00f6glicherweise nicht so regelm\u00e4ssig aktualisiert wird.</li> <li>K\u00f6nnte manchmal Probleme mit der Kompatibilit\u00e4t haben, insbesondere mit den neuesten MySQL-Versionen oder spezifischen MySQL-Funktionen.</li> </ul>"},{"location":"le10/","title":"LE10 - ORM und PostgreSQL","text":"Object Relational Mapping <p>Pros and Cons of ORM</p>"},{"location":"le10/PeeWee_Example/","title":"Einf\u00fchrungsbeispiel mit PeeWee ORM-Framework","text":""},{"location":"le10/PeeWee_Example/#peewee-als-orm-framework-mit-pip-installieren","title":"PeeWee als ORM-Framework mit pip installieren","text":"<pre><code>pip install peewee\n</code></pre>"},{"location":"le10/PeeWee_Example/#einfaches-beispiel-erstellen","title":"Einfaches Beispiel erstellen","text":"<p>Dieses Beispiel zeigt die grundlegenden Operationen mit <code>Peewee</code> und einer MySQL-Datenbank: Tabellen erstellen, Daten einf\u00fcgen und Abfragen durchf\u00fchren.</p> <p>Wie ihr seht, sind dazu keine SQL-Kommandos notwendig. Dies wird durch das ORM-Framework peewee im Hintergrund gemacht.</p> <p>Beachtet die Erkl\u00e4rungen hinter den (+)</p> <p>Beantworte folgende Fragen:</p> <ul> <li>Wie wird der PrimaryKey erstellt? Untersuche dazu die Tabelle mit der MySQL-Workbench.</li> <li>In welchem Zusammenhang steht die Funktion <code>PrimaryKeyField()</code> in PeeWee, welche hier nicht verwendet wird. Der PrimaryKey wird aber trotzdem erstellt...</li> </ul> <p>Die offizielle Dokumentation zum <code>peewee</code>-Framework findet ihr hier.          </p> <pre><code>from peewee import *\nimport pymysql\n\n# Verbinde dich mit einer MySQL-Datenbank\ndb = MySQLDatabase(  # (1)\n    'MYDB',  # Name der Datenbank. Irgendeine dummy-DB\n    user='ihr_username',  # Benutzername\n    password='ihr_passwort',  # Passwort\n    host='localhost',  # Hostname dbserver.bfh.ch oder localhost\n    port=3306  # Portnummer\n)\n\n# Definiere eine Basisklasse f\u00fcr alle Modelle\nclass BaseModel(Model): # (2)\n    class Meta:\n        database = db  # Weist die MySQL-Datenbank zu\n\n# Definiere das User-Modell \nclass User(BaseModel): # (3)\n    username = CharField()\n    password = CharField()\n    email = CharField()\n\n    class Meta:\n        table_name = 'peeweeusers'  # Legt den Tabellennamen fest\n\n# Erstelle die Tabelle in der Datenbank\ndb.connect()\ndb.create_tables([User]) # (4)\n\n# F\u00fcge einige Benutzerdaten ein # (5)\nuser1 = User.create(username='tjaeggi', password='passwordtj', email='thomas.jaeggi@bfh.ch')\nuser2 = User.create(username='tluginbuehl', password='secretpw', email='tim.luginbuehl@bfh.ch')\n\n# F\u00fchre eine einfache Abfrage durch\nusers = User.select()  # (6)\nfor user in users:\n    print(user.username, user.email)\n</code></pre> <ol> <li> Wir verwenden MySQL als Datenbank und stellen die Verbindung mit MySQLDatabase her, wobei die Datenbankdetails wie Name, Benutzername, Passwort, Host und Port angegeben werden.</li> <li> Das <code>BaseModel</code> definiert die gemeinsame Datenbank f\u00fcr alle Modelle.</li> <li> User-Modell: Definiert die User-Tabelle mit drei Feldern: <code>username</code>, <code>password</code> und <code>email</code>.</li> <li> Tabellen erstellen: <code>db.create_tables([User])</code> erstellt die User-Tabelle in der Datenbank.</li> <li> Daten einf\u00fcgen: <code>User.create(...)</code> f\u00fcgt neue Benutzer in die User-Tabelle ein.</li> <li> Abfrage: <code>User.select()</code> w\u00e4hlt alle Benutzer aus der Tabelle und gibt deren Benutzernamen und E-Mail aus.</li> </ol>"},{"location":"le10/postgresql-install/","title":"DOCKER-Installationsanleitung PostgreSQL mit pgadmin4","text":"<p>Die aktuelle Version von PostgreSQL ist im Moment (Dez. 2024) <code>17.2</code>.</p> <p>F\u00fchren Sie folgende Schritte aus:</p>"},{"location":"le10/postgresql-install/#start-docker-desktop-auf-ihrem-notebook","title":"Start Docker-Desktop auf Ihrem Notebook","text":"<p>Stellen Sie sicher, dass Sie eingeloggt sind. </p> Docker-Desktop gestartet und angemeldet <p>F\u00fchren Sie ein update des Docker-Desktops aus, falls das notwendig ist. Die aktuelle Version zum Zeitpunkt dieses Dokumentes ist 4.36.0 (Windows)</p>"},{"location":"le10/postgresql-install/#verzeichniss-erstellen-fur-ihre-docker-files","title":"Verzeichniss erstellen f\u00fcr Ihre Docker-Files","text":"<pre><code>mkdir docker-compose-files\ncd .\\docker-compose-files\\\nmkdir postgresql\ncd postgresql\n</code></pre>"},{"location":"le10/postgresql-install/#erstellung-der-docker-compose-files","title":"Erstellung der docker-compose-Files","text":"<p>Erstellen Sie 2 Dateien. </p> <p>Die Environment-Variablen mit sicherheitskritischen Informationen schreiben wir in ein eigenes File mit Namen <code>.postgresqlenv</code>, welches wir mit dem <code>docker-compose.yml</code> referenzieren. Stellen Sie sicher, dass das <code>.postgresqlenv</code> keine Extension (z. Bsp. .txt) hat.</p> <p>Dies ist eine g\u00e4ngige Methode um sicherheitsrelevante Informationen in eigenen Dateien abzuspeichern. Diese Dateien k\u00f6nnen dann mit eingeschr\u00e4nkten Rechten verwaltet werden, damit nicht jedermann die Passw\u00f6rter einsehen kann.</p> <p>Die Dateien sehen so aus:</p> 2 Dateien erstellen <p>Inhalte der Dateien:</p> .\\.postgresqlenv<pre><code># PostgreSQL database\nPGDATA=/var/lib/postgresql/data/pgdata\nPOSTGRES_USER=student\nPOSTGRES_PASSWORD=btw2201btw2201\nPOSTGRES_DB=pg_mydb\nPOSTGRES_HOST=localhost\n# pgAdmin\nPGADMIN_DEFAULT_EMAIL=student@bfh.ch\nPGADMIN_DEFAULT_PASSWORD=btw2201btw2201\n</code></pre> docker-compose.yml<pre><code>services:\n  db:\n    image: postgres:latest\n    container_name: postgreSQL-latest\n    restart: always\n    env_file:\n      - .postgresqlenv\n    ports:\n      - 5432:5432\n    volumes:\n      - db_data:/var/lib/postgresql/data\n  pgadmin:\n    image: dpage/pgadmin4\n    container_name: pgadmin4\n    restart: always\n    ports:\n      - \"5050:80\"\n    env_file:\n      - .postgresqlenv\n    volumes:\n      - pgadmin-data:/var/lib/pgadmin\n\nvolumes:\n  db_data:\n  pgadmin-data:\n</code></pre>"},{"location":"le10/postgresql-install/#docker-container-starten","title":"Docker-Container starten","text":"<p>Im Verzeichnis, wo das <code>docker-compose.yml</code> liegt, starten Sie die Container mit</p> <p><code>docker compose up -d</code></p> <p>Kontrolle, ob Container laufen mit</p> <p><code>docker ps -a</code></p> Container running? <p>Auch im Docker-Desktop sind die laufenden Container sichtbar:</p> Container running?"},{"location":"le10/postgresql-install/#verbindung-mit-pgadmin4","title":"Verbindung mit pgAdmin4","text":"<p>pgAdmin4</p> <p>pgAdmin4 ist ein Web-Interface um PostgreSQL zu administrieren. pgAdmin4 ist dasselbe, wie die MySQL-Workbench f\u00fcr die MySQL-Datenbank.</p> <p>pgAdmin4 erreichen wir auf <code>http://localhost:5050</code></p> <p>Login mit user <code>student@bfh.ch</code>, Passwort: <code>btw2201btw2201</code>. Diese Credentials haben wir im <code>.postgresqlenv</code> definiert.</p> <p>Nun m\u00fcssen wir pgAdmin mit der PostgreSQL-Datenbank verbinden. Klicke auf \"Add New Server\"</p> pgAdmin mit Datenbank verbinden <p>Machen Sie folgende Angaben:</p> General Tab Connection Tab <p>Nun erscheint der DatenbankServer in der linken Sidebar. Erstellen Sie eine neue Datenbank mit Rechtsklick</p> Ansicht `Datenbank erstellen <p>Machen Sie sich vertraut mit pgAdmin4, indem Sie das Video unten anschauen. Springe auf Minute 5. Danach lernen Sie neue Dinge..</p> <p>Video zu pgAdmin</p>"},{"location":"le10/python-postgresql/","title":"Python mit PostgreSQL verbinden","text":"<p>Das vorliegende Beispiel soll als Referenz f\u00fcr folgende Aufgaben dienen:</p> <ul> <li>Verbindung mit dem PostgreSQL-RDBMS erstellen</li> <li>Tabellen erstellen</li> <li>Records in Tabellen einf\u00fcgen</li> <li>Records aus Tabellen lesen</li> </ul>"},{"location":"le10/python-postgresql/#installiere-die-library-psycopg2","title":"Installiere die Library <code>psycopg2</code>","text":"<p>Diese Library wird ben\u00f6tigt, damit mit Python auf eine PostgreSQL-Datenbank zugegriffen werden kann.</p> <pre><code>pip install psycopg2-binary\n</code></pre>"},{"location":"le10/python-postgresql/#datenbank-kreieren","title":"Datenbank kreieren","text":"<p>Kreiere mit pgadmin eine Datenbank mit dem Namen <code>btw2201_test</code></p>"},{"location":"le10/python-postgresql/#python-script-fur-die-erstellung-einer-tabelle-rows-und-einem-query-fur-eine-bestehende-database","title":"Python Script f\u00fcr die Erstellung einer Tabelle, Rows und einem Query f\u00fcr eine bestehende Database","text":"<pre><code>import psycopg2\nfrom psycopg2 import sql\n\ndef connect_to_db():\n    try:\n        # Connect to your PostgreSQL database\n        conn = psycopg2.connect(\n            dbname=\"btw2201_test\",\n            user=\"student\",\n            password=\"btw2201btw2201\",\n            host=\"localhost\",\n            port=\"5432\"\n        )\n        print(\"Connection successful\")\n        return conn\n    except Exception as e:\n        print(f\"Unable to connect to the database: {e}\")\n        return None\n\ndef create_table(cursor):\n    try:\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS users (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(100),\n                age INTEGER\n            )\n        \"\"\")\n        print(\"Table created successfully\")\n    except Exception as e:\n        print(f\"Error creating table: {e}\")\n\ndef insert_rows(cursor):\n    try:\n        cursor.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (\"Ana\u00efs\", 21))\n        cursor.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (\"Ren\u00e9\", 25))\n        cursor.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (\"Madeleine\", 22))\n        cursor.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (\"Herv\u00e9\", 28))\n        print(\"Rows inserted successfully\")\n    except Exception as e:\n        print(f\"Error inserting rows: {e}\")\n\ndef select_rows(cursor):\n    try:\n        cursor.execute(\"SELECT * FROM users\")\n        rows = cursor.fetchall()\n        print(\"Rows selected successfully\")\n        for row in rows:\n            print(f\"id: {row[0]}, name: {row[1]}, age: {row[2]}\")\n    except Exception as e:\n        print(f\"Error selecting rows: {e}\")\n\ndef main():\n    conn = connect_to_db()\n    if conn:\n        cursor = conn.cursor()\n        create_table(cursor)\n        insert_rows(cursor)\n        select_rows(cursor)\n        conn.commit()\n        cursor.close()\n        conn.close()\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"le10/ue10-01/","title":"UE10-01 ORM - Aufgabe mit PeeWee","text":""},{"location":"le10/ue10-01/#ziel","title":"Ziel","text":"<p>Ein tiefgehenderes Verst\u00e4ndnis und Anwendung von Object-Relational Mapping (ORM) mit Peewee und MySQL durch das Erstellen und Verwalten einer komplexeren Datenbank f\u00fcr Benutzer- und Artikelinformationen.</p>"},{"location":"le10/ue10-01/#voraussetzung","title":"Voraussetzung","text":"<ul> <li>Installiertes Peewee-Framework</li> <li>MySQL-Server und MySQL-Python-Connector (pymysql)</li> </ul>"},{"location":"le10/ue10-01/#aufgabe","title":"Aufgabe","text":"<p>Erstelle dazu ein Python-Skript mit folgenden Anforderungen:</p> <ul> <li>Erstelle zwei Tabellen: <code>User</code> und <code>Article</code>.</li> <li>Ein User kann mehrere Article-Eintr\u00e4ge haben (1:m-Beziehung).</li> <li>Felder f\u00fcr User: <code>id</code> (Prim\u00e4rschl\u00fcssel), <code>username</code>, <code>password</code>, <code>email</code>, <code>created_at</code>.</li> <li>Felder f\u00fcr Article: <code>id</code> (Prim\u00e4rschl\u00fcssel), <code>title</code>, <code>content</code>, <code>user</code> (Fremdschl\u00fcssel zu User), <code>created_at</code>.</li> </ul> <p>Anforderungen:</p> <ul> <li>Definiere die Verbindung zur MySQL-Datenbank.</li> <li>Definiere eine Basisklasse <code>BaseModel</code>, die alle Modelle erweitern.</li> <li>Erstelle die Modelle <code>User</code> und <code>Article</code>, und definiere die Beziehungen zwischen ihnen.</li> <li>F\u00fcge Benutzerdaten und Artikeldaten in die Tabellen ein.</li> <li>F\u00fchre eine Abfrage durch, um alle Artikel eines bestimmten Benutzers anzuzeigen.</li> </ul>"},{"location":"le10/ue10-01/#hinweise","title":"Hinweise","text":"<ul> <li>Fremdschl\u00fcssel: Verwende <code>ForeignKeyField</code> in Peewee, um Beziehungen zwischen Tabellen zu erstellen.</li> <li>Abfragen: Nutze die Abfrage-API von Peewee, um komplexe Abfragen einfach zu gestalten.</li> <li>Zeitstempel: Verwende <code>DateTimeField()</code> mit einem Standardwert von <code>datetime.now</code>, um automatisch Zeitstempel zu erstellen.</li> <li>Link Relationships and Joins mit PeeWee</li> <li>Link zu Query-Beispielen mit PeeWee</li> </ul>"},{"location":"le10/ue10-01/#beispielcode-als-vorlage-fur-die-aufgabe","title":"Beispielcode als Vorlage f\u00fcr die Aufgabe","text":"<pre><code>from peewee import *\nimport pymysql\nfrom datetime import datetime\n\n# Verbinde dich mit einer MySQL-Datenbank\ndb = MySQLDatabase(\n    'my_database',  # Name der Datenbank\n    user='my_user',  # Benutzername\n    password='my_password',  # Passwort\n    host='localhost',  # Hostname\n    port=3306  # Portnummer\n)\n\n# Definiere eine Basisklasse f\u00fcr alle Modelle\nclass BaseModel(Model):\n    class Meta:\n        database = db  # Weist die MySQL-Datenbank zu\n\n# Definiere das User-Modell\nclass User(BaseModel):\n    username = CharField()\n    password = CharField()\n    email = CharField()\n    created_at = DateTimeField(default=datetime.now)\n\n    class Meta:\n        table_name = 'users'\n\n# Definiere das Article-Modell\nclass Article(BaseModel):\n    title = CharField()\n    content = TextField()\n    user = ForeignKeyField(User, backref='articles')\n    created_at = DateTimeField(default=datetime.now)\n\n    class Meta:\n        table_name = 'articles'\n\n# Erstelle die Tabellen in der Datenbank\ndb.connect()\ndb.create_tables([User, Article])\n\n# F\u00fcge einige Benutzerdaten ein\nuser1 = User.create(username='tjaeggi', password='passwordtj', email='thomas.jaeggi@bfh.ch')\nuser2 = User.create(username='janesmith', password='mypassword', email='jane@example.com')\n\n# F\u00fcge einige Artikeldaten ein\nArticle.create(title='Erster Artikel', content='Inhalt des ersten Artikels.', user=user1)\nArticle.create(title='Zweiter Artikel', content='Inhalt des zweiten Artikels.', user=user1)\nArticle.create(title='Artikel von Jane', content='Inhalt von Jane\\'s Artikel.', user=user2)\n\n# F\u00fchre eine Abfrage durch, um alle Artikel von \"john_doe\" anzuzeigen\narticles = Article.select().where(Article.user == user1)\nfor article in articles:\n    print(article.title, article.content)\n</code></pre>"},{"location":"le10/ue10-01/#nutzliche-links","title":"n\u00fctzliche Links","text":"<p>Quickstart mit PeeWee.</p> <p>PeeWee Dokumentation.</p> <p>PeeWee Query Builder.</p>"},{"location":"le10/ue10-02/","title":"UE10-02 ORM - Aufgabe mit PeeWee basierend auf UE10-01","text":"<ul> <li>F\u00fcge zus\u00e4tzliche Felder und Validierungen zu den Modellen hinzu: <code>last_login</code> f\u00fcr <code>User</code> und <code>category</code> f\u00fcr <code>Article</code>.</li> <li>Implementiere eine einfache Suchfunktion, um Artikel anhand von Titeln zu finden.</li> <li>Verschl\u00fcssele die Passw\u00f6rter vor dem Speichern in der Datenbank. (z. Bsp mit <code>bcrypt</code>)</li> <li>Erstelle eine Funktion, die das Erstellungsdatum der Artikel in einem benutzerfreundlichen Format ausgibt.</li> <li>F\u00fcge Kommentare in den Code ein, um die Funktion jedes Teils zu erkl\u00e4ren.</li> <li>Warum gibt die Passwortpr\u00fcfung von user1 <code>tjaeggi</code> einen Fehler?</li> </ul> <p>Hinweis:</p> <p>Halte parallel die MySQL-Workbench offen, und beobachte die Erstellung der Tabellen mit dem Inhalt. L\u00e4sst Du das Skript mehrmals laufen, l\u00f6sche jeweils zuerst die Tabellen.</p> <p>Ziel der Aufgabe: Code-Verst\u00e4ndnis und Erweiterung der Python-Grundkenntnisse allgemein und mit ORM</p> <pre><code>from peewee import *\nimport pymysql\nfrom datetime import datetime\nimport bcrypt\n\n# Verbinde dich mit einer MySQL-Datenbank\ndb = MySQLDatabase(\n    'MYDB',  # Name der Datenbank\n    user='ihr_user',  # Benutzername\n    password='ihr_passwort',  # Passwort\n    host='localhost',  # Hostname\n    port=3306  # Portnummer\n)\n\n# Definiere eine Basisklasse f\u00fcr alle Modelle\nclass BaseModel(Model):\n    class Meta:\n        database = db  # Weist die MySQL-Datenbank zu\n\n# Definiere das User-Modell\nclass User(BaseModel):\n    username = CharField(unique=True)\n    password = CharField()\n    email = CharField(unique=True)\n    created_at = DateTimeField(default=datetime.now)\n    last_login = DateTimeField(null=True)  # Neues Feld f\u00fcr den letzten Login\n\n    def save(self, *args, **kwargs):\n        # Verschl\u00fcsselung des Passworts mit bcrypt\n        self.password = bcrypt.hashpw(self.password.encode('utf-8'), bcrypt.gensalt()) # (1)\n        super(User, self).save(*args, **kwargs)  # (2)\n\n    class Meta:\n        table_name = 'users'\n\n# Definiere das Article-Modell mit `category` Feld\nclass Article(BaseModel):\n    title = CharField()\n    content = TextField()\n    category = CharField(default='General')  # Neues Feld f\u00fcr die Kategorie\n    user = ForeignKeyField(User, backref='articles')\n    created_at = DateTimeField(default=datetime.now)\n\n    class Meta:\n        table_name = 'articles'\n\n# Funktion zur Formatierung des Erstellungsdatums\ndef format_created_at(article):\n    return article.created_at.strftime('%d.%m.%Y %H:%M:%S')\n\n# Suchfunktion, um Artikel anhand von Titeln zu finden\ndef search_articles_by_title(search_term):\n    articles = Article.select().where(Article.title.contains(search_term))\n    return articles\n\n# Passwort\u00fcberpr\u00fcfung\ndef check_password(username, password):\n    user = User.get(User.username == username)\n    return bcrypt.checkpw(password.encode('utf-8'), user.password.encode('utf-8'))\n\n# Hauptfunktion zur Ausf\u00fchrung des Skripts\ndef main():\n    # Erstelle die Tabellen in der Datenbank\n    db.connect()\n    db.create_tables([User, Article])\n\n    # F\u00fcge einige Benutzerdaten ein\n    user1 = User.create(username='tjaeggi', password='dbsecret', email='tj@bfh.ch')\n    user2 = User.create(username='jane_smith', password='mypassword', email='jane@example.com')\n\n    # F\u00fcge einige Artikeldaten ein\n    Article.create(title='Erster Artikel', content='Inhalt des ersten Artikels.', user=user1, category='Tech')\n    Article.create(title='Zweiter Artikel', content='Inhalt des zweiten Artikels.', user=user1, category='Health')\n    Article.create(title='Artikel von Jane', content='Inhalt von Jane\\'s Artikel.', user=user2, category='Finance')\n\n    # F\u00fchre eine Abfrage durch, um alle Artikel von \"john_doe\" anzuzeigen\n    articles = Article.select().where(Article.user == user1)\n    for article in articles:\n        print(f\"Title: {article.title}, Content: {article.content}, Category: {article.category}, Created At: {format_created_at(article)}\")\n\n    # Beispielhafte Verwendung der Suchfunktion\n    search_term = 'Artikel'\n    found_articles = search_articles_by_title(search_term)\n    for article in found_articles:\n        print(f\"Found Title: {article.title}, Content: {article.content}, User: {article.user.username}\")\n\n    # Beispielhafte Verwendung der Passwort\u00fcberpr\u00fcfung\n    username = 'tjaeggi'\n    password = 'password123'\n    print(f\"Passwort\u00fcberpr\u00fcfung f\u00fcr {username}: {'Erfolgreich' if check_password(username, password) else 'Fehlgeschlagen'}\")\n\n# Ausf\u00fchrung des Skripts\n# \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <code>self</code>: Wird verwendet, um auf Instanzvariablen und Methoden innerhalb derselben Klasse zuzugreifen. Bezieht sich auf die aktuelle Instanz der Klasse. Es erm\u00f6glicht die Manipulation und den Zugriff auf die Attribute der spezifischen Instanz.</li> <li> <code>super</code>: Wird verwendet, um auf Methoden und Eigenschaften der Basisklasse (Superklasse) zuzugreifen. Hilfreich bei der Arbeit mit Vererbung, um Methoden der Basisklasse aufzurufen und zu erweitern. <code>super</code> wird h\u00e4ufig in Vererbungsszenarien verwendet, um die Methoden der Basisklasse zu \u00fcberschreiben oder zu erweitern.</li> </ol> <p>Bedeutung <code>if __name__ == '__main__':</code>  (ist eventuell Repetition)</p> <pre><code>if __name__ == '__main__':\nmain()\n</code></pre> <p>Das Konstrukt <code>if __name__ == '__main__': main()</code> ist ein wichtiger Bestandteil vieler Python-Skripte. Es dient dazu, den Unterschied zwischen dem direkten Ausf\u00fchren eines Skripts und dem Importieren des Skripts als Modul in ein anderes Skript zu kl\u00e4ren.</p> <p>Erkl\u00e4rung:</p> <ol> <li> <p><code>__name__</code> Variable:</p> <ol> <li>Jedes Python-Skript hat eine spezielle Variable namens <code>__name__</code>.</li> <li>Wenn ein Skript direkt ausgef\u00fchrt wird (z.B. durch <code>python main.py</code>), wird <code>__name__</code> auf <code>'__main__'</code> gesetzt.</li> <li>Wenn ein Skript importiert wird (z.B. <code>import main</code>), wird <code>__name__</code> auf den Namen des Skripts (z.B. <code>main</code>) gesetzt.</li> </ol> </li> <li> <p>if <code>__name__</code> == <code>'__main__'</code>:</p> <ol> <li>Diese Bedingung \u00fcberpr\u00fcft, ob das Skript direkt ausgef\u00fchrt wird.</li> <li>Wenn die Bedingung wahr ist (also <code>__name__ == '__main__'</code>), wird der Codeblock darunter ausgef\u00fchrt.</li> <li>Wenn das Skript importiert wird, ist die Bedingung falsch und der Codeblock wird nicht ausgef\u00fchrt.</li> </ol> </li> <li> <p><code>main()</code> Funktion:</p> <ol> <li>Die <code>main()</code> Funktion enth\u00e4lt den Code, der nur ausgef\u00fchrt werden soll, wenn das Skript direkt gestartet wird.</li> <li>Diese Funktion wird durch das Konstrukt <code>if __name__ == '__main__':</code> aufgerufen.</li> </ol> </li> </ol> <p>Beispiel:</p> script.py<pre><code>def main():\n    print(\"Das Skript wurde direkt ausgef\u00fchrt.\")\n\nif __name__ == '__main__':\n    main()\n</code></pre> direktes Ausf\u00fchren<pre><code>python script.py\n</code></pre> <ul> <li>Ausgabe: <code>Das Skript wurde direkt ausgef\u00fchrt.</code></li> <li>In diesem Fall ist <code>__name__</code> gleich <code>'__main__'</code>, daher wird <code>main()</code> aufgerufen.</li> </ul> Importieren mit script2.py<pre><code>import script\n</code></pre> <ul> <li>Ausgabe: Nichts</li> <li>In diesem Fall ist <code>__name__</code> gleich <code>'script'</code>, daher wird <code>main()</code> nicht aufgerufen.</li> </ul> <p>Das Konstrukt <code>if __name__ == '__main__': main()</code> erlaubt es einem Python-Skript, sowohl als eigenst\u00e4ndiges Programm ausgef\u00fchrt als auch als Modul in anderen Skripten importiert zu werden, ohne dass der Hauptcode unbeabsichtigt ausgef\u00fchrt wird. Es ist eine bew\u00e4hrte Methode, die sicherstellt, dass der Code nur dann ausgef\u00fchrt wird, wenn er beabsichtigt ist, und erleichtert die Wiederverwendung von Skripten als Module.</p>"},{"location":"le10/ue10-03/","title":"UE10-03 ORM - eigene L\u00f6sung","text":""},{"location":"le10/ue10-03/#aufgabe","title":"Aufgabe","text":"<p>Setzen Sie die Aufgabe aus UE06-01, Aufgabe 5, mit peewee-ORM um.</p> <p>Zur Erinnerung:</p> <ul> <li>Datenbankname: <code>Mitarbeiter</code></li> <li>Tabellen: <code>Abteilung</code>, <code>Abteilungsmitarbeiter</code></li> <li>Kardinalit\u00e4t Abteilung - Abteilungsmitarbeiter 1:m</li> <li>Legen Sie einige Datens\u00e4tze an</li> <li>Verwende PK und FK gem\u00e4ss den M\u00f6glichkeiten von peewee</li> <li>F\u00fchre Abfragen mit jeweils einem Selektionskriterium f\u00fcr Abteilung und einem Selektionskriterium f\u00fcr Mitarbeiter aus</li> </ul>"},{"location":"le10/ue10-03/#nutzliche-links","title":"n\u00fctzliche Links","text":"<p>Quickstart mit PeeWee.</p> <p>PeeWee Dokumentation.</p> <p>PeeWee Query Builder.</p>"},{"location":"le10/ue10-04/","title":"Mit PostgreSQL \"spielen\"","text":"<p>Nach dem Installieren und Einrichten von Postgres ist die Idee nun eine Datenbank zu erstellen mit einzelnen Tabellen und Beziehungen. Dazu kannst Du zum Beispiel die Datenbank aus UE06-01, Aufgabe 6 (Mitarbeiter) oder die Datenbank aus UE06-02, Aufgabe 3, welche wir im MySQL gebaut haben, in Postgres nachbauen. </p> <p>Verwende dazu pgAdmin und versuche auch mittels Python auf die Postgres-Datenbank zuzugreifen und Datens\u00e4tze zu lesen und zu schreiben. Als Beispiel k\u00f6nnte z. Bsp die Aufgabe UE05-02, angepasst an Postgres,  als Grundlage dienen.</p> <p>Es geht prim\u00e4r darum Unterschiede und Gemeinsamkeiten zu sehen.</p>"},{"location":"le10/ue10-04/#postgresql-datentypen-und-cheat-sheet","title":"PostgreSQL Datentypen und Cheat-Sheet","text":"<ul> <li>Ein Problem, das bei Verwendung von verschiedenen Datenbanken oft betrachtet werden muss, ist die Kompatibilit\u00e4t von Datentypen. Eine Gegen\u00fcberstellung von PostgreSQL und MySQL findet sich hier.</li> <li>Detaillierte Darstellung der Datentypen von PostgreSQL ist hier.</li> <li>Hier etwas Praktisches f\u00fcr den Umgang mit PostgreSQL: postgresql-cheat-sheet-a4.pdf</li> </ul>"},{"location":"le11/","title":"LE11 - Timeseries Datenbanken","text":""},{"location":"le11/#eigenschaften-von-timeseries-datenbanken-tsdb","title":"Eigenschaften von Timeseries-Datenbanken (TSDB)","text":"<p>Eine Zeitreihendatenbank (Time Series Database, TSDB) ist speziell f\u00fcr das Speichern und Analysieren von Zeitreihendaten optimiert</p> <p>allgemeine Eigenschaften von Zeitreihendatenbanken:</p> <ul> <li>Indizierung \u00fcber Zeitstempel: Zeitstempel sind der Hauptindex f\u00fcr die Daten.</li> <li>Skalierbarkeit: Sie k\u00f6nnen grosse Datenmengen verarbeiten, oft mehrere 100.000 Messwerte pro Sekunde.</li> <li>Keine starke Konsistenzgarantien: Stattdessen bieten sie Skalierbarkeit und schnelle Abfragen.</li> <li>Automatische Datenreduktion: \u00c4ltere Daten k\u00f6nnen automatisch gel\u00f6scht oder komprimiert werden.</li> <li>Zeitbasierte Abfragen: Abfragen basierend auf Zeitstempeln sind einfach und schnell</li> </ul> Zeitreihendaten-Beispiel in InfluxDB"},{"location":"le11/influxdb-einfuehrungsbeispiel/","title":"Einf\u00fchrungsbeispiel mit InfluxDB und Python","text":"<p>Hier ist ein vollst\u00e4ndiges Python-Skript, das zeigt, wie man Sensordaten in ein InfluxDB-Bucket l\u00e4dt und anschliessend einige Basisabfragen durchf\u00fchrt.</p> <p>Erstellen Sie vorher einen weiteren Testbucket, z. Bsp. mit Namen <code>sensor_data</code>.</p> sensor_data.py<pre><code>import influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime, timedelta\nimport random\n\n# Verbindung zu InfluxDB herstellen\nbucket = \"sensor_data\"\norg = \"BFH\"\ntoken = \"IHR TOKEN\"\nurl = \"http://localhost:8086\"\n\nclient = influxdb_client.InfluxDBClient(\n    url=url,\n    token=token,\n    org=org\n)\n\n# Beispiel-Sensordaten mit Timestamps schreiben\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\n# Funktion zur Generierung von zuf\u00e4lligen Sensordaten mit Timestamps\ndef generate_sensor_data(num_points):\n    data = []\n    current_time = datetime.utcnow()\n    for i in range(num_points):\n        # Temperaturdaten\n        temp_data = f\"temperature,location=room1 value={round(random.uniform(20.0, 25.0), 1)} {int(current_time.timestamp() * 1e9)}\"\n        data.append(temp_data)\n        temp_data = f\"temperature,location=room2 value={round(random.uniform(20.0, 25.0), 1)} {int(current_time.timestamp() * 1e9)}\"\n        data.append(temp_data)\n\n        # Feuchtigkeitsdaten\n        humidity_data = f\"humidity,location=room1 value={round(random.uniform(40.0, 50.0), 1)} {int(current_time.timestamp() * 1e9)}\"\n        data.append(humidity_data)\n        humidity_data = f\"humidity,location=room2 value={round(random.uniform(40.0, 50.0), 1)} {int(current_time.timestamp() * 1e9)}\"\n        data.append(humidity_data)\n\n        current_time -= timedelta(minutes=5)\n    return data\n\n# Generiere 100 Datenpunkte\nsensor_data = generate_sensor_data(100)\n\n# Schreibe die generierten Datenpunkte in InfluxDB\nwrite_api.write(bucket=bucket, record=sensor_data)\nprint(\"Daten erfolgreich geschrieben\")\n\n# Basisabfragen mit Flux durchf\u00fchren\nquery_api = client.query_api()\n\n# Abfrage: Alle Datenpunkte\nquery = f'from(bucket: \"{bucket}\") |&gt; range(start: -30d)'\nresult = query_api.query(org=org, query=query)\n\nprint(\"\\nAlle Datenpunkte:\")\nfor table in result:\n    for record in table.records:\n        print(record)\n\n# Abfrage: Filter nach Measurement 'temperature'\nquery = f'''\nfrom(bucket: \"{bucket}\")\n  |&gt; range(start: -30d)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n'''\nresult = query_api.query(org=org, query=query)\n\nprint(\"\\nDatenpunkte f\u00fcr 'temperature':\")\nfor table in result:\n    for record in table.records:\n        print(record)\n\n# Abfrage: Durchschnittswert der Temperatur\nquery = f'''\nfrom(bucket: \"{bucket}\")\n  |&gt; range(start: -30d)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n  |&gt; mean(column: \"_value\")\n'''\nresult = query_api.query(org=org, query=query)\n\nprint(\"\\nDurchschnittswert der Temperatur:\")\nfor table in result:\n    for record in table.records:\n        print(record)\n\n# Abfrage: Gruppieren nach Tag 'location' und Durchschnittswert berechnen\nquery = f'''\nfrom(bucket: \"{bucket}\")\n  |&gt; range(start: -30d)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n  |&gt; group(columns: [\"location\"])\n  |&gt; mean(column: \"_value\")\n'''\nresult = query_api.query(org=org, query=query)\n\nprint(\"\\nDurchschnittswert der Temperatur gruppiert nach 'location':\")\nfor table in result:\n    for record in table.records:\n        print(record)\n\n# Verbindung schliessen\nclient.close()\n</code></pre> <p>Machen Sie sich mit dem Data Explorer von Influx vertraut und analysiere die generierten Daten</p> InfluxDB Data Explorer"},{"location":"le11/influxdb-einfuehrungsbeispiel/#dokumentation-flux","title":"Dokumentation Flux","text":"<p>Flux ist eine Data scripting language designed for querying, analyzing, and acting on data</p> <p>Flux overview</p> <p>Flux query basics</p> <p>Flux syntax basics</p>"},{"location":"le11/inst-influxdb-docker/","title":"Installation von InfluxDB mit docker","text":"<p>F\u00fchren Sie folgende Schritte aus:</p>"},{"location":"le11/inst-influxdb-docker/#start-docker-desktop-auf-notebook","title":"Start Docker-Desktop auf Notebook","text":"<p>Stellen Sie sicher, dass Sie eingeloggt sind. </p> Docker-Desktop aktualisiert und angemeldet <p>F\u00fchren Sie ein update des Docker-Desktops aus, falls das notwendig ist. </p>"},{"location":"le11/inst-influxdb-docker/#verzeichniss-erstellen-fur-ihre-docker-files","title":"Verzeichniss erstellen f\u00fcr Ihre Docker-Files","text":"<pre><code>mkdir docker-compose-files\ncd .\\docker-compose-files\\\nmkdir influxdb\ncd influxdb\n</code></pre>"},{"location":"le11/inst-influxdb-docker/#erstellung-des-docker-composeyml-files","title":"Erstellung des <code>docker-compose.yml</code>-Files","text":"docker-compose.yml<pre><code>services:\n  influxdb:\n    image: influxdb:2\n    container_name: influxdb\n    restart: always\n    ports:\n      - '8086:8086'\n    environment:\n      DOCKER_INFLUXDB_INIT_MODE: setup\n      DOCKER_INFLUXDB_INIT_USERNAME: admin\n      DOCKER_INFLUXDB_INIT_PASSWORD: btw2201btw2201\n      DOCKER_INFLUXDB_INIT_ORG: BFH\n      DOCKER_INFLUXDB_INIT_BUCKET: OpenWeather\n    volumes:\n      - ./data:/var/lib/influxdb2\n      - ./config:/etc/influxdb2\n</code></pre>"},{"location":"le11/inst-influxdb-docker/#docker-container-starten","title":"Docker-Container starten","text":"<p>Im Verzeichnis, wo das <code>docker-compose.yml</code> liegt, starten Sie die Container mit</p> <p><code>docker compose up -d</code></p> <p>Kontrolle, ob Container l\u00e4uft mit</p> <p><code>docker ps -a</code></p> Container running? <p>Auch im Docker-Desktop sind die laufenden Container sichtbar:</p> Container running?"},{"location":"le11/inst-influxdb-docker/#verbindung-mit-gui-von-influxdb","title":"Verbindung mit GUI von InfluxDB","text":"<p>Das Web-GUI von InfluxDB erreichen wir nun auf <code>http://localhost:8086</code></p> <p>Login mit user <code>admin</code>, Passwort: <code>btw2201btw2201</code>. Diese Credentials haben wir im <code>docker-compose.yml</code> definiert.</p> Login WebGUI InfluxDB"},{"location":"le11/inst-influxdb-docker/#api-token-generieren","title":"API Token generieren","text":"<p>Generiere dir als Erstes ein API Token. Dieses werden wir f\u00fcr alles Weitere als Zugangskey verwenden. </p> All Access API Token generieren <p>Das bei Dir generierte Token muss notiert werden!</p> <p>Beispieltoken \u26a0\ufe0f NICHT kopieren ist ein Beispiel! \u26a0\ufe0f <pre><code>h9Bdxy-qm6djPdE5fP8PD8SnEwp46A88cFlgJU-VKlIraRbdQlHHJut44Lelf1Tqr1Ck4WyeYqYD5dZPSILCtw==\n</code></pre></p>"},{"location":"le11/inst-influxdb-docker/#bucket-zum-testen-erstellen","title":"Bucket zum Testen erstellen","text":"<p>Erstellen Sie auch einen Testbucket, z.Bsp mit Namen <code>MYBUCKET</code></p> Bucket erstellen"},{"location":"le11/inst-influxdb-docker/#introbeispiel","title":"Introbeispiel","text":"<p>F\u00fchren Sie das Python Programmierbeispiel durch. Verwende jedoch immer Dein generiertes Token von oben.</p> Write and query data using Python <p>Note</p> <ul> <li>Ihr URL lautet: <code>url=\"http://localhost:8086\"</code></li> <li>Token: <code>token=\"IHR TOKEN\"</code></li> <li>Organisation: <code>org=\"BFH\"</code></li> <li>Bucket: <code>bucket=\"MYBUCKET\"</code></li> <li>Initialisierungsstring client: <code>client = InfluxDBClient(url=url, token=token, org=org)</code></li> </ul> <p>Angelehnt an das Beispiel in Get Started w\u00fcrde ein Skript zum Schreiben der Testdaten so ausschauen</p> <pre><code>import influxdb_client, os, time\nfrom influxdb_client import InfluxDBClient, Point, WritePrecision\nfrom influxdb_client.client.write_api import SYNCHRONOUS\n\nclient = InfluxDBClient(url=\"http://localhost:8086\", token=\"IHR TOKEN\", org=\"BFH\")\n\nbucket=\"MYBUCKET\"\n\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\nfor value in range(5):\n  point = (\n    Point(\"measurement1\")\n    .tag(\"tagname1\", \"tagvalue1\")\n    .field(\"field1\", value)\n  )\n  write_api.write(bucket=bucket, org=\"BFH\", record=point)\n  time.sleep(1) # separate points by 1 second\n</code></pre>"},{"location":"le11/tsdb-einfuehrung/","title":"Einf\u00fchrung InfluxDB als Vertreter einer TSDB","text":"<p>InfluxDB ist eine NoSQL-Datenbank. Sie ist speziell f\u00fcr das Speichern und Abfragen von Zeitreihendaten entwickelt worden, also Daten, die \u00fcber die Zeit hinweg aufgezeichnet werden, wie Sensordaten, Metriken, Events und Logs. Mehr Details zu NoSQL-Datenbanken folgen in LE12.</p>"},{"location":"le11/tsdb-einfuehrung/#merkmale-von-influxdb","title":"Merkmale von InfluxDB","text":"<ul> <li>Zeitreihen-Datenbank: Optimiert f\u00fcr das Speichern von Datenpunkten, die mit Zeitstempeln versehen sind.</li> <li>SQL-\u00e4hnliche Abfragesprache: Verwendet InfluxQL, eine SQL-\u00e4hnliche Abfragesprache, die speziell f\u00fcr Zeitreihendaten entwickelt wurde.</li> <li>Hohe Schreibgeschwindigkeit: InfluxDB ist auf hohe Schreibgeschwindigkeiten ausgelegt und kann grosse Mengen an Daten effizient verarbeiten.</li> <li>Retention Policies: Unterst\u00fctzt Aufbewahrungsrichtlinien, um Daten nach einer bestimmten Zeit automatisch zu l\u00f6schen oder zu archivieren.</li> <li>Einfach zu verwenden: Installation und Konfiguration sind relativ einfach, und es bietet eine Vielzahl von Visualisierungs- und Analysewerkzeugen.</li> </ul>"},{"location":"le11/tsdb-einfuehrung/#spezifische-eigenschaften-von-influxdb","title":"Spezifische Eigenschaften von InfluxDB","text":"<p>InfluxDB ist eine beliebte Open-Source-Zeitreihendatenbank, die f\u00fcr die Verwaltung und Analyse von Zeitreihendaten entwickelt wurde. InfluxDB wurde von Google in der Programmiersprache Go entwickelt.</p> <p>Hier sind einige ihrer spezifischen Eigenschaften:</p> <ul> <li>Skalierbarkeit: InfluxDB kann grosse Datenmengen effizient verarbeiten</li> <li>Datenkompression: Grosse Datenmengen k\u00f6nnen komprimiert werden, um Speicherplatz zu sparen.</li> <li>Datenrichtlinien: Es erm\u00f6glicht die Festlegung von Richtlinien, um genau zu bestimmen, wo bestimmte Daten gespeichert werden</li> <li>Echtzeit-Datenverarbeitung: InfluxDB kann Echtzeit- und parallel eintreffende Daten effizient speichern und analysieren.</li> <li>Flexibilit\u00e4t: Sie kann sowohl lokal als auch in der Cloud betrieben werden und unterst\u00fctzt verschiedene Programmiersprachen.</li> <li>Zeitreihen-Daten: InfluxDB speichert Datenpunkte, die mit Zeitstempeln versehen sind, wie z.B. Temperaturmessungen, Leistungsmetriken oder Finanzdaten</li> <li>Abfragesprache Flux: InfluxDB verwendet eine eigene Abfragesprache namens InfluxQL f\u00fcr die Datenabfrage und Analyse. Ab InfluxDB 2.0 wird die neue Abfragesprache Flux unterst\u00fctzt, die f\u00fcr ETL-Prozesse optimiert ist.</li> <li>Datenmodell: Die Daten werden in drei Hauptkomponenten gespeichert: Measurements, Fields und Tags. Measurements sind \u00e4hnlich wie Tabellen, Fields sind die Werte und Tags sind zus\u00e4tzliche Metadaten, die die Daten kategorisieren2</li> <li>InfluxDB ist besonders n\u00fctzlich f\u00fcr Anwendungen wie IoT und \u00dcberwachung und Analyse von Sensordaten</li> </ul>"},{"location":"le11/tsdb-einfuehrung/#anwendungsbereiche-von-influxdb","title":"Anwendungsbereiche von InfluxDB","text":"<ul> <li>\u00dcberwachung und Metriken: In der IT zur \u00dcberwachung von Servern, Netzwerken und Anwendungen.</li> <li>IoT (Internet der Dinge): Zur Verarbeitung und Analyse von Sensordaten aus IoT-Ger\u00e4ten.</li> <li>Finanzanalyse: Zur Analyse und \u00dcberwachung von Finanzmarktdaten und Handelsaktivit\u00e4ten.</li> </ul>"},{"location":"le11/tsdb-einfuehrung/#influxdb-konzepte","title":"InfluxDB-Konzepte","text":""},{"location":"le11/tsdb-einfuehrung/#bucket","title":"Bucket","text":"<p>In InfluxDB ist ein Bucket eine grundlegende Datenspeichereinheit, die sowohl Zeitreihendaten als auch Metadaten speichert. Hier sind einige wesentliche Punkte:</p> <ul> <li>Zeitbasierte Retention<ul> <li>Buckets haben eine vordefinierte Aufbewahrungsrichtlinie (RetentionPolicy), die bestimmt, wie lange die Daten aufbewahrt werden, bevor sie automatisch gel\u00f6scht werden. Dies hilft, den Speicherbedarf zu verwalten.</li> </ul> </li> <li>Namespace f\u00fcr Daten<ul> <li>Ein Bucket fungiert als Namespace f\u00fcr Messungen, Tags und Felder. Es hilft, die Daten organisatorisch zu trennen und zu verwalten.</li> </ul> </li> <li>Zugriffskontrolle<ul> <li>Buckets erm\u00f6glichen es, granulare Zugriffskontrollen durchzusetzen. Das bedeutet, dass Benutzer unterschiedliche Berechtigungen f\u00fcr verschiedene Buckets haben k\u00f6nnen.</li> </ul> </li> <li>Datenanreicherung und Abfragen<ul> <li>Daten k\u00f6nnen in Buckets geschrieben, abgerufen und analysiert werden, wobei InfluxDB's Abfragesprachen wie InfluxQL oder Flux verwendet werden.</li> </ul> </li> </ul> <p>Beispiel:</p> <p>Angenommen, du hast Sensordaten, die Temperatur- und Feuchtigkeitsmessungen enthalten. Du k\u00f6nntest einen Bucket namens <code>sensor_data</code> erstellen, um diese Daten zu speichern:</p> <p><pre><code>from(bucket: \"sensor_data\")\n  |&gt; range(start: -1h)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n  |&gt; mean()\n</code></pre> In diesem Beispiel filterst du Daten aus dem <code>sensor_data</code> Bucket und berechnest den Durchschnitt der Temperaturmessungen der letzten Stunde.</p> <p>Ein Bucket in InfluxDB ist ein zentraler Punkt, um Daten mit klaren Aufbewahrungsrichtlinien, Zugriffssteuerungen und einer strukturierten Organisation zu verwalten. Es ist ein grundlegender Baustein im Datenmanagement mit InfluxDB.</p>"},{"location":"le11/tsdb-einfuehrung/#measurement-messung","title":"Measurement (Messung)","text":"<p>In InfluxDB ist ein Measurement (Messung) ein logisches Konzept, das einer Tabelle in relationalen Datenbanken \u00e4hnelt.</p> <p>Konzept eines Measurements</p> <ul> <li> <p>Sammlung von Datenpunkten</p> <ul> <li>Ein Measurement ist eine Sammlung von Datenpunkten, die bestimmte Werte \u00fcber die Zeit speichern. Zum Beispiel k\u00f6nnten Temperaturmessungen, CPU-Nutzungsdaten oder Netzwerkbandbreite alle in separaten Measurements gespeichert werden.</li> </ul> </li> <li> <p>Name des Measurements</p> <ul> <li>Jedes Measurement hat einen Namen, der es eindeutig identifiziert. Der Name sollte beschreibend sein und klar machen, welche Art von Daten darin gespeichert wird. Zum Beispiel: <code>temperature</code>, <code>cpu_usage</code>, <code>network_traffic</code>.</li> </ul> </li> <li> <p>Felddaten und Tags</p> <ul> <li>Fields (Felder): Dies sind die tats\u00e4chlichen gemessenen Werte, z.B. die Temperatur in Grad Celsius oder die CPU-Auslastung in Prozent. Fields sind nicht indexiert und k\u00f6nnen numerisch, textuell oder boolish sein.</li> <li>Tags: Tags sind indexierte Metadaten, die Datenpunkte kategorisieren. Sie erm\u00f6glichen schnelle Abfragen und Gruppierungen. Ein Beispiel f\u00fcr Tags k\u00f6nnte der Standort eines Sensors oder der Hostname eines Servers sein.</li> </ul> </li> </ul> <p>Beispiel f\u00fcr ein Measurement</p> <p>Angenommen, du hast ein Measurement namens temperature:</p> <ul> <li>Fields: value (die tats\u00e4chliche Temperaturmessung)</li> <li>Tags: location (z.B. \"living_room\", \"kitchen\")</li> </ul> <p>Datenpunkte k\u00f6nnten so aussehen:</p> <pre><code>time                | location     | value\n--------------------|--------------|------\n2023-01-01T00:00:00Z| living_room  | 21.5\n2023-01-01T00:05:00Z| kitchen      | 22.3\n</code></pre> <p>Abfrage eines Measurements</p> <p>Hier ist ein Beispiel, wie du mit Flux-Daten aus einem Measurement abfragen k\u00f6nntest:</p> <pre><code>from(bucket: \"my_bucket\")\n  |&gt; range(start: -1d)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n  |&gt; filter(fn: (r) =&gt; r.location == \"living_room\")\n  |&gt; mean()\n</code></pre> <p>Ein Measurement in InfluxDB ist eine zentrale Komponente, die eine Sammlung von Datenpunkten darstellt, die \u00fcber die Zeit hinweg erfasst wurden. Es enth\u00e4lt Fields f\u00fcr die tats\u00e4chlichen Messwerte und Tags f\u00fcr die Kategorisierung und schnelle Abfrage von Daten.</p>"},{"location":"le11/tsdb-einfuehrung/#konzeptvergleich-fields-und-tags-gegenuber-einem-rdbms","title":"Konzeptvergleich Fields und Tags gegen\u00fcber einem RDBMS","text":"<ul> <li> <p>Fields und Tags bei InfluxDB k\u00f6nnen wie folgt mit einem relationalen Datenbankmanagementsystem (RDBMS) verglichen werden:</p> <ul> <li>Fields: In InfluxDB enthalten Fields die eigentlichen Messwerte (Datenpunkte), die Sie speichern. Sie entsprechen den Spaltenwerten in einer Tabelle eines RDBMS. Beispielsweise k\u00f6nnten Temperatur und Feuchtigkeit in einer Wetterdatenbank als Fields gespeichert werden. Fields sind nicht indexiert, was bedeutet, dass Abfragen, die auf Fields basieren, langsamer sein k\u00f6nnen, insbesondere bei grossen Datenmengen.</li> <li>Tags: Tags sind Metadaten, die verwendet werden, um Daten in InfluxDB zu indexieren und zu organisieren. Tags entsprechen den Schl\u00fcsseln in einer Datenbank, die verwendet werden, um Abfragen zu optimieren. In einem RDBMS w\u00fcrden Tags den indexierten Spalten oder den Prim\u00e4r- und Fremdschl\u00fcsseln \u00e4hnlich sein. Zum Beispiel k\u00f6nnten der Standort (z.B. Stadtname) und der Sensortyp als Tags gespeichert werden.</li> </ul> </li> </ul> <p>Hier eine Vergleichstabelle zur Verdeutlichung:</p> InfluxDB RDBMS Fields Spaltenwerte Tags Indexierte Spalten/Schl\u00fcssel <p>Tags werden verwendet, um Daten effizient zu finden und zu filtern, w\u00e4hrend Fields die spezifischen Messwerte enthalten.</p>"},{"location":"le11/tsdb-einfuehrung/#abfragesprache-flux","title":"Abfragesprache Flux","text":"<p>Die Abfragesprache Flux ist eine auf Zeitreihendaten spezialisierte Sprache, die f\u00fcr die Verwaltung und Analyse von Daten in InfluxDB entwickelt wurde.</p> Pipeline Architektur von FLUX <p>aus Flux documentation</p> <p>Eigenschaften:</p> <ol> <li> <p>Flexibilit\u00e4t und Ausdruckskraft</p> <ol> <li>Flux ist darauf ausgelegt, komplexe Datenanalysen und Transformationen einfach zu machen. Sie unterst\u00fctzt nicht nur einfache Abfragen, sondern auch fortgeschrittene Datenmanipulationen und Aggregationen.</li> </ol> </li> <li> <p>Pipeline-Architektur</p> <ol> <li>Flux verwendet eine Pipeline-Architektur, bei der Daten durch eine Reihe von Funktionen und Transformationen fliessen k\u00f6nnen. Dies erm\u00f6glicht es, Daten schrittweise zu filtern, zu transformieren und zu analysieren:   <pre><code>from(bucket: \"example-bucket\")\n|&gt; range(start: -1h)\n|&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n|&gt; mean()\n</code></pre></li> </ol> </li> <li>Integration von Zeitreihendaten und relationale Daten<ol> <li>Flux kann nicht nur mit Zeitreihendaten umgehen, sondern auch mit  relationalen Datenquellen interagieren. Dies erm\u00f6glicht die Kombination von Daten aus verschiedenen Quellen. Zum Beispiel MySQL.</li> </ol> </li> <li>Eingebaute Funktionen und Operatoren<ol> <li>Flux bietet eine Vielzahl von eingebauten Funktionen und Operatoren, die speziell f\u00fcr die Zeitreihenanalyse entwickelt wurden, wie Aggregationen (mean, max, min, sum), Filter, Join-Operationen und vieles mehr.</li> </ol> </li> <li> <p>Benutzerdefinierte Funktionen</p> <ol> <li>Mit Flux k\u00f6nnen Benutzer ihre eigenen Funktionen erstellen, um spezifische Analysen und Transformationen durchzuf\u00fchren. Dies erh\u00f6ht die Flexibilit\u00e4t und Anpassungsf\u00e4higkeit der Abfragesprache:   <pre><code>myFunc = (table=&lt;-) =&gt; table |&gt; range(start: -1h) |&gt; mean()\n\nfrom(bucket: \"example-bucket\")\n|&gt; myFunc()\n</code></pre></li> </ol> <p>siehe dazu: Define custom functions</p> </li> <li> <p>Support f\u00fcr Task Automation</p> <ol> <li>Flux kann f\u00fcr die Automatisierung von Aufgaben verwendet werden, z.B. f\u00fcr geplante Abfragen und Benachrichtigungen. Dies ist n\u00fctzlich f\u00fcr kontinuierliche Datenverarbeitung und \u00dcberwachung.</li> </ol> </li> <li>Cross-Database Queries<ol> <li>Flux erm\u00f6glicht Abfragen, die \u00fcber mehrere InfluxDB-Instanzen und -Datenbanken hinweg gehen, was f\u00fcr verteilte und hybride Cloud-Umgebungen n\u00fctzlich ist.</li> </ol> </li> </ol> <p>Beispiel eines Flux-Queries</p> <p>Hier ist ein Beispiel f\u00fcr eine typische Flux-Abfrage, die die Durchschnittstemperatur der letzten Stunde berechnet:</p> <pre><code>from(bucket: \"sensor_data\")\n  |&gt; range(start: -1h)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\")\n  |&gt; mean()\n  |&gt; yield(name: \"mean_temperature\")\n</code></pre> <p>Beispiel eines Flux-Cross-Queries mit MySQL</p> <pre><code>import \"sql\"\n\n// MySQL-Daten abrufen\nmysql_data = sql.from(\n    driverName: \"mysql\",\n    dataSourceName: \"user:password@tcp(localhost:3306)/db\",\n    query: \"SELECT * FROM example_table\"\n)\n\n// InfluxDB-Daten abrufen\ninflux_data = from(bucket: \"weather\")\n  |&gt; range(start: -24h)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"temperature\" or r._measurement == \"humidity\")\n\n// Daten zusammenf\u00fchren basierend auf einem gemeinsamen Schl\u00fcssel\ncombined_data = join(\n    tables: {mysql: mysql_data, influx: influx_data},\n    on: [\"common_column\"]\n)\n\ncombined_data\n</code></pre> <p>Flux ist eine Abfragesprache, die speziell f\u00fcr die Arbeit mit Zeitreihendaten optimiert ist. Ihre Eigenschaften erm\u00f6glichen Datenanalysen, die \u00fcber einfache SQL-Abfragen hinausgehen.</p>"},{"location":"le11/tsdb-einfuehrung/#hinweis-zu-query-sprachen-in-influxdb","title":"Hinweis zu Query-Sprachen in InfluxDB","text":"<p>Query Sprachen in InfluxDB</p> <p>Bei den Versionen 0.x und 1.x von InfluxDB wurde die Sprache InfluxQL verwendet. Wir arbeiten hier mit der Version 2.7. In Version 2.x wurde InfluxQL durch Flux abgel\u00f6st. Hinweise dazu finden sich hier unter Query data with InfluxQL.</p>"},{"location":"le11/tsdb-einfuehrung/#links","title":"Links","text":"<p>Get started with InfluxDB and Key Concepts</p>"},{"location":"le11/ue11-01/","title":"InfluxDB \u00dcbung","text":"<p>UE11-01-Wetterdaten importieren mit Python Skript</p> <p>Erstelle ein Python-Skript, das Wetterdaten von der OpenWeatherMap-API abruft und in ein InfluxDB-Bucket f\u00fcr den Standort die Standorte Berne, Basel, Samedan und Montreux importiert.</p> <p>Dazu ben\u00f6tigst Du die One Call API 3.0 von OpenWeather. Damit sind 1,000 API calls per day for free.</p> <p>Stelle die Wetterdaten im Data Explorer grafisch dar.</p> L\u00f6sungsvorschlag Python-Skript ue11-01-ReadImportOpenWeaterDATA.py<pre><code>import requests\nimport influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime, timezone\n\n# OpenWeatherMap API-Schl\u00fcssel und Standort konfigurieren\napi_key = \"YOUR WEATER API KEY\"\ncities = [\"Berne\", \"Basel\", \"Samedan\", \"Montreux\"]\ncountry = \"CH\"\nurl_template = \"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&amp;appid={api_key}&amp;units=metric\"\n\n# Verbindung zu InfluxDB herstellen\nbucket = \"weather_data\"\norg = \"BFH\"\ntoken = \"YOUR INFLUX TOKEN\"\nurl_influx = \"http://localhost:8086\"\n\nclient = influxdb_client.InfluxDBClient(\n    url=url_influx,\n    token=token,\n    org=org\n)\n\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\n# Funktion zum Abrufen der Wetterdaten\ndef get_weather_data(city, country):\n    url = url_template.format(city=city, country=country, api_key=api_key)\n    response = requests.get(url)\n    data = response.json()\n    if response.status_code == 200:\n        return {\n            \"temperature\": data[\"main\"][\"temp\"],\n            \"humidity\": data[\"main\"][\"humidity\"],\n            \"pressure\": data[\"main\"][\"pressure\"],\n            \"weather\": data[\"weather\"][0][\"description\"],\n            \"wind_speed\": data[\"wind\"][\"speed\"],\n            \"wind_deg\": data[\"wind\"][\"deg\"],\n            \"timestamp\": datetime.now(timezone.utc)\n        }\n    else:\n        print(f\"Fehler: {data}\")\n        return None\n\n# Wetterdaten f\u00fcr alle St\u00e4dte abrufen und in InfluxDB schreiben\nfor city in cities:\n    weather_data = get_weather_data(city, country)\n    if weather_data:\n        point = influxdb_client.Point(\"weather\") \\\n            .tag(\"location\", city) \\\n            .field(\"temperature\", weather_data[\"temperature\"]) \\\n            .field(\"humidity\", weather_data[\"humidity\"]) \\\n            .field(\"pressure\", weather_data[\"pressure\"]) \\\n            .field(\"weather\", weather_data[\"weather\"]) \\\n            .field(\"wind_speed\", weather_data[\"wind_speed\"]) \\\n            .field(\"wind_deg\", weather_data[\"wind_deg\"]) \\\n            .time(weather_data[\"timestamp\"])\n\n        # Datenpunkt schreiben\n        write_api.write(bucket=bucket, record=point)\n        print(f\"Wetterdaten f\u00fcr {city} erfolgreich in InfluxDB geschrieben\")\n\n# Verbindung schliessen\nclient.close()\n</code></pre>"},{"location":"le11/ue11-02/","title":"InfluxDB \u00dcbung 2","text":""},{"location":"le11/ue11-02/#aufgabe-1","title":"Aufgabe 1","text":"<p>UE11-02-Wetterdaten importieren und Abfragen sowie L\u00f6schvorg\u00e4nge mit Flux ausf\u00fchren</p> <ol> <li>Erstelle eine einfache Datenbank (Bucket) mit einigen Datens\u00e4tzen zum Wetter. Die Daten kommen von 3 Wetterstationen <code>Station1</code>, <code>Station2</code> und <code>Station3</code>. Jede Station zeichnet die Temperatur, die Feuchtigkeit und die Uhrzeit jede Stunde auf. Die Wetterstationen sollen als Tags gespeichert werden. Der Bucket, welcher Wetterdaten abspeichert hat den Namen <code>weather</code>. Das Einf\u00fcgen der Daten in den Bucket soll mit einem Python-Script erfolgen.</li> <li>F\u00fchre mit Flux Abfragen im Data Explorer durch. InfluxDB UI - Execute a Flux query<ol> <li>Alle Daten selektieren</li> <li>Daten von <code>Station1</code> selektieren. Hinweis hierzu: Um Selektionen auf Tags in Flux zu machen, nutzt du die <code>filter</code>-Funktion. Tags sind in Flux einfach zu filtern, da sie indexiert sind und schnelle Abfragen erm\u00f6glichen.</li> <li>Durchschnittstemperatur berechnen</li> <li>Daten innerhalb eines bestimmten Zeitraums selektieren</li> </ol> </li> <li>Daten mit Flux l\u00f6schen<ol> <li>Alle Daten von <code>Station2</code> l\u00f6schen</li> <li>Daten innerhalb eines bestimmten Zeitraums l\u00f6schen</li> </ol> </li> </ol> <p>Wie man Daten in einen Bucket schreibt</p> <p>Write data to InfluxDB with Python</p>"},{"location":"le11/ue11-02/#losungsvorschlage","title":"L\u00f6sungsvorschl\u00e4ge","text":"L\u00f6sungsvorschlag Python-Skript zum Generieren und Laden von Wetterdaten f\u00fcr 3 Wetterstationen UE11-02-Station123Weather.py<pre><code>import random\nimport influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime, timedelta\n\n# Verbindung zu InfluxDB herstellen\nbucket = \"weather\"\norg = \"BFH\"\ntoken = \"IHR TOKEN\"\nurl = \"http://localhost:8086\"\n\nclient = influxdb_client.InfluxDBClient(\n    url=url,\n    token=token,\n    org=org\n)\n\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\n\n# Beispiel-Daten von 3 Wetterstationen hinzuf\u00fcgen\nstations = [\"Station1\", \"Station2\", \"Station3\"]\nstart_time = datetime.utcnow() - timedelta(days=2)  # Vor 2 Tagen starten\n\ndata_points = []\nfor station in stations:\n    current_time = start_time\n    for _ in range(100):  # 100 verschiedene Werte f\u00fcr jede Station\n        temperature = round(random.uniform(15, 25), 2)  # Zuf\u00e4llige Temperatur zwischen 15 und 25 Grad\n        humidity = round(random.uniform(50, 80), 2)     # Zuf\u00e4llige Feuchtigkeit zwischen 50 und 80 Prozent\n        point = influxdb_client.Point(\"weather\") \\\n            .tag(\"station\", station) \\\n            .field(\"temperature\", temperature) \\\n            .field(\"humidity\", humidity) \\\n            .time(current_time)\n\n        data_points.append(point)\n        current_time += timedelta(hours=1)\n\n# Datenpunkte in den Bucket schreiben\nwrite_api.write(bucket=bucket, record=data_points)\nprint(\"Wetterdaten erfolgreich in den InfluxDB-Bucket geschrieben\")\n\n# Verbindung schliessen\nclient.close()\n</code></pre> <p>Query-Bedingung</p> <p>Alle Daten selektieren</p> FLUX <pre><code>from(bucket: \"weather\")\n|&gt; range(start: -2d)\n|&gt; filter(fn: (r) =&gt; r._measurement == \"weather\")\n</code></pre> <p>Query-Bedingung</p> <p>Daten von Station1 selektieren</p> FLUX <pre><code>from(bucket: \"weather\")\n|&gt; range(start: -2d)\n|&gt; filter(fn: (r) =&gt; r._measurement == \"weather\" and r.station == \"Station1\")\n</code></pre> <p>Query-Bedingung</p> <p>Durchschnittstemperatur berechnen</p> FLUX <pre><code>from(bucket: \"weather\")\n|&gt; range(start: -2d)\n|&gt; filter(fn: (r) =&gt; r._measurement == \"weather\")\n|&gt; mean(column: \"temperature\")\n</code></pre> <p>Query-Bedingung</p> <p>Daten innerhalb eines bestimmten Zeitraums selektieren</p> FLUX <pre><code>from(bucket: \"weather\")\n|&gt; range(start: 2023-12-05T00:00:00Z, stop: 2023-12-07T00:00:00Z)\n|&gt; filter(fn: (r) =&gt; r._measurement == \"weather\")\n</code></pre> <p>Query-Bedingung</p> <p>Durchschnittstemperatur berechnen</p> FLUX <pre><code>from(bucket: \"weather\")\n|&gt; range(start: -2d)\n|&gt; filter(fn: (r) =&gt; r._measurement == \"weather\")\n|&gt; mean(column: \"temperature\")\n</code></pre> <p>Query-Bedingung</p> <p>Alle Daten von Station2 l\u00f6schen</p> FLUX <pre><code>import \"influxdata/influxdb/v1\"\n\nv1.delete(\n  bucket: \"weather\",\n  predicate: (r) =&gt; r._measurement == \"weather\" and r.station == \"Station2\",\n  start: -30d,\n  stop: now()\n)\n</code></pre> <p>Query-Bedingung</p> <p>Daten innerhalb eines bestimmten Zeitraums l\u00f6schen</p> FLUX <pre><code>import \"influxdata/influxdb/v1\"\n\nv1.delete(\n  bucket: \"weather\",\n  predicate: (r) =&gt; r._measurement == \"weather\",\n  start: 2023-12-05T00:00:00Z,\n  stop: 2023-12-07T00:00:00Z\n)\n</code></pre>"},{"location":"le11/ue11-02/#aufgabe-2","title":"Aufgabe 2","text":"<p>UE11-02-InfluxDB-CLI</p> <p>In anderen Modulen werden TSDB und insb. InfluxDB ein Thema bleiben. Lernen Sie auch das CLI (Command Line Interface) kennen. Abfragen mit dem CLI sind praktisch und schnell.</p> <p>CLI Installation</p> <p>F\u00fchren Sie die Abfragen von oben mit dem CLI aus.</p> <p>CLI Execute a Flux query</p>"},{"location":"le11/ue11-03/","title":"InfluxDB \u00dcbung 3 als Option","text":"<p>Das Schreiben von Sensorwerten mit dem ESP32 und einem Sensor in eine InfluxDB ist fakultativ und greift thematisch vor.</p> BMP280-Sensordaten in InfluxDB speichern <p>Eine Einf\u00fchrung zu dieser Problemstellung findest du hier und hier.</p>"},{"location":"le11/ue11-04/","title":"UE11-04-Integrierte Aufgabe RDBMS (MySQL) - TSDB (InfluxDB)","text":""},{"location":"le11/ue11-04/#problemstellung","title":"Problemstellung","text":"<p>Nehme an, Du arbeitest als Wirtschaftsingenieur in einem Industrieunternehmen mit mehreren Standorten. Du bist unter anderem verantwortlich f\u00fcr die Geb\u00e4udeautomation. Dazu werden viele Sensordaten verarbeitet um folgende Bereiche optimal zu betreiben:</p> <ol> <li>intelligente Lichtsteuerung der Geb\u00e4ude und R\u00e4umlichkeiten</li> <li>Heizung, Ventilation, Klima</li> <li>Zugangskontrollen</li> <li>Unterhalt und Wartung von Anlagen</li> <li>Lagermanagement</li> <li>Energiemanagement</li> <li>Remote Monitoring von Produktionsanlagen</li> </ol> <p>Die Verwaltung der Sensorsysteme soll in einem RDBMS verwaltet werden. Die erzeugten Sensorwerte werden in einer TSDB (InfluxDB) gespeichert.</p> <p>F\u00fcr die Verwaltung der Sensorsysteme (statische Verwaltung)  gelten folgende Anforderungen:</p> <ul> <li>Ein Sensor kann einen oder auch mehrere Sensorwerte registrieren. Bsp: BME280 (Temperatur, Feuchtigkeit und Luftdruck) </li> <li>Jeder Sensorwert kann mit einer physikalischen Einheit oder einem boolschen Wert beschrieben werden.</li> <li>Jeder Sensor hat Eigenschaften, welche in einem technischen Datenblatt (pdf oder http-Link) beschrieben sind.</li> <li>F\u00fcr einen Sensor gibt es eine Typenbezeichnung, einen Hersteller und einen Lieferanten.</li> <li>Ein Sensorelement hat einen Preis und ein Inbetriebnahmedatum. Es kann aber auch inaktiv in einem Lager liegen. </li> <li>F\u00fcr jeden Sensorwert kann der Ort der Erfassung eindeutig identifiziert werden</li> <li>Ein Standort ist gekennzeichnet durch ein Geb\u00e4ude mit Adresse und Raumnummer. Eine Standortbeschreibung    kann auch keine postalische Adresse sein.  </li> <li>Ein Sensorwert kann einem der 7 Bereiche zugeordnet werden.</li> </ul>"},{"location":"le11/ue11-04/#aufgaben","title":"Aufgaben","text":"<p>Die Aufgaben k\u00f6nnen im Team bearbeitet werden und ist eine gute \u00dcbung f\u00fcr die Pr\u00fcfung am Ende des Semesters.</p> <p>a. Erstelle dazu ein konzeptionelles ERM.</p> <p>b. Erstelle ein logisches ERM mit Beispieldatens\u00e4tzen.</p> <p>c. Die Daten der Sensorinfrastruktur werden in MySQL verwaltet und die Sensorwerte in InfluxDB.</p> <p>d. Erstelle die Tabellen mit SQL mit Beispiel-Datens\u00e4tzen</p> <p>e. Wie gedenkst du die Zeitserien der einzelnen Sensorwerte in InfluxDB mit den Eigenschaften der Sensoren zu verkn\u00fcfen?</p> <p>Bsp: Wenn Sensorwerte fehlerhaft sind oder g\u00e4nzlich fehlen: wie identifizieren sie den Standort der Sensorkomponente? Wie identifizieren sie den Sensortyp, den Einbauort? Zu welcher Sensorkomponente geh\u00f6rt ein einzelner Sensorwert? Welche anderen Sensoren haben denselben Einbauort?</p> <p>f. Erstelle zu den Fragen aus e. enstsprechende SQL&amp;FLUX-Abfragen.</p> <p>g. Pr\u00e4sentiert das ERM und eure Gedanken zu dieser Problemstellung der Klasse</p>"},{"location":"le11/ue11-04/#beispiele-von-sensoren","title":"Beispiele von Sensoren","text":"<p>Hier einige Sensoren als Beispiele. Beachten Sie, dass viele Sensorkomponenten mehrere Sensorsignale liefern. Ihr BMP280 vom ESP32-Kit liefert beispielsweise zwei Signale: Temperatur in \u00b0C und Luftdruck in Pascal. </p> <p>In einer Datenbank, m\u00fcssen wir die Signale als Einzelsignale identifizieren k\u00f6nnen. Das bedeutet, dass es m\u00f6glich sein muss, ein einzelnes Sensorsignal einer Sensorkomponente zuzuweisen. Dies ist dann von Bedeutung, wenn wir mit einem Signal Probleme haben. Dann sollten wir wissen, welche Komponente betroffen ist.</p>"},{"location":"le11/ue11-04/#pir","title":"PIR","text":"<p>Motion Detector</p> <p>misst:</p> <ul> <li>Wert 1 bei Bewegung, Wert 0 keine Bewegung</li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#bmp280","title":"BMP280","text":"<p>misst:</p> <ul> <li>Air-Pressure in Pascal</li> <li>Temperature in \u00b0C</li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#bme680","title":"BME680","text":"<p>misst:</p> <ul> <li>Air-Pressure in Pascal</li> <li>Temperature in \u00b0C</li> <li>Humidity in rel. %</li> <li>Gas</li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#am2302","title":"AM2302","text":"<p>misst:</p> <ul> <li>Temperature in \u00b0C</li> <li>Humidity in rel. %</li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#bh1750","title":"BH1750","text":"<p>misst:</p> <ul> <li>Luminosity in LUX</li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#tcs34725","title":"TCS34725","text":"<p>misst:</p> <ul> <li>RGB color sensor. Output as RGB-Hex-Value, eg:  <code>#F5151B</code></li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#mpu-9250","title":"MPU-9250","text":"<p>Gyroscope oder Inertial Motion Unit</p> <p>misst:</p> <ul> <li>gibt Werte f\u00fcr yaw, pitch und roll</li> </ul> <p>Datasheet</p> <p>Bild zu Yaw, Pitch und Roll</p>"},{"location":"le11/ue11-04/#senseair-k30","title":"Senseair K30","text":"<p>CO2-Sensor</p> <p>misst:</p> <ul> <li>CO2 Werte in ppm</li> </ul> <p>Datasheet</p>"},{"location":"le11/ue11-04/#losungsvorschlage","title":"L\u00d6SUNGSVORSCHL\u00c4GE","text":"Schritt 1: Konzeptionelles ERM <p>Das ist eine m\u00f6gliche Darstellungsform mit einem Tool (hier draw.io). Das Diagramm kann aber auch von Hand erstellt werden. Dabei werden die Entit\u00e4ten als Kasten und die Attribute als Bubbles dargestellt. Mit PK und FK werden die Keys Primary Key und Foreign Key bezeichnet.</p> <p>Beachte hier im Beispiel die Verbindung <code>Component</code> - <code>Location</code>. Diese Beziehung realisiert verbaut in mit dem Attribut startDateOperation. Dieses Merkmal ist ein Beziehungsattribut, welches wir in der Verbindungstabelle aufnehmen m\u00fcssen! Die Verbindungstabellen, also die Aufl\u00f6sung von <code>m:m</code>-Beziehungen, nehmen wir im logischen ERM vor.</p> <p> konzeptionelles ERM </p> Schritt 2: Logisches ERM <p>Das logische ERM ist die Vorgabe f\u00fcr das physische ERM. Hier werden die <code>m:m</code>-Beziehungen mit Verbindungstabellen aufgel\u00f6st. Alle PK und FK werden erg\u00e4nzt und die Attribute komplett aufgenommen. Eine gute Praxis ist in diesem Schritt auch die Darstellung von Demo-Datens\u00e4tzen. Dies erm\u00f6glicht dem ERM-Designer seine ERM-Struktur zu \u00fcberpr\u00fcfen und hilft dem Betrachter das ERM schneller zu verstehen.  </p> <p> logisches ERM </p> <p> Demo-Records f\u00fcr einige Tabellen </p> Schritt 3: Physisches ERM (SQL-Create-Statements) <p>Das physische ERM baut auf dem logischen ERM auf. Als DB-Entwickler und SQL-Expert bietet das logische ERM alle Informationen, um die DB-Struktur physisch zu realisieren. M\u00f6gliche Ausnahme: INDEXE ! Diese werden oft auch im Nachhinein definiert, um die Query-Performance zu optimieren.</p> SQLs: CREATE TABLES AND INSERT DEMO RECORDS<pre><code>-- Schema sensors erstellen, mit\nCREATE DATABASE sensors;\n-- Fokus auf sensors-DB\nUSE sensors;\n/* -------------CREATE TABLES and INSERT DEMO RECODRS-------------------------------------- */\nCREATE TABLE `sector` (\n  `idsector` int NOT NULL,\n  `description` varchar(45) DEFAULT NULL,\n  PRIMARY KEY (`idsector`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nselect * from sector;\nINSERT INTO sector (idsector, description) VALUES (1, 'Smart Lighting Systems');\nINSERT INTO sector (idsector, description) VALUES (2, 'Automated HVAC Systems');\nINSERT INTO sector (idsector, description) VALUES (3, 'Security and Access Control');\nINSERT INTO sector (idsector, description) VALUES (4, 'Predictive Maintenance');\nINSERT INTO sector (idsector, description) VALUES (5, 'Inventory Management');\nINSERT INTO sector (idsector, description) VALUES (6, 'Energy Management');\nINSERT INTO sector (idsector, description) VALUES (7, 'Remote Monitoring and Control');\nINSERT INTO sector (idsector, description) VALUES (8, 'other');\nselect * from sector;\n/* --------------------------------------------------------------------------------------- */\nCREATE TABLE `unit` (\n  `idunit` int NOT NULL,\n  `description` varchar(45) DEFAULT NULL,\n  `unitname` varchar(45) DEFAULT NULL,\n  `abbreviation` varchar(45) DEFAULT NULL,\n  PRIMARY KEY (`idunit`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nselect * from unit;\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('1', 'Temperature', 'Degree     Celsius', '\u00b0C');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('2', 'Humidity', 'relative     Humidity', '%');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('3', 'Air Pressure', 'Pascal', 'Pa');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('4', 'Luminosity', 'Lumen', 'Lux');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('5', 'Geo Position', 'Latitudue and     Longitude', 'Lat_Long');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('6', 'Yaw_Pitch_Roll', 'Angle',     'Deg');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('7', 'Saturation CO2', 'CO2 Parts     per Million', 'ppm');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('8', 'Color', 'RGB HEX Value', '#');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('9', 'Air Quality', 'Value 1..10',     'Dec');\nINSERT INTO unit (idunit, description, unitname, abbreviation) VALUES ('10', 'True or False', 'True_False',     'boolean');\nselect * from unit;\n/* --------------------------------------------------------------------------------------- */\nCREATE TABLE `sensor` (\n  `idsensor` int NOT NULL,\n  `description` varchar(45) DEFAULT NULL,\n  `precision` varchar(45) DEFAULT NULL,\n  `unit` int DEFAULT NULL,\n  `sector` int DEFAULT NULL,\n  PRIMARY KEY (`idsensor`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nselect * from sensor;\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('1', 'Temperature', '1%',     '1', '2');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('2', 'Air Pressure', '5%',     '3', '2');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('3', 'Temperature', '10%',     '1', '2');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('4', 'Humidity', '5%', '2',     '2');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('5', 'Luminosity', '1%', '4',     '1');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('6', 'CO2 Saturation', '1%',     '7', '2');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('7', 'Motion', '1%', '6',     '7');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('8', 'Color RGB', '0.1%',     '8', '3');\nINSERT INTO sensor (idsensor, description, `precision`, unit, sector) VALUES ('9', 'Air Quality', '2%',     '9', '2');\nselect * from sensor;\n/* --------------------------------------------------------------------------------------- */\nCREATE TABLE `component` (\n  `idcomponent` int NOT NULL,\n  `componentName` varchar(80) DEFAULT NULL,\n  `componentType` varchar(45) DEFAULT NULL,\n  `Manufacturer` varchar(45) DEFAULT NULL,\n  `Datasheet` varchar(200) DEFAULT NULL,\n  PRIMARY KEY (`idcomponent`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nselect * from component;\nINSERT INTO component (idcomponent, componentName, componentType, Manufacturer, Datasheet) VALUES (1,     'Digital Pressure Sensor', 'BMP280', 'Bosch', 'https://cdn-shop.adafruit.com/datasheets/BST-BMP280-DS001-11.    pdf');\nINSERT INTO component (idcomponent, componentName, componentType, Manufacturer, Datasheet) VALUES (2,     'Ambient Light Sensor', 'BH1750', 'Adafruit', 'https://cdn-learn.adafruit.com/downloads/pdf/    adafruit-bh1750-ambient-light-sensor.pdf');\nINSERT INTO component (idcomponent, componentName, componentType, Manufacturer, Datasheet) VALUES (3, 'CO2     Sensor', 'K30', 'Senseair', 'https://rmtplusstoragesenseair.blob.core.windows.net/docs/publicerat/PSH0131.    pdf');\nINSERT INTO component (idcomponent, componentName, componentType, Manufacturer, Datasheet) VALUES (4,     'Luftfeuchtigkeits-, Druck-, Temperatur- &amp; Luftg\u00fctesensor', 'BME680', 'Bosch', 'https://www.bosch-sensortec.    com/media/boschsensortec/downloads/datasheets/bst-bme680-ds001.pdf');\nINSERT INTO component (idcomponent, componentName, componentType, Manufacturer, Datasheet) VALUES (5,     'Luftfeuchtigkeits- &amp; Temperatur-Sensor', 'AM2302/DHT22', 'Bosch', 'https://www.bosch-sensortec.com/media/    boschsensortec/downloads/datasheets/bst-bme680-ds001.pdf');\nINSERT INTO component (idcomponent, componentName, componentType, Manufacturer, Datasheet) VALUES (6,     'Color Sensor', 'TCS34725', 'Waveshare', 'https://www.waveshare.com/wiki/TCS34725_Color_Sensor');\nselect * from component;\n/* --------------------------------------------------------------------------------------- */\n\nCREATE TABLE `sensor_component` (\n  `idsensor_component` int NOT NULL,\n  `idsensor` int DEFAULT NULL,\n  `idcomponent` int DEFAULT NULL,\n  PRIMARY KEY (`idsensor_component`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nselect * from sensor_component;\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (1, 1, 1);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (2, 2, 1);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (3, 1, 1);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (4, 2, 1);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (5, 1, 4);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (6, 2, 4);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (7, 4, 4);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (8, 4, 4);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (9, 5, 2);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (10, 5, 2);\nINSERT INTO sensor_component (idsensor_component, idsensor, idcomponent) VALUES (11, 8, 6);\nselect * from sensor_component;\n/* --------------------------------------------------------------------------------------- */\n\nCREATE TABLE `supplier` (\n  `idsupplier` int NOT NULL,\n  `name` varchar(45) DEFAULT NULL,\n  `orderLink` varchar(200) DEFAULT NULL,\n  PRIMARY KEY (`idsupplier`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nSELECT * FROM supplier;\nINSERT INTO supplier (idsupplier, name, orderLink) VALUES (1, 'Mouser', 'https://www.mouser.ch/');\nINSERT INTO supplier (idsupplier, name, orderLink) VALUES (2, 'DigiKey', 'https://www.digikey.ch/en');\nINSERT INTO supplier (idsupplier, name, orderLink) VALUES (3, 'Conrad', 'https://www.conrad.ch/');\nINSERT INTO supplier (idsupplier, name, orderLink) VALUES (4, 'Reichelt', 'https://www.reichelt.de/');\nINSERT INTO supplier (idsupplier, name, orderLink) VALUES (5, 'Bastelgarage', 'https://www.bastelgarage.ch/    ');\nINSERT INTO supplier (idsupplier, name, orderLink) VALUES (6, 'Adafruit', 'https://www.adafruit.com/');\nSELECT * FROM supplier;\n/* --------------------------------------------------------------------------------------- */\n\nCREATE TABLE `component_supplier` (\n  `idcomponent_supplier` int NOT NULL,\n  `idcomponent` int DEFAULT NULL,\n  `idsupplier` int DEFAULT NULL,\n  PRIMARY KEY (`idcomponent_supplier`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nSELECT * FROM component_supplier;\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (1, 1, 6);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (2, 1, 5);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (3, 2, 6);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (4, 3, 2);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (5, 3, 1);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (6, 4, 4);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (7, 4, 5);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (8, 5, 5);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (9, 5, 4);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (10, 6, 4);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (11, 6, 5);\nINSERT INTO component_supplier (idcomponent_supplier, idcomponent, idsupplier) VALUES (12, 6, 6);\nSELECT * FROM component_supplier;\n/* --------------------------------------------------------------------------------------- */\n\n\nCREATE TABLE `location` (\n  `idlocation` int NOT NULL,\n  `locationName` varchar(45) DEFAULT NULL,\n  `address` varchar(45) DEFAULT NULL,\n  `plz` int DEFAULT NULL,\n  `city` varchar(45) DEFAULT NULL,\n  `geoPosition` point DEFAULT NULL,\n  PRIMARY KEY (`idlocation`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nSELECT idlocation, address, plz, city, ST_AsText(geoposition) AS longitude_lattitude FROM location;\nINSERT INTO location (idlocation, locationName, address, plz, city, geoPosition) VALUES (1, 'Bern     Freiburgstrasse', 'Freiburgstrasse 13', '3000', 'Bern', POINT(7.44425, 46.9485));\nINSERT INTO location (idlocation, locationName, address, plz, city, geoPosition) VALUES (2, 'Bern     Lorraine', 'Lorrainestrasse 5B', '3000', 'Bern', POINT(7.4471, 46.9483)); \nINSERT INTO location (idlocation, locationName, address, plz, city, geoPosition) VALUES (3, 'Lausanne',     'rte de la gare 1', '3000', 'Bern', POINT(6.6242, 46.5098));\nINSERT INTO location (idlocation, locationName, address, plz, city, geoPosition) VALUES (4, 'Biel',     'Quellgasse 12', '2502', 'Biel', POINT(7.2470, 47.1350));\nINSERT INTO location (idlocation, locationName, address, plz, city, geoPosition) VALUES (5, 'Biel Lager',     'Quellgasse 12', '2502', 'Biel', POINT(7.2470, 47.1350));\nSELECT idlocation, address, plz, city, ST_AsText(geoposition) AS longitude_lattitude FROM location;\n/* --------------------------------------------------------------------------------------- */\n\nCREATE TABLE `instlocation` (\n  `idinstlocation` int NOT NULL,\n  `idsensorcomponent` int DEFAULT NULL,\n  `idlocation` int DEFAULT NULL,\n  `roomNr` varchar(45) DEFAULT NULL,\n  `startDateOperation` date DEFAULT NULL,\n  PRIMARY KEY (`idinstlocation`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;\n\n\nSELECT * FROM instlocation;\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr, startDateOperation) VALUES     (1, 1, 1, 12, '2022-05-28');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr, startDateOperation) VALUES     ('2', '2', '2', '303', '2024-03-18');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr, startDateOperation) VALUES     ('3', '3', '3', 'E1-16', '2021-02-02');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr, startDateOperation) VALUES     ('4', '4', '4', 'W14', '2024-12-14');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr, startDateOperation) VALUES     ('5', '2', '2', '303', '2024-06-01');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr, startDateOperation) VALUES     ('6', '4', '2', '301', '2024-08-06');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('7', '5', '5',     'W00');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('8', '6', '5',     'W00');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('9', '7', '5',     'W00');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('10', '8', '5',     'W00');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('11', '9', '5',     'W00');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('12', '10', '5',     'W00');\nINSERT INTO instlocation (idinstlocation, idsensorcomponent, idlocation, roomNr) VALUES ('13', '11', '5',     'W00');\nSELECT * FROM instlocation;\n/* --------------------------------------------------------------------------------------- */\n</code></pre> Query-SQL: Query aller Sensoren mit den Komponenten<pre><code>-- Alle Sensoren mit entsprechenden Komponenten\nSELECT * FROM sensor_component\nINNER JOIN sensor\n ON sensor.idsensor = sensor_component.idsensor\nINNER JOIN component\n ON component.idcomponent = sensor_component.idcomponent\n</code></pre> <p> Alle Sensoren mit entsprechenden Komponenten </p> Query-SQL: Alle Sensoren mit entsprechenden Komponenten, Einbaudatum und Location<pre><code>SELECT * FROM sensor_component\nINNER JOIN sensor\n ON sensor.idsensor = sensor_component.idsensor\nINNER JOIN component\n ON component.idcomponent = sensor_component.idcomponent\nINNER JOIN instlocation\n ON instlocation.idsensorcomponent = sensor_component.idsensor_component\nJOIN location \n ON location.idlocation = instlocation.idlocation\nORDER BY description\n</code></pre> <p> Alle Sensoren mit entsprechenden Komponenten und Location mit Startdate of Operation </p> <p>WICHTIG: zum Schluss erstellen wir noch die Referential Integrity </p> <pre><code>/* ---------- ALTER TABLES-define FOREIGN KEYS and REFERENTIAL INTEGRITY ---------------- */\n\n-- sensor-table     \n\nALTER TABLE `sensors`.`sensor` \nADD INDEX `FK_sector_idx` (`sector` ASC) VISIBLE;\n;\nALTER TABLE `sensors`.`sensor` \nADD CONSTRAINT `FK_sector`\n  FOREIGN KEY (`sector`)\n  REFERENCES `sensors`.`sector` (`idsector`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT;\n\n\n-- unit\nALTER TABLE `sensors`.`sensor` \nADD INDEX `FK_unit_idx` (`unit` ASC) VISIBLE;\n;\nALTER TABLE `sensors`.`sensor` \nADD CONSTRAINT `FK_unit`\n  FOREIGN KEY (`unit`)\n  REFERENCES `sensors`.`unit` (`idunit`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT;    \n\n\n-- sensor_component\nALTER TABLE `sensors`.`sensor_component` \nADD INDEX `FK_sensor_idx` (`idsensor` ASC) VISIBLE;\n;\nALTER TABLE `sensors`.`sensor_component` \nADD CONSTRAINT `FK_sensor`\n  FOREIGN KEY (`idsensor`)\n  REFERENCES `sensors`.`sensor` (`idsensor`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT;\n\nALTER TABLE `sensors`.`sensor_component` \nADD INDEX `FK_component_idx` (`idcomponent` ASC) VISIBLE;\n;\nALTER TABLE `sensors`.`sensor_component` \nADD CONSTRAINT `FK_component`\n  FOREIGN KEY (`idcomponent`)\n  REFERENCES `sensors`.`component` (`idcomponent`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT;    \n\n-- component-supplier    \n\nALTER TABLE `sensors`.`component_supplier` \nADD INDEX `FK_componentid_idx` (`idcomponent` ASC) VISIBLE,\nADD INDEX `FK_supplier_idx` (`idsupplier` ASC) VISIBLE;\n;\nALTER TABLE `sensors`.`component_supplier` \nADD CONSTRAINT `FK_componentid`\n  FOREIGN KEY (`idcomponent`)\n  REFERENCES `sensors`.`component` (`idcomponent`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT,\nADD CONSTRAINT `FK_supplier`\n  FOREIGN KEY (`idsupplier`)\n  REFERENCES `sensors`.`supplier` (`idsupplier`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT;\n\n-- instlocation    \n\nALTER TABLE `sensors`.`instlocation` \nADD INDEX `FK_sensorcomponent_idx` (`idsensorcomponent` ASC) VISIBLE,\nADD INDEX `FK_location_idx` (`idlocation` ASC) VISIBLE;\n;\nALTER TABLE `sensors`.`instlocation` \nADD CONSTRAINT `FK_sensorcomponent`\n  FOREIGN KEY (`idsensorcomponent`)\n  REFERENCES `sensors`.`sensor_component` (`idsensor_component`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT,\nADD CONSTRAINT `FK_location`\n  FOREIGN KEY (`idlocation`)\n  REFERENCES `sensors`.`location` (`idlocation`)\n  ON DELETE CASCADE\n  ON UPDATE RESTRICT;\n\n/* ------------------------------------------- */\n</code></pre> <p>Erst jetzt liefert der Reverse Engineer ein komplettes ERM mit Beziehungen und Kardinalit\u00e4ten.</p> <p> Mit Reverse Engineer kann das ERM jetzt automatisch erstellt werden. Pr\u00fcfe dieses! </p> Daten aus Influx mit MySQL verkn\u00fcpfen <p>Die Tabelle <code>sensor_component</code> mit dem PK <code>idsensor_component</code> identifiziert jedes Sensorsignal eindeutig. </p> <p>Ziel: wir wollen Daten aus MySQL (statische Daten) mit den dynamischen Sensorsignalen kombinieren. Ziel soll sein: </p> <p>Sensorwerte (aus InfluxDB) mit dem Standort (in MySQL gespeichert) sichtbar machen.</p> <p>Mit Hilfe des folgenden SQL's in MySQL kann der Standort eines Sensorsignals abgefragt werden:</p> <p>Query-SQL: Standort des Sensorsignals<pre><code>SELECT sensor_component.idsensor_component, locationName \n    FROM sensor_component\n    INNER JOIN sensor ON sensor.idsensor = sensor_component.idsensor\n    INNER JOIN component ON component.idcomponent = sensor_component.idcomponent\n    INNER JOIN instlocation ON instlocation.idsensorcomponent = sensor_component.idsensor_component\n    INNER JOIN location ON location.idlocation = instlocation.idlocation\n</code></pre> Standorte der Sensorsignale </p> <p>Damit wir in InfluxDB ein verbindendes Element in die MySQL-Datenbank haben, definieren wir einen <code>Tag</code> mit dem Namen des Primary Key, <code>idsensor_component</code>, der Tabelle <code>sensor_component</code>. Diese Tabelle enth\u00e4lt jedes einzelne Sensorsignal und die dazugeh\u00f6renden statischen Daten, neben anderem also auch der <code>locationName</code>.</p> <p>Hier das Python-Skript, welches zuf\u00e4llige Sensordaten mit dem Tag <code>idsensor_component</code> erzeugt. Wir nehmen hier als Beispiel die id's 1 und 2. 1 geh\u00f6rt zu Bern Freiburgstrasse und 2 geh\u00f6rt zur Location Bern Lorraine.</p> <p>Beachte die markierten Code-Zeilen!</p> Erzeugung von Sensordaten mit tag *idsensor_component*<pre><code>import influxdb_client\nfrom influxdb_client.client.write_api import SYNCHRONOUS\nfrom datetime import datetime, timedelta, timezone\nimport random    \n\n\nclient = influxdb_client.InfluxDBClient(url=\"http://localhost:8086\", token=\"DEIN TOKEN\", org=\"BFH\")    \n\nbucket=\"DEMOBUCKET\"    \n\nwrite_api = client.write_api(write_options=SYNCHRONOUS)    \n\n\ndef generate_sensor_data(num_points):\n    data = []\n    current_time = datetime.now(timezone.utc)\n    for i in range(num_points):\n        # Temperaturdaten in Grad Celsius normalverteilt zwischen 15 und 25\u00b0\n        temp_data = f\"temperature,idsensor_component=1 value={round(random.uniform(15.0, 25.0), 1)} {int    (current_time.timestamp() * 1e9)}\"\n        data.append(temp_data)\n\n\n        # Luftdruckdaten in normalverteilt zwischen 950 und 1050 hPa\n        pressure_data = f\"pressure,idsensor_component=2 value={round(random.uniform(950.0, 1050.0), 1)} {int    (current_time.timestamp() * 1e9)}\"\n        data.append(pressure_data)\n\n        # die current time wird nach jedem loop um 5Min zur\u00fcckgesetzt. So entsteht eine Time Serie\n        current_time -= timedelta(minutes=5)\n    return data    \n\n# Generiere 100 Datenpunkte\nsensor_data = generate_sensor_data(100)\n# Schreibe die generierten Datenpunkte in InfluxDB\nwrite_api.write(bucket=bucket, record=sensor_data)    \n\nprint(\"Daten erfolgreich geschrieben\")\n</code></pre> <p>Nachdem wir einige 100 Sensor-Datens\u00e4tze nach InfluxDB geschrieben haben, wollen wir diese Abfragen. Die Abfrage soll den Sensorwert (aus Influx) und auch die Location (aus MySQL) anzeigen:</p> <p> Influx-Timeseries mit MySQL-Daten kombiniert </p> <p>Das Flux-Query-Statement wird mit dem Query Builder im Data Explorer eingegeben. Schieben Sie den Switch auf View Raw Data.</p> <p> Query Builder im Data Explorer </p> <p>Passen Sie ihre Credentials an!</p> <pre><code>import \"sql\"\nimport \"experimental\"\n\n// MySQL data retrieval\nmysql_data = sql.from(\n    driverName: \"mysql\",\n    dataSourceName: \"tom:password@tcp(hercule.lan.tj:3306)/sensors\",\n    query: \"\n        SELECT \n            sensor_component.idsensor_component,\n            locationName \n        FROM sensor_component\n        INNER JOIN sensor ON sensor.idsensor = sensor_component.idsensor\n        INNER JOIN component ON component.idcomponent = sensor_component.idcomponent\n        INNER JOIN instlocation ON instlocation.idsensorcomponent = sensor_component.idsensor_component\n        INNER JOIN location ON location.idlocation = instlocation.idlocation\n    \"\n)\n|&gt; yield(name: \"mysql_data\")  // Debug: Verify MySQL data output\n\n// InfluxDB data retrieval with conversion of idsensor_component to int\ninflux_data = from(bucket: \"DEMOBUCKET\")\n  |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"pressure\" or r._measurement == \"temperature\")\n  |&gt; filter(fn: (r) =&gt; r._field == \"value\")\n  |&gt; map(fn: (r) =&gt; ({ r with idsensor_component: int(v: r.idsensor_component) }))\n  |&gt; filter(fn: (r) =&gt; r.idsensor_component == 1 or r.idsensor_component == 2)\n  |&gt; aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |&gt; yield(name: \"influx_data\")  // Debug: Verify InfluxDB data output\n\n// Join operation\ncombined_data = join(\n    tables: {mysql: mysql_data, influx: influx_data},\n    on: [\"idsensor_component\"]\n)\n|&gt; map(fn: (r) =&gt; ({\n    _time: r._time,\n    _measurement: r._measurement,\n    _field: r._field,\n    _value: r._value,\n    idsensor_component: r.idsensor_component,\n    locationName: r.locationName\n}))\n|&gt; yield(name: \"combined_data\")\n</code></pre> <p>Erkl\u00e4rungen zum Code:</p> <p><pre><code>import \"sql\"\nimport \"experimental\"\n</code></pre> Diese Anweisungen importieren die notwendigen Pakete f\u00fcr den SQL-Zugriff und experimentelle Funktionen in Flux.</p> <p>Abrufen der MySQL-Daten</p> <p><pre><code>mysql_data = sql.from(\ndriverName: \"mysql\",\ndataSourceName: \"tom:password@tcp(hercule.lan.tj:3306)/sensors\",\nquery: \"\n    SELECT \n        sensor_component.idsensor_component,\n        locationName \n    FROM sensor_component\n    INNER JOIN sensor ON sensor.idsensor = sensor_component.idsensor\n    INNER JOIN component ON component.idcomponent = sensor_component.idcomponent\n    INNER JOIN instlocation ON instlocation.idsensorcomponent = sensor_component.idsensor_component\n    INNER JOIN location ON location.idlocation = instlocation.idlocation\n\"\n)\n|&gt; yield(name: \"mysql_data\")  // Debug: \u00dcberpr\u00fcfung der MySQL-Daten\n</code></pre> Dies ruft Daten aus der MySQL-Datenbank ab, indem die SQL-Abfrage von oben verwendet wird, und speichert sie in der Variablen <code>mysql_data</code>. Die Abfrage holt <code>idsensor_component</code> und <code>locationName</code>, indem mehrere Tabellen verbunden werden. Die <code>yield</code>-Anweisung wird zum Debuggen verwendet, um sicherzustellen, dass die Daten korrekt abgerufen werden.</p> <p>Abrufen der InfluxDB-Daten und Konvertierung</p> <p><pre><code>influx_data = from(bucket: \"DEMOBUCKET\")\n  |&gt; range(start: v.timeRangeStart, stop: v.timeRangeStop)\n  |&gt; filter(fn: (r) =&gt; r._measurement == \"pressure\" or r._measurement == \"temperature\")\n  |&gt; filter(fn: (r) =&gt; r._field == \"value\")\n  |&gt; map(fn: (r) =&gt; ({ r with idsensor_component: int(v: r.idsensor_component) }))\n  |&gt; filter(fn: (r) =&gt; r.idsensor_component == 1 or r.idsensor_component == 2)\n  |&gt; aggregateWindow(every: v.windowPeriod, fn: mean, createEmpty: false)\n  |&gt; yield(name: \"influx_data\")  // Debug: \u00dcberpr\u00fcfung der InfluxDB-Daten\n</code></pre> Dies ruft Zeitreihendaten aus InfluxDB ab:</p> <ol> <li><code>from(bucket: \"DEMOBUCKET\")</code>: Gibt das Bucket an, aus dem abgefragt wird.</li> <li><code>range(start: v.timeRangeStart, stop: v.timeRangeStop)</code>: Definiert den Zeitbereich f\u00fcr die Abfrage.</li> <li><code>filter</code>: Filtert die Daten, um nur Messungen f\u00fcr <code>pressure</code> und <code>temperature</code> einzuschliessen.</li> <li><code>map</code>: Konvertiert das <code>idsensor_component</code> Feld von String zu Integer. In InfluxDB ist der Datentyp per default String, in MySQL hingegen Integer. Damit das mapping klappt, m\u00fcssen wir den Datentyp konvertieren auf Integer.</li> <li><code>filter</code>: Filtert die Daten weiter, um die <code>idsensor_component</code> Werte 1 und 2 einzuschliessen.</li> <li><code>aggregateWindow</code>: Aggregiert Daten \u00fcber den angegebenen Zeitraum.</li> <li><code>yield</code>: Wird zum Debuggen verwendet, um sicherzustellen, dass die Daten korrekt sind.</li> </ol> <p>Zusammenf\u00fchren der Daten</p> <pre><code>combined_data = join(\n    tables: {mysql: mysql_data, influx: influx_data},\n    on: [\"idsensor_component\"]\n)\n|&gt; map(fn: (r) =&gt; ({\n    _time: r._time,\n    _measurement: r._measurement,\n    _field: r._field,\n    _value: r._value,\n    idsensor_component: r.idsensor_component,\n    locationName: r.locationName\n}))\n|&gt; yield(name: \"combined_data\")\n</code></pre> <p>Dies verbindet die MySQL-Daten (<code>mysql_data</code>) mit den InfluxDB-Daten (<code>influx_data</code>) basierend auf dem Feld <code>idsensor_component</code>:</p> <ol> <li><code>join</code>: Kombiniert die beiden Datens\u00e4tze basierend auf <code>idsensor_component</code>.</li> <li><code>map</code>: Stellt sicher, dass die resultierenden Datenfelder aus beiden Datens\u00e4tzen enthalten, speziell <code>_time</code>, <code>_measurement</code>, <code>_field</code>, <code>_value</code>, <code>idsensor_component</code> und <code>locationName</code>.</li> <li><code>yield</code>: Gibt die verbundenen Daten zur finalen \u00dcberpr\u00fcfung aus.</li> </ol> <p>Durch diese Schritte stellen wir sicher, dass das Feld <code>locationName</code> aus MySQL korrekt mit den Zeitreihendaten aus InfluxDB zusammengef\u00fchrt wird, sodass es im Data Explorer sichtbar wird.</p>"},{"location":"le12/","title":"LE12 - NoSQL-Datenbanken","text":""},{"location":"le12/#leverage-the-nosql-boom","title":"Leverage the NoSQL boom","text":"Leverage the NoSQL boom"},{"location":"le12/#database-landscape","title":"Database Landscape","text":"DB Landscape"},{"location":"le12/ApacheCassandraTutorial/","title":"Tutorial","text":"<p>Zielsetzung des Tutorials</p> <p>Erfahren Sie, wie eine horizontal skalierbare Datenbank funktioniert. Das gibt Ihnen einen Eindruck, wie Millionen von Endusern mit derselben Anwendung arbeiten k\u00f6nnen. Beispiele von Anwendungen grosser Firmen, die wir alle kennen: Netflix, Spotify, X, LinkedIn, Apple, Facebook, Instagramm, Uber, Pinterest, JPMorgan Chase, Visa, NetApp,.. </p> <p>All diese Firmen setzen f\u00fcr Ihre global verteilten Anwendungen auch Apache Cassandra ein.</p> <p>Weiter ist es eine gute Erfahrung, w\u00e4hrend der Ausbildung einmal mit NoSQL gearbeitet zu haben. Diese Datenbanken sind bei Anwendungen im Bereich von BigData omnipr\u00e4sent. </p>"},{"location":"le12/ApacheCassandraTutorial/#eigenschaften-von-cassandra","title":"Eigenschaften von Cassandra","text":"<p>Cassandra ist ein verteiltes Datenbank Management System, welches f\u00fcr die Verarbeitung grosser Datenvolumen geeignet ist. Die Architektur ist auf mehrere Nodes (Rechner) verteilt. Die Daten werden unter den Nodes repliziert. Das f\u00fchrt zu einer hohen Verf\u00fcgbarkeit ohne single point of failure, wenn &gt;= 3 Nodes eingesetzt werden.</p> <p>Im unteren Bild entsprechen die Kreise einzelner Nodes. Die Verbindungen zwischen den Kreisen symbolisieren die verteilte Architektur. Der Client sendet seine Anfragen an einen Node in einem Cassandra-Cluster. Der Node, welcher die Anfrage annimmt, wird als Coodinator bezeichnet. Jeder Node innerhalb eines Cluster kann diese Rolle \u00fcbernehmen.</p>"},{"location":"le12/ApacheCassandraTutorial/#cassandra-als-teil-der-applikationsentwicklung","title":"Cassandra als Teil der Applikationsentwicklung","text":"<p>Im Relationalen Modell wird zuerst die Datenstruktur analysiert und dann am Ende die Applikation darauf angewendet. Die Applikation ruft Queries auf, um die ben\u00f6tigten Informationen aus dem Datenmodell zu holen.</p> <p>Bei Apache Cassandra ist der Prozess umgekehrt. Hier wird zuerst die Anwendung entwickelt. Es wird entschieden, welche Queries in der Anwendung ausgef\u00fchrt werden m\u00fcssen. Dies f\u00fchrt zur \u00dcberlegung, wie das logische Datenmodell aussehen muss. Nachdem die Queries und das Datenmodell definiert sind, werden die Daten in Cassandra geladen.</p> <p>Dieses Vorgehen, hier vereinfacht beschrieben,  ist nicht nur f\u00fcr Cassandra typisch, sondern f\u00fcr alle NoSQL-Datenbanken.</p>"},{"location":"le12/ApacheCassandraTutorial/#cassandra-und-das-cap-theorem","title":"Cassandra und das CAP-Theorem","text":"<p>Im Zusammenhang mit dem CAP-Theorem pr\u00e4feriert Cassandra Availability und Partition Tolearance und \"opfert\" die Konsistenz. Man spricht daher auch von einem AP-System.  Die Consistency  kann jedoch mit der Anzahl eingesetzter Nodes in einem Cassandra-Setup \"getuned\" werden. Details dazu folgen. </p>"},{"location":"le12/ApacheCassandraTutorial/#verteilte-skalierbare-nosql-datenbank","title":"Verteilte, skalierbare NoSQL-Datenbank","text":"<p>Cassandra ist somit eine horizontal skalierbare, opensource NoSQL Datenbank. Die Datenbank besteht aus mehreren Nodes, die logisch in einem Ring zusammengefasst sind. Die Nodes replizieren sich, so dass die DB stets konsistent ist. </p> <p>Die Skalierbarkeit kann damit erreicht werden, indem die Anzahl Nodes angepasst werden kann. In der Dokumentation wird folgendes gesagt:</p> <p>\u201eTo double your capacity or double your throughput, double the number of nodes.\u201c</p> <p>Die Message ist somit einfach: Je mehr Nodes, desto mehr Durchsatz.</p> <p>In diesem Zusammenhang bildet das Konzept der Partitions eine entscheidende Rolle.</p>"},{"location":"le12/ApacheCassandraTutorial/#partitions","title":"Partitions","text":"<p>Daten k\u00f6nnen auf mehrere Nodes repliziert werden. Dies erh\u00f6ht die Verf\u00fcgbarkeit und Fehlertoleranz. In Cassandra exisitert der Begriff des Replication Factors RF. Dieser besagt, auf wie vielen Nodes die Daten repliziert werden sollen. Wenn unser Cassandra System aus einem Node besteht, ist der RF=1. Mit RF=2 werden die Daten auf einem weiteren Node repliziert, usw.</p> <p>Der Betrieb von Cassandra macht Sinn, wenn das System aus mehrere Nodes besteht. Alle Nodes zusammen bilden einen Cluster oder \"Ring\". Nodes sind untereinander gleichwertig und jeder Node hat dieselben Funktionalit\u00e4ten wie alle andern. Man spricht hier auch von einer masterless architecture. </p> <p>Jeder Node besitzt einen Satz von sogenannten Tokens. Diese Tokens spielen eine Rolle, wenn es darum geht, auf welchem Node innerhalb des Clusters die Daten abgespeichert werden sollen.  Dazu wird jeder verwaltbaren Datenzeile ein Partition Key zugeordnet. Dieser Key ist entscheidend f\u00fcr die Nodes, welche die Datenzeile abspeichern sollen. Wenn Daten in einem Cassandra Cluster eingef\u00fcgt werden, wird als erster Schritt dieser Partition Key erstellt. Dieser Key bestimmt dann den Node, welcher f\u00fcr die Daten \"verantwortlich\" ist. Die Erstellung des Keys erfolgt durch das Generieren eines Hash-Wertes, welcher auf die Wertebereiche der Tokens abgestimmt ist, f\u00fcr welche die Nodes verantwortlich sind. F\u00fcr die Zuweisung des Nodes ist der Database Coodinator verantwortlich. Jeder Node in einem Cluster kann diese Rolle als Coodinator \u00fcbernehmen. Alle Nodes kommunizieren mit dem sogenannten gossip-Protokoll. Dieses Protokoll legt den aktuellen Coodinator fest. Das gossip-Protokoll legt auch fest, welcher Node f\u00fcr welche Token-Ranges verantwortlich ist.  Wenn nun Daten im Cluster eingef\u00fcgt werden, beispielsweise eine neue Zeile in einer Tabelle, bestimmt der aktuelle Coodinator die Partition Nummer, beispielsweise Nr. 59. Der Coordinator schaut nun, welcher Node f\u00fcr Token Nr 59 verantwortlich ist und leitet die Daten an diesen weiter. Dieser Node wird auch Replica Node genannt. Wenn RF &gt; 1 ist, werden diese Daten an mehrere Nodes weitergeleitet. Bei RF=1 erh\u00e4lt nur ein Node die Daten.</p> <p>Beachte:  Der Coodinator Node ist nicht auf einen Node beschr\u00e4nkt. Damit h\u00e4tten wir einen single point of failure. Jeder Node kann diese Rolle \u00fcbernehmen! Wer die Rolle zu einem bestimmten Zeitpunkt inne hat, wird durch das gossip-Protokoll bestimmt.</p> <p>Die Replikation in einem Cassandra-Datenbank-System garantiert Reliability und Fehlertoleranz.</p> <p>Cassandra kennt weiter den Begriff des Datacenter. Cassandra repliziert die Daten zwischen den verschiedenen Datacenters. Dies hat vorallem Performance-Vorteile, wenn die Daten \u00fcber mehrer Kontinente verteilt werden m\u00fcssen. Datacenter k\u00f6nnen bei der Konfiguration von Cassandra eingerichtet werden. Im Standard besteht das Cassandra-System aus einem Datacenter. Es ist so auch nicht erstaunlich, dass die Erfinder von Cassandra ehemalige Facebook-Mitarbeiter waren. </p> <p>Cassandra gilt als AP-System (Available Partition-tolerant) im Sinne des CAP-Theorems, welches Sie bereits kennen. Es \"opfert\" also das dritte Element in diesem Konzept, die Consistency. Diese Consistency kann jedoch parametriert werden, indem der Consistency Level (CL) in Cassandra eingestellt werden kann. Der CL besagt, wie viele Nodes im Cluster minimal ein Read/Write acknowledge dem Coordinator abgeben m\u00fcssen, bevor die Operation als erfolgreich behandelt wird. Dieser CL basiert auf dem Replication Factor RF. Wenn RF=3 ist, m\u00fcssen die Mehrheit der Nodes f\u00fcr eine Operation dem Coordinator ein acknowledge senden. In diesem Fall m\u00fcssen das mindestens 2 Nodes machen. Wenn das nicht der Fall ist, wird die Operation vom Coodinator als nicht erfolgreich behandelt. Bei RF=6 ist der CL=(6/2+1)=4. Mindestens 4 Nodes m\u00fcssen in diesem Fall ein acknowledge senden. Der Consistency Level CL wird auch als Quorum bezeichnet. </p> <p>Diese Eigenschaft macht Cassandra sehr flexibel. Es spielt keine Rolle, wo sich die Nodes eines Cassandra-Clusters befinden - ob in einer Cloud oder in einem eigenen Rechencenter. Beliebige Infrastruktur-Kombinationen bilden eine einzige Datenbank, die global verf\u00fcgbar sein kann.</p>"},{"location":"le12/ApacheCassandraTutorial/#praktische-vorbereitung","title":"Praktische Vorbereitung","text":""},{"location":"le12/ApacheCassandraTutorial/#erstellen-von-keyspaces-und-cluster-replication","title":"Erstellen von Keyspaces und Cluster-Replication","text":"<p>Wir werden in diesem Tutorial queries an Cassandra senden. Dazu verwenden wir die Cassandra SQL - Shell. Diese heisst <code>cqlsh</code>. <code>cqlsh</code> wird in Docker wie folgt gestartet:</p> <p><code>docker exec -it cassandra-1 cqlsh</code></p> <pre><code>vmadmin@cassandravm:~/scripts$ docker exec -it cassandra-1 cqlsh\nConnected to my-cluster at 127.0.0.1:9042\n[cqlsh 6.2.0 | Cassandra 5.0.2 | CQL spec 3.4.7 | Native protocol v5]\nUse HELP for help.\ncqlsh&gt;\n</code></pre> <p>Hier Ihr erstes Query:</p> <pre><code>cqlsh&gt; DESCRIBE keyspaces;\n\nsystem       system_distributed  system_traces  system_virtual_schema\nsystem_auth  system_schema       system_views \n\ncqlsh&gt;\n</code></pre> <p>Als Antwort erhalten Sie alle existierenden keyspaces. Ein Keyspace enth\u00e4lt Tabellen und ist analog zu einem Datenbankschema in einer relationalen Datenbank. </p> <p>Bevor wir Tabellen anlegen, m\u00fcssen wir einen Keyspace erstellen. Wir erstellen diesen mit einem Replication Factor 3. Daten werden also auf allen 3 Nodes verwaltet:</p> <p><code>cqlsh&gt;</code> Eingabe:</p> <p><code>CREATE KEYSPACE M165 WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 3};</code></p> <p>Die Syntax wird hier erkl\u00e4rt.</p> <p>Damit wir ein Keyspace mit RF=3 erstellt. Die Strategie definiert, wie Daten in verschiedenen Datacenters repliziert werden soll. Vergleiche dazu das Kapitel zu den Partitions.</p> <p>Warum starten wir mit 3 Nodes?</p> <ul> <li> <p>Es wird empfohlen, stets mit mindestens 3 Nodes ein Cassandra System zu betreiben.     </p> </li> <li> <p>Wenn Consistency verlangt wird, ben\u00f6tigen wir ein Acknowlegement von mindestens 2 Nodes.    </p> </li> <li> <p>Wenn ein Node ausf\u00e4llt, ist der Cluster immer noch verf\u00fcgbar, weil noch 2  Nodes vorhanden sind.    </p> </li> <li> <p>Das Verst\u00e4ndnis dazu wird im Verlaufe dieser \u00dcbungen entstehen.</p> </li> </ul> <p>Unsere 3 Nodes sollten also hiermit Up and Running sein: <pre><code>vmadmin@cassandravm:~/scripts$ docker exec cassandra-3 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \nUN  172.17.0.4  192.62 KiB  16      64.7%             224bb3b4-3cec-45b6-89a7-0b231a237743  rack1\nUN  172.17.0.3  211.35 KiB  16      59.3%             87693efc-73ec-45c6-a8eb-094071bb8942  rack1\nUN  172.17.0.2  216.33 KiB  16      76.0%             c2e09e05-7ddb-4e90-a289-8cd0795e4d6e  rack1\n</code></pre></p> <p>Die Nodes h\u00f6ren auf den Ports <code>9042</code>, <code>9043</code> und <code>9044</code> auf Client-Requests. Den jeweils verwendeten Port sehen Sie beim Starten der cqlsh-Shell: <code>docker exec -it cassandra-1 cqlsh</code></p> <pre><code>Connected to my-cluster at 127.0.0.1:9042\n[cqlsh 6.2.0 | Cassandra 5.0.2 | CQL spec 3.4.7 | Native protocol v5]\nUse HELP for help.\ncqlsh&gt;\n</code></pre> <p>Dieses Szenario hier ist realistisch f\u00fcr einen kleinen Cluster. In einer produktiven Umgebung w\u00fcrden die Nodes auf eigenen Rechnern laufen, um die Performance zu erh\u00f6hen.</p> <p>Bevor wir beginnen Tabellen zu erstellen, ist es n\u00fctzlich einige Basics zum Tabellendesign zu verstehen.</p>"},{"location":"le12/ApacheCassandraTutorial/#eintauchen-und-hands-on-in-die-cassandra-architektur","title":"Eintauchen und Hands-On in die Cassandra Architektur","text":"<p>Cassandra ist eine dezentralisierte multi-node Datenbank, welche physisch \u00fcber mehrere Standorte verteilt ist. Sie verwendet Replikation und Partitionen und ist dadurch enorm skalierbar f\u00fcr Lese- und Schreiboperationen.</p>"},{"location":"le12/ApacheCassandraTutorial/#dezentralisation","title":"Dezentralisation","text":"<p>Cassandra ist dezentral organisiert. Jeder Node ist gleichwertig; es sind Peers! Jeder Node agiert in verschiedenen Rollen und ohne zentralen Controller.</p> <p>Die dezentrale Architektur von Cassandra erlaubt es, dass Nodes ausfallen k\u00f6nnen und auch nodes hinzugef\u00fcgt werden, ohne dass das ganze System dadurch gest\u00f6rt wird.</p>"},{"location":"le12/ApacheCassandraTutorial/#jeder-node-ist-ein-coordinator","title":"Jeder Node ist ein Coordinator","text":"<p>Daten werden auf verschiedene Nodes repliziert. Wenn ein Request anliegt, kann dieser von jedem Node verarbeitet werden.</p> <p>Der initiale Request-Empf\u00e4nger wird zu einem Coodinator Node f\u00fcr diesen Request. Wenn andere Nodes zwecks Konsistenz\u00fcberpr\u00fcfung kontaktiert werden m\u00fcssen, verlangt der Koordinator die ben\u00f6tigten Daten von den Replica Nodes.</p> <p>Der Koordinator kann mit einem sogenannten  Consistent Hashing Algorithmus den Node oder die Nodes ermitteln, welche die Daten bereitstellen k\u00f6nnen. </p> <p>Der Koordinator-Node ist f\u00fcr viele Dinge zust\u00e4ndig, z. Bsp. auch f\u00fcr das Reparieren von Daten oder f\u00fcr Read- und Write-Retries.</p>"},{"location":"le12/ApacheCassandraTutorial/#data-partitioning","title":"Data Partitioning","text":"<p>Partitioning </p> <p>Partitioning is a method of splitting and storing a single logical dataset in multiple databases. By distributing the data among multiple machines, a cluster of database systems can store larger datasets and handle additional requests.</p> <p>In Cassandra werden Daten in einem definierten Schema, dem sogenannten Keyspace verwaltet. Innerhalb dieses Keyspace werden die Tabellen angelegt.</p> <p>Zus\u00e4tzlich wird auch in Cassandra ein Primary Key f\u00fcr jede Tabelle definiert. Nur so ist eine eindeutige Identifizierung eines Datensatzes m\u00f6glich.</p> <p>Das Konzept der Primary Keys in Cassandra ist komplexer als in traditionellen relationalen Datenbanken, wie . Bsp, MySQL.</p> <p>In Cassandra beinhaltet ein Primary Key 2 Teile: * einen obligatorischen  Partition Key und * ein  optionales Set von  Clustering Columns.</p> <p>Dazu mehr im Kapitel  \"Data Modeling\".</p> <p>Im Folgenden widmen wir uns dem Partitioning.</p> <p>Betrachten Sie folgende Tabelle:</p> <pre><code>Table Users | Legend: p - Partition-Key, c - Clustering Column\n\ncountry (p) | user_email (c)  | first_name | last_name | age\n----------------------------------------------------------------\nUS          | john@email.com  | John       | Wick      | 55  \nUK          | peter@email.com | Peter      | Clark     | 65  \nUK          | bob@email.com   | Bob        | Sandler   | 23 \nUK          | alice@email.com | Alice      | Brown     | 26\n</code></pre> <p>Die Spalten  <code>user_email</code> und <code>country</code> bilden zusammen den Primary Key.</p> <p>Die  <code>country</code> - Spalte ist der Partition Key (p). Das <code>CREATE</code>-statement wollen wir nun erstellen:</p> <p>Starten der Cassandra-SQL-Shell <code>cqlsh</code> und wechseln in den erstellten <code>m165</code>-Keyspace mit <code>use</code>:</p> <pre><code>vmadmin@cassandravm:~/scripts$ docker exec -it cassandra-1 cqlsh\nConnected to Test Cluster at 127.0.0.1:9042\n[cqlsh 6.1.0 | Cassandra 4.1.1 | CQL spec 3.4.6 | Native protocol v5]\nUse HELP for help.\n\ncqlsh&gt; DESCRIBE KEYSPACES;\n\nm165    system_auth         system_schema  system_views         \nsystem  system_distributed  system_traces  system_virtual_schema\n\ncqlsh&gt; use M165;\n</code></pre> <p>Beachten Sie den <code>use</code>-Befehl. Damit wechseln wir in den erstellten Keyspace und m\u00fcssen diesen dann nicht weiter angeben. Das Erstellen einer Tabelle <code>users_by_country</code> wird dann so gemacht:</p> <pre><code>cqlsh&gt;\nCREATE TABLE users_by_country (\n    country text,\n    user_email text,\n    first_name text,\n    last_name text,\n    age smallint,\n    PRIMARY KEY ((country), user_email)\n);\n</code></pre> <p>Pasten Sie den obigen CREATE-Befehl einfach in den <code>cqlsh</code>-Prompt. Es werden jeweils Punkte am Anfang der Zeile erscheinen, bis der Befehl mit <code>;</code> abgeschlossen ist. Das sieht dann so aus:</p> <p>Die erste Gruppe im <code>PRIMARY KEY</code> - Statement definiert den Partition Key. Alle weiteren Elemente bezeichnen die Clustering Spalten:</p> <p>Laden wir einige Daten in die Tabelle:</p> <pre><code>cqlsh&gt; \nINSERT INTO users_by_country (country,user_email,first_name,last_name,age)\n  VALUES('US', 'john@email.com', 'John','Wick',55);\n\nINSERT INTO users_by_country (country,user_email,first_name,last_name,age)\n  VALUES('UK', 'peter@email.com', 'Peter','Clark',65);\n\nINSERT INTO users_by_country (country,user_email,first_name,last_name,age)\n  VALUES('UK', 'bob@email.com', 'Bob','Sandler',23);\n\nINSERT INTO users_by_country (country,user_email,first_name,last_name,age)\n  VALUES('UK', 'alice@email.com', 'Alice','Brown',26);\n</code></pre> <p>Pasten Sie wiederum die INSERT-Statements in den <code>cqlsh</code>-Prompt und schliessen Sie die Eingabe mit <code>Enter</code> ab.</p> <p>Vielleicht stellen Sie sich hier die Frage, warum <code>country</code> als essentiellen Teil des Primary Keys verwendet wird? In einer relationalen DB h\u00e4tten Sie das wohl nie gemacht.</p> <p>Dies wird klar, nachdem Sie hier die Basics von Cassandra  durchgearbeitet haben.</p> <p>Das Konzept des Partitioning ist die Grundlage f\u00fcr die enorme Skalierungsf\u00e4higkeit von Cassandra. In diesem Beispiel basieren die Partitionen auf dem <code>country</code>. Alle Zeilen (Rows) mit <code>country</code>=US werden in ein und derselben Partition gehalten. Alle andern Zeilen mit <code>country</code>=UK werden in einer andern Partition verwaltet.</p> <p>Neben dem Begriff Partitioning wird auch der Begriff Shard verwendet. Die Bedeutung ist dieselbe.</p> <p>Basierend auf dem Wert des Partition Keys werden Partitionen mit entsprechenden Daten gef\u00fcllt. Diese Partition Keys sind also zust\u00e4ndig, welche Nodes welche Partitionen verwalten. Das Verteilen der Daten auf mehrere Nodes ist die Basis der Skalierbarkeit von Cassandra.</p> <p>Der User / die Anwendung liest oder schreibt die Daten, indem dazu mehrere Nodes involviert sind. Der Partition Key ist immer daf\u00fcr verantwortlich, dass die richtigen Nodes angesprochen werden.</p> <p>Das Verst\u00e4ndnis der Datenverteilung, basierend auf Partitionen, ist sehr wichtig beim Design einer Applikation.</p> <p>Hier unterscheidet sich Cassandra sehr stark gegen\u00fcber den klassischen relationalen Datenbanken.</p> <p>Was bedeutet horizontale Skalierung??</p> <p>Horizontal scaling means you can increase throughput by adding more nodes. If your data is distributed to more servers, then more CPU, memory, and network capacity is available.</p> <p>Warum braucht man dann <code>eMail</code> als Primary Key?</p> <p>Der PK definiert die Spalte, um Records eindeutig zu identifizieren. Nur <code>country</code> w\u00fcrde eine Zeile nicht eindeutig identifizieren.</p> <p>Der Partition Key ist wichtig, um Daten gleichm\u00e4ssig zwischen Nodes zu verteilen und wichtig, um die Daten effizient lesen zu k\u00f6nnen. Das definierte Schema wurde so festgelegt, dass die Daten meist pro Land abgefragt werden sollen, deshalb wurde <code>country</code> als Partition Key definiert: <code>PRIMARY KEY ((country), user_email)</code></p> <p>Eine Abfrage \u00fcber alle Zeilen mit Selektion des Landes performed damit sehr gut: </p> <p><code>cqlsh&gt; SELECT * FROM users_by_country WHERE country='US';</code></p> <p>Ihre <code>cqlsh</code>-shell sendet den Request nur an einen Casandra-Node - by default. Dies bezeichnet man als Consitency Level of One.</p> <p>Was bedeutet eine  Consitency Level of One?</p> <p>A consistency level of one means that only a single node is asked to return the data. With this approach, you will lose strong consistency guarantees and instead experience eventual consistency. Eventual consistency erkl\u00e4ren wir sp\u00e4ter.</p> <p>Kreieren wir eine weitere Tabelle. Hier ist der Partition Key die E-Mail-Adresse:</p> <pre><code>cqlsh&gt; \nCREATE TABLE users_by_email (\n    user_email text,\n    country text,\n    first_name text,\n    last_name text,\n    age smallint,\n    PRIMARY KEY (user_email)\n);\n</code></pre> <p>... und noch einige Daten einf\u00fcgen:</p> <pre><code>cqlsh&gt; \nINSERT INTO users_by_email (user_email, country,first_name,last_name,age)\n  VALUES('john@email.com', 'US', 'John','Wick',55);\n\nINSERT INTO users_by_email (user_email,country,first_name,last_name,age)\n  VALUES('peter@email.com', 'UK', 'Peter','Clark',65); \n\nINSERT INTO users_by_email (user_email,country,first_name,last_name,age)\n  VALUES('bob@email.com', 'UK', 'Bob','Sandler',23);\n\nINSERT INTO users_by_email (user_email,country,first_name,last_name,age)\n  VALUES('alice@email.com', 'UK', 'Alice','Brown',26);\n</code></pre> <p>Hier erh\u00e4lt jeder Record eine eigene Partition, weil jede E-Mail-Adresse einmalig ist. Dies wurde beim Kreieren der Tabelle so festgelegt: <pre><code>...\n...\nPRIMARY KEY (user_email));\n</code></pre></p> <p>Wenn es darum geht, dass Records sehr schnell aufgrund der E-Mail-Adresse abgefragt werden sollen, ist das eine gute Idee, den Partition Key so zu definieren! <code>SELECT * FROM users_by_email WHERE user_email='alice@email.com';</code></p> <p>Wenn die Daten mit dem Partition Key   <code>user_email</code>  eingef\u00fcgt wurden und eine Abfrage nach <code>age</code> erfolgt, m\u00fcssten die Daten \u00fcber alle Partitionen \"gesucht\" werden!</p> <p>Dies h\u00e4tte Performance-Einbussen zur Folge. Insbesondere dann, wenn der Cluster aus sehr vielen Nodes besteht.</p> <p>Cassandra versucht dies zu verhindern. Wenn eine Abfrage mit einem Spaltennamen erfolgt, welcher nicht ein Partition Key ist, muss Cassandra dies explizit mitgeteilt werden, indem das Keyword <code>ALLOW FILTERING</code> verwendet wird. Das geht so: <pre><code>cqlsh&gt; \nSELECT * FROM users_by_email WHERE age=26 ALLOW FILTERING;\n</code></pre></p> <p>Ohne  <code>ALLOW FILTERING</code> w\u00fcrde der Query nicht ausgef\u00fchrt werden, bzw. eine Fehlermeldung wird dabei generiert. Dies soll verhindern, dass \"expensive Queries\", also Queries, welche eine hohe Last auf den Nodes erzeugen, abgesetzt werden.</p> <p>Fehlermeldung, wenn <code>ALLOW FILTERING</code> nicht verwendet wird:</p> <p><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=\"Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING\"</code></p>"},{"location":"le12/ApacheCassandraTutorial/#bedeutung-des-partition-keys-zusammenfassung","title":"Bedeutung des Partition Keys - Zusammenfassung","text":"<p>Zusammenfassung - Bedeutung des Partition Keys</p> <p>Der Partition Key spielt eine zentrale Rolle bei der Organisation und Verwaltung von Daten in Apache Cassandra. Hier sind einige wichtige Aspekte und Bedeutungen des Partition Keys:</p> <ol> <li>Datenverteilung: Der Partition Key bestimmt, auf welchem Knoten im Cassandra-Cluster die Daten gespeichert werden. Dies geschieht durch einen Hashing-Mechanismus, der sicherstellt, dass Daten gleichm\u00e4ssig \u00fcber alle Knoten verteilt werden. Dadurch wird eine optimale Lastverteilung und Skalierbarkeit erreicht.</li> <li>Performance und Abfrageoptimierung: Da der Partition Key verwendet wird, um Daten auf spezifischen Knoten zu speichern, k\u00f6nnen Abfragen, die den Partition Key verwenden, sehr effizient und schnell ausgef\u00fchrt werden. Abfragen, die den Partition Key nicht enthalten, k\u00f6nnen jedoch langsamer sein, da sie alle Knoten im Cluster durchsuchen m\u00fcssen.</li> <li>Datenlokalisierung: Daten, die denselben Partition Key teilen, werden auf demselben Knoten gespeichert. Dies erleichtert das Abrufen und die Verarbeitung zusammenh\u00e4ngender Daten. Zum Beispiel k\u00f6nnen alle Informationen zu einem spezifischen Benutzer unter einem einzigen Partition Key gespeichert werden, was schnelle und effiziente Abfragen erm\u00f6glicht.</li> <li>Replikation und Fehlertoleranz: Der Partition Key spielt auch eine Rolle bei der Replikation von Daten. Cassandra verwendet den Partition Key, um zu bestimmen, auf welchen Knoten Daten repliziert werden, basierend auf der Konfiguration des Replikationsfaktors. Dies erh\u00f6ht die Fehlertoleranz und die Verf\u00fcgbarkeit der Daten.</li> <li>Datenmodellierung: Bei der Gestaltung des Datenmodells in Cassandra ist die Wahl des Partition Keys von entscheidender Bedeutung. Ein gut gew\u00e4hlter Partition Key kann die Abfrageleistung erheblich verbessern und die Effizienz der Datenverteilung sicherstellen.</li> </ol> <p>Zusammengefasst ist der Partition Key ein kritischer Bestandteil von Cassandra, der die Datenverteilung, Abfrageleistung, Fehlertoleranz und die gesamte Effizienz des Systems beeinflusst.</p>"},{"location":"le12/ApacheCassandraTutorial/#replication","title":"Replication","text":"<p>Skalierbarkeit mit nur Partitionierung ist limitiert! Dieses Element wird erg\u00e4nzt mit dem Konzept der Replikation!</p> <p>Betrachten Sie write requests, welche nur eine Partition betreffen. Alle requests w\u00fcrden an einen einzigen Node gesendet werden, welcher f\u00fcr diese Partition zust\u00e4ndig ist. Seine CPU, das Memory, der I/O-Durchsatz w\u00e4ren sehr belastet. Zus\u00e4tzlich m\u00f6chten Sie auch, dass der Request bearbeitet wird, wenn der Node nicht verf\u00fcgbar ist.</p> <p>Hier kommt das Konzept der Replikation zum Zug. Durch das Duplizieren der Daten auf mehrere Nodes, sog. Replicas, kann der Durchsatz erh\u00f6ht werden, indem Nodes simultan Daten bereitstellen k\u00f6nnen. Auch erh\u00f6ht dieses Konzept die Verf\u00fcgbarkeit, weil ein Ausfall eines Nodes von andern Nodes abgefangen werden kann.</p> <p>In Cassandra kann deshalb f\u00fcr jeden Keyspace ein Replication Factor (RF) definiert werden. Unseren Keyspace <code>M165</code> haben wir mit eine RF von 3 festgelegt.</p> <pre><code>cqlsh&gt; CREATE KEYSPACE M165\n  WITH REPLICATION = { \n   'class' : 'NetworkTopologyStrategy',\n   'datacenter1' : 3 \n  };\n</code></pre> <p>RF=1: Nur eine Kopie eines Records wird im Cluster gehalten. Wenn der Node mit dieser Kopie ausf\u00e4llt, dass der Record nicht abgefragt werden.</p> <p>RF=2: Ein Record wird auf 2 verschiedenen Nodes gehalten. Jeder Node ist gleichgestellt. Es gibt keinen \"Master\"-Replica-Node. Ein Node kann auch ausfallen und die Abfrage wird trotzdem erfolgreich sein.</p> <p>Als generelle Regel sollte der Replication Factor RF nie gr\u00f6sser als die Anzahl Nodes in einem Cluster sein. Man kann aber den Replication Factor zu Beginn h\u00f6her einstellen und dann sp\u00e4ter die gew\u00fcnschte Anzahl Nodes hinzuf\u00fcgen.</p> <p>In produktiven Umgebungen ist ein RF von 3 empfohlen. Dies gew\u00e4hrt Sicherheit, dass es sehr unwahrscheinlich wird, dass Daten verloren gehen oder kein Zugriff darauf erfolgen kann.</p> <p>Auch im Fall von inkonsistenten Daten (die Replicas haben unterschiedliche Daten), k\u00f6nnen die einzelnen Nodes nach dem Daten-Status abgefragt werden. Es kann dann entschieden werden, ob der Status der Mehrheit der Nodes der g\u00fcltige sein soll.</p> <p>In unserem Cassandra Setup besteht der Cluster aus 3 Nodes mit 3 Replicas. Die Mehrheit der Nodes ist somit 2. Dies erlaubt uns, einige \u00dcberlegungen zum Konsistenz-Level im n\u00e4chsten Kapitel anzustellen.</p>"},{"location":"le12/ApacheCassandraTutorial/#consistency-level","title":"Consistency Level","text":"<p>Nach dem Partitioning und der Replikation nehmen wir uns dem Thema der Datenkonsistenz an. Cassandra hat in diesem Bereich viel zu bieten. Der Begriff hier ist: Tunable Consistency.</p> <p>Das Konsistenz-Level f\u00fcr Read/Write-Queries kann eingestellt werden. Settings dazu sind hier ausf\u00fchrlich dokumentiert.</p> <p>Schauen wir uns das bekannteste Setting zuerst an. Nehmen wir an, wir haben 3 Replicas definiert. </p> <p>Erste Frage:  Haben wir einen Bedarf an hoher Datenkonsistenz?</p> <p>Was bedeutet Strong Consistency?</p> <p>In contrast to eventual consistency, strong consistency means only one state of your data can be observed at any time in any location.</p> <p>For example, when consistency is critical, like in a banking domain, you want to be sure that everything is correct. You would rather accept a decrease in availability and increase of latency to ensure correctness.</p> <p>Hier spielt das  CAP Theorem eine wichtige Rolle. Es ist unm\u00f6glich in einem bestimmten Zeitpunkt gleichzeitig Verf\u00fcgbarkeit UND Konsistenz zu garantieren, wenn Verbindungs- oder andere technische Probleme in einem Cluster auftauchen.</p> <p>Beispiel: Sie schreiben einen einzelnen Wert in eine Tabelle. Die Daten werden auf 2 Nodes repliziert. Jetzt entsteht zwischen den Nodes ein Unterbruch. Zuerst wird der Write-Request an Node 1 gesendet. Dann werden Daten von Node 2 gelesen.</p> <p>Wie verhalten wir uns in dieser Situation?</p> <ol> <li>Soll das Schreiben verhindert werden, weil Node 2 nicht erreichbar ist? Dies w\u00fcrde bedeuten, dass wir die Verf\u00fcgbarkeit opfern zugunsten der Datenkonsistenz.</li> <li>Akzeptiren wir den Schreibzugriff auf Node 1 und lassen wir Lesezugriffe von Node 1 und 2 zu? Das System w\u00e4re mit diesem Entscheid weiterhin verf\u00fcgbar. Wir riskieren jedoch, dass Daten nicht auf jedem Replication Node identisch ist, weil ja hier in unserem Beispiel Node 1 mit Node 2 nicht replizieren kann. In diesem Fall opfern wir Datenkonsistenz zugunsten der Verf\u00fcgbarkeit.</li> </ol> <p>Die entscheidende Frage ist: Wollen wir Verf\u00fcgbarkeit oder Datenkonsistenz?</p> <p>Ein anderer Faktor ist folgender:  Das Garantieren von Konsistenz bedingt, dass wir mehrere Nodes anfragen m\u00fcssen. Dies versursacht eine \"Wartezeit\", um alle Antworten der Nodes auswerten zu k\u00f6nnen. Wollen wir diese Wartezeit (Latency) zugunsten der Konsistenz akzeptieren?</p>"},{"location":"le12/ApacheCassandraTutorial/#tune-for-consistency-by-setting-up-a-strong-consistency-application","title":"Tune for Consistency by Setting up a Strong Consistency Application","text":"<p>Es gibt eine wichtige Formel, welche eine hohe Dastenkonsistenz garantiert: </p> <p><code>[read-consistency-level] + [write-consistency-level] &gt; [replication-factor]</code></p> <p>Was heisst Konsistenz Level?</p> <p>Consistency level means how many nodes need to acknowledge a read or a write query.</p> <p>Das Read- und Write-Konsistez-Level k\u00f6nnen damit festgelegt werden. Das Konsistenzlevel kann aber auch zugunsten der Perfomance reduziert werden:</p> <p>F\u00fcr read-intensive Anwendungen, wird empfohlen die Read-Konsistenz tief zu halten, weil Read-IOs h\u00e4ufiger sind als Write-IOs. Nehmen wir an, wir haben einen Replication Factor von 3.  Die Formel w\u00fcrde dann lauten: <code>1 + [write-consistency-level] &gt; 3</code></p> <p>Die write-Konsistenz w\u00e4re in diesem Fall 3, um ein stark konsistentes System zu betreiben.</p> <p>F\u00fcr  write-intensive Anwendungen w\u00e4re gem\u00e4ss Formel das write-consistence-level = 1 und somit das read-consistence-level = 3.</p> <p>Entweder wird bei einem Read jeder Node \u00fcberpr\u00fcft, ob jeder Node den gleichen Datenstatus hat - oder - bei einem Write wird \u00fcberpr\u00fcft, dass alle Nodes die Daten in ihrem lokalen Storage geschrieben haben. Diese Checks garantieren, dass Daten zum Lesen und Schreiben korrekt sind.</p> <p>In unserem Beispiel haben wir einen RF von 3. Wir k\u00f6nnen so den Konsistenz Level maximal definieren mit  <code>ALL</code> oder <code>THREE</code>:</p> <pre><code>cqlsh&gt; \n   CONSISTENCY ALL;\n   SELECT * FROM users_by_country WHERE country='US';\n</code></pre>"},{"location":"le12/ApacheCassandraTutorial/#tune-for-performance-by-using-eventual-consistency","title":"Tune for Performance by Using Eventual Consistency","text":"<p>Um Performance zu gewinnen, kann der Konsistenz-Level f\u00fcr Queries reduziert werden, z. Bsp auf 1: <pre><code>cqlsh&gt; \n   CONSISTENCY ONE;\n   SELECT * FROM users_by_country WHERE country='US';\n</code></pre></p> <p>Damit gewinnt man Performance, steht aber im Risiko mit der Datenkonsistenz, welche durch viele Faktoren beieintr\u00e4chtigt werden kann.</p> <p>Cassandra hat in diesem Bereich viele Einstellungsm\u00f6glichkeiten, wie Inkonsistenzen behandelt werden sollen.</p>"},{"location":"le12/ApacheCassandraTutorial/#optimize-data-storage-for-reading-or-writing","title":"Optimize Data Storage for Reading or Writing","text":"<p>Writes are cheaper than reads in Cassandra due to its storage engine. Writing data means simply appending something to a so-called commit-log.</p> <p>Commit-logs are append-only logs of all mutations local to a Cassandra node and reduce the required I/O to a minimum.</p> <p>Reading is more expensive, because it might require checking different disk locations until all the query data is eventually found. </p> <p>But this does not mean Cassandra is terrible at reading. Instead, Cassandra's storage engine can be tuned for reading performance or writing performance.</p>"},{"location":"le12/ApacheCassandraTutorial/#understanding-compaction","title":"Understanding Compaction","text":"<p>For every write operation, data is written to disk to provide durability. This means that if something goes wrong, like a power outage, data is not lost.</p> <p>The foundation for storing data are the so-called  SSTables . SSTables are immutable data files Cassandra uses to persist data on disk.</p> <p>You can set various strategies for a table that define how data should be merged and compacted. These strategies affect read and write performance:</p> <ul> <li><code>SizeTieredCompactionStrategy</code> is the default, and is especially performant if you have more writes than reads,</li> <li><code>LeveledCompactionStrategy</code> optimizes for reads over writes. This optimization can be costly and needs to be tried out in production carefully</li> <li><code>TimeWindowCompactionStrategy</code> is for Time-series data</li> </ul> <p>By default, tables use the <code>SizeTieredCompactionStrategy</code>:</p> <pre><code>cqlsh&gt; \n   DESCRIBE TABLE users_by_country;\n</code></pre> <p>Although you can alter the compaction strategy of an existing table, I would not suggest doing so, because all Cassandra nodes start this migration simultaneously. This will lead to significant performance issues in a production system.</p> <p>Instead, define the compaction strategy explicitly during table creation of your new table:</p> <pre><code>cqlsh&gt; \nCREATE TABLE users_by_country_with_leveled_compaction (\n    country text,\n    user_email text,\n    first_name text,\n    last_name text,\n    age smallint,\n    PRIMARY KEY ((country), user_email)\n) WITH\n  compaction = { 'class' :  'LeveledCompactionStrategy'  };\n</code></pre> <p>Let\u2019s check the result:</p> <p><code>DESCRIBE TABLE users_by_country_with_leveled_compaction;</code></p> <p>The strategies define when and how compaction is executed. Compaction means rearranging data on disk to remove old data and keep performance as good as possible when more data needs to be stored.</p> <p>Check out the excellent DataStax documentation about compaction   for details. There may even be better strategies in the future for the performance of your use-case.</p>"},{"location":"le12/ApacheCassandraTutorial/#presorting-data-on-cassandra-nodes","title":"Presorting Data on Cassandra Nodes","text":"<p>A table always requires a primary key. A primary key consists of 2 parts:</p> <ul> <li>At least 1 column(s) as partition key and</li> <li>Zero or more clustering columns for nesting rows of the data.</li> </ul> <p>All columns of the partition key together are used to identify partitions. All primary key columns, meaning partition key and clustering columns, identify a specific row within a partition.</p> <p>In Cassandra, data is already sorted on disk. So if you want to avoid sorting data later, you can make sure sorting is applied as needed. This can be ensured on the table level and avoids having to sort data in the client applications that query Cassandra.</p> <p>In our <code>users_by_country</code> table, you can define <code>age</code> as another clustering column to sort stored data:</p> <pre><code>cqlsh&gt; \nCREATE TABLE users_by_country_sorted_by_age_asc (\n    country text,\n    user_email text,\n    first_name text,\n    last_name text,\n    age smallint,\n    PRIMARY KEY ((country), age, user_email)\n) WITH CLUSTERING ORDER BY (age ASC);\n</code></pre> <p>Let\u2019s add the same data again:</p> <pre><code>cqlsh&gt; \nINSERT INTO users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)\n  VALUES('US','john@email.com', 'John','Wick',10);\n\nINSERT INTO users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)\n  VALUES('UK', 'peter@email.com', 'Peter','Clark',30);\n\nINSERT INTO users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)\n  VALUES('UK', 'bob@email.com', 'Bob','Sandler',20);\n\nINSERT INTO users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)\n  VALUES('UK', 'alice@email.com', 'Alice','Brown',40);\n</code></pre> <p>And get the data by country:</p> <pre><code>cqlsh&gt; \n      SELECT * FROM users_by_country_sorted_by_age_asc WHERE country='UK';\n</code></pre> <p>In this example, the clustering columns are <code>age</code> and <code>user_email</code>. So the data is first sorted by <code>age</code> and then by <code>user_email</code>. At its core, Cassandra is still like a key-value store. Therefore, you can only query the table by:</p> <ul> <li><code>country</code></li> <li><code>country</code> and <code>age</code></li> <li><code>country</code>, <code>age</code>, and <code>user_email</code></li> </ul> <p>But never by <code>country</code> and <code>user_email</code>.</p> <p>After learning about partitioning, replication and consistency levels, let's head into data modeling and have more fun with the Cassandra cluster.</p>"},{"location":"le12/ApacheCassandraTutorial/#data-modeling-fur-das-erstellen-einer-to-do-liste","title":"Data Modeling f\u00fcr das Erstellen einer To-Do-Liste","text":"<p>You've already learned a lot about the fundamentals of Cassandra.</p> <p>Let's put your knowledge into practice and design a to-do list application that receives many more reads than writes.</p> <p>The best approach is to analyze some user stories you want to fulfill with your table design:</p> <ol> <li>As a user, I want to create a to-do element </li> </ol> <p>Note:  This is only about creating data. For now, you can delay some decisions because you want to focus on how data is read.</p> <ol> <li>As a user, I want to list all my to-do elements in ascending order</li> </ol> <p>First, you need to query by <code>user_email</code>. Create a table called <code>todos_by_user_email</code>.</p> <p>You need 1 table that contains all the information of a to-do element of a user. Data should be partitioned by <code>user_email</code> for efficient read and writes by <code>user_email</code>.</p> <p>Also, the oldest records should be displayed first, which means using the creation date as a clustering column. The <code>creation_date</code> also ensures uniqueness.:</p> <pre><code>cqlsh&gt; \nCREATE TABLE todo_by_user_email (\n    user_email text,\n    name text,\n    creation_date timestamp,\n    PRIMARY KEY ((user_email), creation_date)\n) WITH CLUSTERING ORDER BY (creation_date DESC)\nAND compaction = { 'class' :  'LeveledCompactionStrategy'  };\n</code></pre> <ol> <li>As a user, I want to share a to-do element with another user</li> </ol> <p>To get all the to-dos shared with a user, you need to create a table called <code>todos_shared_by_target_user_email</code> to display all shared to-dos for the target user. </p> <p>The table contains the to-do name to display it.</p> <p>But the user also wants to see the to-dos they shared with other users. This is another table, <code>todos_shared_by_source_user_email</code>.</p> <p>Both tables have, according to the use-case, the required <code>user_email</code> as partition keys to allow efficient queries. Also, <code>creation_date</code> is added as a clustering column for sorting and uniqueness:</p> <pre><code>cqlsh&gt; \nCREATE TABLE todos_shared_by_target_user_email (\n    target_user_email text,\n    source_user_email text,\n    creation_date timestamp,\n    name text,\n    PRIMARY KEY ((target_user_email), creation_date)\n) WITH CLUSTERING ORDER BY (creation_date DESC)\nAND compaction = { 'class' :  'LeveledCompactionStrategy'  };\n\nCREATE TABLE todos_shared_by_source_user_email (\n    target_user_email text,\n    source_user_email text,\n    creation_date timestamp,\n    name text,\n    PRIMARY KEY ((source_user_email), creation_date)\n) WITH CLUSTERING ORDER BY (creation_date DESC)\nAND compaction = { 'class' :  'LeveledCompactionStrategy'  };\n</code></pre> <p>This type of modeling is different than thinking about foreign keys and primary keys that you might know from traditional databases. In the beginning, it's all about defining tables and thinking about what values you want to filter and need to display.</p> <p>You need to set a partition key to ensure the data is organised for efficient read and write operations. Also, you need to set clustering columns to ensure uniqueness, sort order, and optional query parameters.</p>"},{"location":"le12/ApacheCassandraTutorial/#keep-data-in-sync-using-batch-statements","title":"Keep Data in Sync Using <code>BATCH</code> Statements","text":"<p>Due to the duplication, you need to take care to keep data consistent. In Cassandra, you can do that by using <code>BATCH</code> statements that give you an all-at-once guarantee, also called atomicity.</p> <p>This might sound like a lot of work, and yes, it is a lot of work! If you have a table schema with many relationships, you will have more work compared to a normalized table schema.</p> <p>What is a normalized table schema?</p> <p>A normalized table schema is optimized to contain no duplications. Instead, data is referenced by ID and needs to be joined later. In Cassandra, you try to avoid normalized tables. It is not even possible to write a query that contains a join.</p> <p>Batch statements are cheap on a single partition, but dangerous when you execute them on different partitions, because:</p> <ul> <li>Data mutations will not be applied at the same time to all partitions, with no isolation</li> <li>It is expensive for the coordinator node, because you have to talk to multiple nodes and prepare for a rollback if something goes wrong</li> <li>There is a batch query size limit of 50kb to avoid overloading the coordinator. This limit can be increased, but this is not recommended</li> </ul> <p>In general, batches are costly.</p> <p>There are other ways to apply changes eventually. If you need to execute them very often, consider using async queries instead with a proper retry mechanism. </p> <p>Depending on the way you access your Cassandra, the driver might already offer you retry capabilities.</p> <p>Still, this approach requires thinking about what will happen if a query is never executed. If every query really needs to be executed eventually, how can you make sure that it does not get lost if your service goes down?</p> <p>The key learning here is: </p> <ul> <li>Single partition batches are cheap and should be used</li> <li>Batches that include different partitions are expensive, and if there are a lot of reads/writes, this might be the reason why a Cassandra cluster is exhausted.</li> </ul> <p>Let\u2019s create a <code>BATCH</code> statement that contains a to-do element that is shared with a user:</p> <p>cqlsh&gt; </p> <pre><code>BEGIN BATCH\n  INSERT INTO todo_by_user_email (user_email,creation_date,name) VALUES('alice@email.com', toTimestamp(now()), 'My first todo entry')\n\n  INSERT INTO todos_shared_by_target_user_email (target_user_email, source_user_email,creation_date,name) VALUES('bob@email.com', 'alice@email.com',toTimestamp(now()), 'My first todo entry')\n\n  INSERT INTO todos_shared_by_source_user_email (target_user_email, source_user_email,creation_date,name) VALUES('alice@email.com', 'bob@email.com', toTimestamp(now()), 'My first todo entry')\n\nAPPLY BATCH;\n</code></pre> <p>Let\u2019s look into one of the tables:</p> <p><code>SELECT * FROM todos_shared_by_target_user_email WHERE target_user_email='bob@email.com';</code></p> <p>All the data exists and can be accessed in a performant way using all the defined tables.</p>"},{"location":"le12/ApacheCassandraTutorial/#tombstones","title":"Tombstones","text":"<p>Cassandra is a multi-node cluster that contains replicated data on different nodes. Therefore, a delete can not simply delete a particular record.</p> <p>For a delete operation, a new entry is added to the commit-log like for any other insert and update mutation. These deletes are called tombstones, and they flag a specific value for deletion.</p> <p>Tombstones exist only on disk and can be analyzed and traced as described in this blog post: About Deletes and Tombstones in Cassandra.</p> <p>In Cassandra, you can set a time to live on inserted data. After the time passed, the record will be automatically deleted. When you set a time to live (<code>TTL</code>), a tombstone is created with a date in the future.</p> <p>In comparison, a regular delete query is the same with the difference that the time date of the tombstone is set to the moment the delete is executed.</p> <p>Let\u2019s create a tombstone by setting a <code>TTL</code> in seconds which basically function as a delayed delete:</p> <pre><code>cqlsh&gt;     \nINSERT INTO todo_by_user_email (user_email,creation_date,name) VALUES('john@email.com', toTimestamp(now()), 'This entry should be removed soon') USING TTL 60;\n</code></pre> <p>Pr\u00fcfe mit: <pre><code>SELECT * FROM todo_by_user_email WHERE user_email='john@email.com';\n</code></pre> Nach 60 Sekunden sollte der Record nicht mehr vorhanden sein:</p> <p>Setting a TTL is one of many ways to  create and execute tombstones.</p> <p>Unfortunately, there are also others.</p> <p>For example, when you insert a null value, a tombstone is created for the given cell. And as mentioned for delete requests, different types of tombstones are stored. </p> <p>By default, after 10 days, data that is marked by a tombstone is freed with a compaction execution. This time can be configured and reduced using the <code>gc_grace_seconds</code> option in the Cassandra configuration.</p> <p>When is a compaction executed?</p> <p>When the operation is executed depends mainly on the selected strategy. In general, a compaction execution takes SSTables and creates new SSTables out of it. The most common executions are:    </p> <ul> <li>When conditions for a compaction are true, that triggers compaction execution when data is inserted</li> <li>A manually executed major compaction using the <code>nodetool</code></li> </ul> <p>Sometimes, tombstones are not deleted for the following reasons:</p> <ul> <li>Null values mark values to be deleted and are stored as tombstones. This can be avoided by either replacing null with a static value, or not setting the value at all if the value is null</li> <li>Empty lists and sets are similar to null for Cassandra and create a tombstone, so don\u2019t insert them if they\u2019re empty. Take care to avoid null pointer exceptions when storing and retrieving data in your application</li> <li>Updated lists and sets create tombstones. If you update an entity and the list or set does not change, it still creates a tombstone to empty the list and set the same values. Therefore, only update necessary fields to avoid issues. The good thing is, they are compacted due to the new values</li> </ul> <p>If you have many tombstones, you might run into another Cassandra issue that prevents a query from being executed.</p> <p>This happens when the <code>tombstone_failure_threshold</code> is reached, which is set by default to 100,000 tombstones. This means that, when a query has iterated over more than 100,000 tombstones, it will be aborted.</p> <p>The issue here is, once a query stops executing, it\u2019s not easy to tidy things up because Cassandra will stop even when you execute a delete, as it has reached the tombstone limit.</p> <p>Usually you would never have that many tombstones. But mistakes happen, and you should take care to avoid this case.</p>"},{"location":"le12/ApacheCassandraTutorial/#updates-sind-inserts-und-inserts-sind-updates","title":"UPDATEs sind INSERTs und INSERTs sind UPDATEs !","text":"<p>In Cassandra, everything is append-only. There is no difference between an update and insert.</p> <p>You already learned that a primary key defines the uniqueness of a row. If there is no entry yet, a new row will appear, and if there is already an entry, the entry will be updated. It does not matter if you execute an update or insert a query.</p> <p>The primary key in our example is set to <code>user_email</code> and <code>creation_date</code> that defines record uniqueness.</p> <p>Let\u2019s insert a new record:</p> <pre><code>cqlsh&gt;      \nINSERT INTO todo_by_user_email (user_email, creation_date, name) VALUES('john@email.com', '2021-03-14 16:07:19.622+0000', 'Insert query');\n</code></pre> <p>And execute an update with a new <code>todo_uuid</code>:</p> <pre><code>cqlsh&gt;    \nUPDATE todo_by_user_email SET \n    name = 'Update query'\n  WHERE user_email = 'john@email.com' AND creation_date = '2021-03-14 16:10:19.622+0000';\n</code></pre> <p>2 new rows appear in our table:</p> <p><code>SELECT * FROM todo_by_user_email WHERE user_email='john@email.com';</code></p> <p>So you inserted a row using an update, and you can also use an insert to update:</p> <p><code>INSERT INTO todo_by_user_email (user_email,creation_date,name) VALUES('john@email.com', '2021-03-14 16:07:19.622+0000', 'Insert query updated');</code></p> <p>Let\u2019s check our updated row: <pre><code>SELECT * FROM todo_by_user_email WHERE user_email='john@email.com';\n</code></pre></p> <p>So <code>UPDATE</code> and <code>INSERT</code> are technically the same. Don\u2019t think that an <code>INSERT</code> fails if there is already a row with the same primary key.</p> <p>The same applies to an <code>UPDATE</code> \u2014 it will be executed, even if the row doesn\u2019t exist.</p> <p>The reason for this is because, by design, Cassandra rarely reads before writing to keep performance high.</p>"},{"location":"le12/NoSQL_Aufgabe/","title":"NoSQL-Aufgabe","text":"<ul> <li>Fasse wichtige Eigenschaften der NoSQL-Technologie zusammen</li> <li>Welche Key-Konzepte beinhaltet Apache Cassandra?</li> <li>Formuliere m\u00f6gliche Pr\u00fcfungsfragen selber und tausche diese mit den Kollegen und Kolleginnen aus.</li> </ul>"},{"location":"le12/install-3nodecassandraCluster/","title":"Apache Cassandra als 3-node Cluster mit docker installieren","text":"<p>das Vorgehen ist gleich, wie bei den vorheregehenden docker-Installationsarbeiten. Verwenden Sie f\u00fcr die aktuellste Version von Cassandra folgendes <code>docker-compose.yml</code></p> docker-compose.yml<pre><code>networks:\n  cassandra-net:\n    driver: bridge\n\nservices:\n\n  cassandra-1:\n    image: \"cassandra:latest\"  # cassandra:5.0.2\n    container_name: \"cassandra-1\"\n    ports:\n      - 7000:7000    # Mac-User: 7001:7000\n      - 9042:9042\n    networks:\n      - cassandra-net\n    environment:\n      - CASSANDRA_START_RPC=true       # default\n      - CASSANDRA_RPC_ADDRESS=0.0.0.0  # default\n      - CASSANDRA_LISTEN_ADDRESS=auto  # default, use IP addr of container # = CASSANDRA_BROADCAST_ADDRESS\n      - CASSANDRA_CLUSTER_NAME=my-cluster\n      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch\n      - CASSANDRA_DC=my-datacenter-1\n    volumes:\n      - cassandra-node-1:/var/lib/cassandra:rw\n    restart:\n      on-failure\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nodetool status\"]\n      interval: 2m\n      start_period: 2m\n      timeout: 10s\n      retries: 3\n\n  cassandra-2:\n    image: \"cassandra:latest\"\n    container_name: \"cassandra-2\"\n    ports:\n      - 9043:9042\n    networks:\n      - cassandra-net\n    environment:\n      - CASSANDRA_START_RPC=true       # default\n      - CASSANDRA_RPC_ADDRESS=0.0.0.0  # default\n      - CASSANDRA_LISTEN_ADDRESS=auto  # default, use IP addr of container # = CASSANDRA_BROADCAST_ADDRESS\n      - CASSANDRA_CLUSTER_NAME=my-cluster\n      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch\n      - CASSANDRA_DC=my-datacenter-1\n      - CASSANDRA_SEEDS=cassandra-1\n    depends_on:\n      cassandra-1:\n        condition: service_healthy\n    volumes:\n      - cassandra-node-2:/var/lib/cassandra:rw\n    restart:\n      on-failure\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nodetool status\"]\n      interval: 2m\n      start_period: 2m\n      timeout: 10s\n      retries: 3\n\n  cassandra-3:\n    image: \"cassandra:latest\"  # cassandra:5.0.2\n    container_name: \"cassandra-3\"\n    ports:\n      - 9044:9042\n    networks:\n      - cassandra-net\n    environment:\n      - CASSANDRA_START_RPC=true       # default\n      - CASSANDRA_RPC_ADDRESS=0.0.0.0  # default\n      - CASSANDRA_LISTEN_ADDRESS=auto  # default, use IP addr of container # = CASSANDRA_BROADCAST_ADDRESS\n      - CASSANDRA_CLUSTER_NAME=my-cluster\n      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch\n      - CASSANDRA_DC=my-datacenter-1\n      - CASSANDRA_SEEDS=cassandra-1\n    depends_on:\n      cassandra-2:\n        condition: service_healthy\n    volumes:\n      - cassandra-node-3:/var/lib/cassandra:rw\n    restart:\n      on-failure\n    healthcheck:\n      test: [\"CMD-SHELL\", \"nodetool status\"]\n      interval: 2m\n      start_period: 2m\n      timeout: 10s\n      retries: 3\n\nvolumes:\n  cassandra-node-1:\n  cassandra-node-2:\n  cassandra-node-3:\n</code></pre> <p>Hinweise zum compose-file:</p> <p>Die Bl\u00f6cke</p> <pre><code>healthcheck:\n      test: [\"CMD-SHELL\", \"nodetool status\"]\n      interval: 2m\n      start_period: 2m\n      timeout: 10s\n      retries: 3\n</code></pre> <p>und</p> <p><pre><code>depends_on:\n      cassandra-1:\n        condition: service_healthy\n</code></pre> und</p> <pre><code>depends_on:\n      cassandra-2:\n        condition: service_healthy\n</code></pre> <p>bewirken, dass <code>cassandra-3</code> (node3) erst startet, wenn <code>cassandra-2</code> (node2) healty ist. <code>cassandra-2</code> (node2) startet erst, wenn <code>cassandra-1</code> (node1) ok ist.</p> <p>Weitere Checks:</p> <ul> <li>mit dem Befehl <code>nodetool</code> innerhalb eines Containers wird der Status nach 2 Min. gepr\u00fcft. Die nodes haben also 2 Min Zeit zu booten.</li> <li>wenn die Antwort innerhalb von 10 sec eintrifft, wird der node als healthy betrachtet</li> <li>Die Checks werden alle 2 Minuten gemacht. Max. 3x.</li> </ul> <p>Wenn alle nodes auf einem Rechner laufen, muss jeder Node einen eigenen Port besitzen, ansonsten wird der Cluster, bzw. die 3 Container nicht starten.</p> <pre><code>cassandra-1:\n    ...\n    ports:\n      - 7000:7000\n      - 9042:9042\n\n  cassandra-2:\n    ...\n    ports:\n      - 9043:9042\n\n  cassandra-3:\n    ...\n    ports:\n      - 9044:9042\n</code></pre> <p>nach <code>docker compose up -d</code> kann es einen Moment dauern bis alle Cluster-Nodes up sind:</p> Das Starten der 3 Cluster-nodes dauert.. <p>Kontrolle im docker-Desktop, ob alle Container laufen. Punkte sind gr\u00fcn!</p> 3 Cassandra Container-Nodes bilden den Cluster <p>oder mit <code>docker ps -a</code></p> Check mit CLI <p>\u00dcberpr\u00fcfen Sie die Stati aller 3 nodes mit</p> <p><code>docker exec cassandra-3 nodetool status</code></p> <p>Erst wenn alle 3 Nodes up sind, ist der Cluster bereit. Jeder Node hat eine eigene IP-Adresse. Diese wird f\u00fcr die Kommunikation unter den Cluster-Nodes verwendet. </p> <p><code>UN</code> heisst: <code>Up</code> and <code>Normal</code></p> Status der Cluster-Nodes"},{"location":"le13/","title":"LE13 - Repetitionslektion","text":"<p>Die folgenden Aufgaben sind Beispiele wie sie an der Abschlusspr\u00fcfung vorkommen k\u00f6nnen. Einige Fragen wurden aus Pr\u00fcfungen der vergangenen Jahre \u00fcbernommen.</p> <p>An der Abschlusspr\u00fcfung wird es 2 Fragetypen geben:</p> <ul> <li> <p>PRAKTISCHE AUFGABEN (75 Minuten)</p> <ul> <li>Das Erstellen von ERM's aufgrund einer fachlichen Anforderung. Dazu kann Bleistift und Papier verwendet werden.</li> <li>Das Erstellen von SQL-Queries zum Ermitteln der gew\u00fcnschten Informationen </li> <li>Das Erstellen von DB's mit Tabellen, Keys und Constraints inkl. dem Einlesen von Demodatens\u00e4tzen mit Hilfe von SQL-Skripts</li> <li>Python-Programmierung in Verbindung mit einer Datenbank. Eingaben, Queries mit Verarbeitung der Daten mittels eines Python-Skripts wird im Rahmen der gemachten \u00dcbungen gefordert.</li> <li>Ihr Arbeitsger\u00e4t ist mit allen Tools, welche wir f\u00fcr die \u00dcbungen verwendet haben, ausgestattet und lauff\u00e4hig. Insbesondere MySQL-Workbench, DB Browser f\u00fcr SQLite (oder DBeaver) und eine Python-Programmierumgebung ihrer Wahl. Zus\u00e4tzliche oder andere Tools sind NICHT verboten. Es gilt nur die Anforderung, dass die Aufgaben gel\u00f6st werden k\u00f6nnen, insbesondere auch dann, wenn Ihnen eine Datenbank auf einem entfernten Rechner zur Verf\u00fcgung gestellt wird.</li> <li>Die Abgabe Ihrer Ergebnisse der praktischen Aufgaben erfolgt in Form eines SQL's, eines SQL-Skriptes, eines Python-Skripts oder auf Papier (z.Bsp. ERM).</li> <li>F\u00fcr die praktischen Aufgaben sind alle Hilfsmittel erlaubt, ausser die Verwendung von LLM's (ChatGPT, etc). </li> </ul> </li> <li> <p>THEORETISCHE AUFGABEN (45 Minuten)</p> <ul> <li>KPRIM und Freitext Aufgaben zu den behandelten Themen</li> <li>zu den theoretischen Aufgabentypen z\u00e4hlen auch Aufgaben zur Beurteilung von vorgegebenem Code (SQL, Python) oder einem vorgegebenem ERM</li> <li>keine Hilfsmittel erlaubt</li> </ul> </li> </ul> <p>Die Gewichtung f\u00fcr die Beurteilung der Abschlusspr\u00fcfung (65%) f\u00fcr Praxis und Theorie ist 60:40.</p>"},{"location":"le13/ue13-praxis/","title":"Praktische Aufgaben","text":""},{"location":"le13/ue13-praxis/#phasenmodell-fur-den-datenbankentwurf","title":"Phasenmodell f\u00fcr den Datenbankentwurf","text":"<p>Die Aufgaben unterliegen dem Modell des Datenbankentwurfs. Hier nochmals in einer grafischen Darstellung.</p> Phasenmodell f\u00fcr den Datenbankentwurf"},{"location":"le13/ue13-praxis/#aufgabe-1-bibliothek","title":"Aufgabe 1: BIBLIOTHEK","text":"<p>UE13-Praxis1 - BIBLIOTHEK</p> <p>ERM1: Sie wurden beauftragt, eine Datenbank f\u00fcr eine Bibliothek zu entwerfen. Die Bibliothek m\u00f6chte ihre B\u00fccher, Autoren, Verlage,     Mitglieder und Ausleihen effizient verwalten. Ihre Aufgabe ist es, ein ERM zu erstellen, das alle relevanten Entit\u00e4ten und deren     Beziehungen abbildet.</p> <p>Ihre Aufgabe ist es f\u00fcr folgende Anforderungen ein konzeptionelles und ein logisches ERM zu erstellen.</p> <p>Anforderungen:</p> <ol> <li> <p>Entit\u00e4ten und Attribute:</p> <ol> <li>Buch: ISBN, Titel, Erscheinungsjahr, Seitenzahl, Verlags-ID (Fremdschl\u00fcssel)</li> <li>Autor: Autor-ID, Vorname, Nachname, Geburtsdatum</li> <li>Verlag: Verlags-ID, Name, Adresse</li> <li>Mitglied: Mitglied-ID, Vorname, Nachname, Geburtsdatum, Mitgliedsdatum</li> <li>Ausleihe: Ausleih-ID, Buch-ISBN (Fremdschl\u00fcssel), Mitglieds-ID (Fremdschl\u00fcssel), Ausleihdatum R\u00fcckgabedatum</li> </ol> </li> <li> <p>Beziehungen</p> <ol> <li>Ein Buch kann von mehreren Autoren geschrieben werden, und ein Autor kann mehrere B\u00fccher schreiben.</li> <li>Ein Buch wird von einem Verlag verlegt.</li> <li>Ein Mitglied kann mehrere B\u00fccher ausleihen, und jedes Buch kann von verschiedenen Mitgliedern ausgeliehen werden.</li> </ol> </li> </ol> L\u00f6sungsvorschlag - BIBLIOTHEK <p>konzeptionelles ERM</p> <p>Kardinalit\u00e4ten</p> <ol> <li>Ein Buch kann von mehreren Autoren geschrieben werden, und ein Autor kann mehrere B\u00fccher schreiben. (Many-to-Many Beziehung). <ol> <li>Daraus folgt, dass eine Zwischentabelle erstellt werden muss: Buch_Autor mit <code>Buch-ISBN</code> und <code>Autor-ID</code> als Fremdschl\u00fcssel. Kombiniert ergeben sie den Prim\u00e4rschl\u00fcssel. Anmerkung hier: aus einem PK, der aus mehreren FK zusammengesetzt ist, kann ein eigener k\u00fcnstlicher PK erstellt werden, z.Bsp. Buch_Autor_ID. Dieser PK wird dann einfach von 1 hochgez\u00e4hlt. Funktional ist diese Methode identisch mit einem zusammengesetzten PK. Die Entscheidung dazu f\u00e4llen wir beim Erstellen des logischen ERM.</li> </ol> </li> <li>Ein Buch wird von einem Verlag verlegt und ein Verlag verlegt mehrere B\u00fccher. (One-to-Many Beziehung)<ol> <li>Ein Buch hat eine <code>Verlags-ID</code> als Fremdschl\u00fcssel.</li> </ol> </li> <li>Ein Mitglied kann mehrere B\u00fccher ausleihen, und jedes Buch kann von verschiedenen Mitgliedern ausgeliehen werden. (Many-to-Many Beziehung). <ol> <li>Hier wird die Beziehung Buch - Mitglied als m:m beschrieben. Die Beziehung ist die Ausleihe!</li> <li>Hier ist etwas offen gelassen: Was ist genau eine Ausleihe? Hat eine Ausleihe mehrere B\u00fccher oder gilt eine Ausleihe pro Buch? Falls eine Ausleihe mehrere B\u00fccher umfassen kann, dann entsteht eine zus\u00e4tzliche Entit\u00e4t \"Ausleihe\", in welcher mehrere B\u00fccher erfasst werden k\u00f6nnen. Wenn jedoch jedes Buch das ausgeliehen wird, als Ausleihe verstanden wird, kann die Beziehung Buch-Mitglied (m:m) direkt als Ausleihe verstanden werden. Die Beziehung Buch - Mitglied ist die Ausleihe! </li> <li>Im Beispiel hier werden wir diesen Fall betrachten. Andernfalls m\u00fcssten wir die Ausleihe als eigene Entit\u00e4t modellieren und die Beziehungen Ausleihe-Buch und Ausleihe - Mitglied untersuchen.</li> </ol> </li> </ol> <p> Konzeptionelles ERM Bibliothek </p> <p>logisches ERM</p> <p> logisches ERM Bibliothek </p> <p>ERM mit Beispiel-Records</p> <p>diese Darstellungsart zeigt sehr schnell, ob sich die Problemstellung in der DB-Struktur abbilden l\u00e4sst.  Man hat hier nicht so sch\u00f6n Platz, um die Datentypen zu spezifizieren.</p> <p>Es ist aber ein guter Test, um sein eigenes Modell zu pr\u00fcfen. Es ist ebanfalls ein gutes Kommunikationsmittel, um  mit dem Auftraggeber die Problemstellung zu verifizieren. Habe ich das Problem in allen Details verstanden? </p> <p> logisches ERM mit Datens\u00e4tzen als Beispiele </p>"},{"location":"le13/ue13-praxis/#aufgabe-2-unihockey","title":"Aufgabe 2: UNIHOCKEY","text":"<p>UE13-Praxis2 - UNIHOCKEY</p> <p>ERM 2:</p> <p>Ein Unihockeyclub nimmt mit seinem Team, bestehend aus mehreren Spielern, an verschiedenen Turnieren teil. Bei den Turnieren gibt es jeweils mehrere Spiele, die gegen andere Klubs ausgetragen werden. Pro gewonnenes Spiel gibt es 3 Punkte f\u00fcr das     siegreiche Team und pro unentschiedenes Spiel jeweils 1 Punkt f\u00fcr beide Teams. Pro Spiel, werden die erzielten Punkte/Goals protokolliert: der Sch\u00fctze und die Zeit.</p> <p>Am Schluss soll eine \u00dcbersicht erstellt werden dar\u00fcber, welche Teams am besten waren.</p> <p>Beispiel:</p> <pre><code>SVSE Unihockey Schweizermeisterschaft\nName        Gewonnen  Unentschieden  Verloren  Punkte\nTeam H          6           0            0       18\nTeam F          5           1            0       16\n....\nTeam B          1           2            3       5\nTeam I          0           1            5       1\n\nPlauschturnier Misch-Masch-Master\nName        Gewonnen  Unentschieden  Verloren  Punkte\nTeam X          4           0            0       12\nTeam Y          3           0            1       9\n....\nTeam W          2           0            2       6\nTeam Z          1           1            2       4\n</code></pre> <p>Mit dieser Ausgangslage bekommst Du den Auftrag eine dazugeh\u00f6rige Applikation samt relationalem Datenbank(-Schema) umzusetzen. </p> <ul> <li> <p>Aufgabe</p> <ul> <li>Erstelle die physische Umsetzung der DB mit SQL-Befehlen in MySQL oder SQLite. F\u00fcge auch Beispieldatens\u00e4tze hinzu. Als Grundlage und Hilfestellung, greifst Du auf das vorher erstellte logische ERM zur\u00fcck.</li> <li>Erstelle ein Python Skript, welches Abfragen auf die erstellte DB ausf\u00fchrt und textlich darstellt.</li> </ul> </li> </ul> L\u00f6sungsvorschlag - Unihockey-Turnier <p>Kardinalit\u00e4ten</p> <ol> <li>Ein Team besteht aus mehreren Spielern, und jeder Spieler geh\u00f6rt genau zu einem Team. (One-to-Many Beziehung)</li> <li>Ein Turnier hat mehrere Spiele, und jedes Spiel geh\u00f6rt zu genau einem Turnier. (One-to-Many Beziehung)</li> <li>Jedes Spiel hat zwei Teams, und ein Team kann an mehreren Spielen teilnehmen. (Many-to-Many Beziehung mit Attributen)</li> <li>Ein Tor wird in einem bestimmten Spiel von einem bestimmten Spieler erzielt. (One-to-Many Beziehung)</li> </ol> <p>Konzeptionelles ERM</p> <p> Konzeptionelles ERM Unihockey-Turnier </p> <ul> <li><code>Team</code> hat eine 1:m Beziehung zu <code>Spieler</code>.</li> <li><code>Spiel</code> hat eine m:m Beziehung zu <code>Team</code> (\u00fcber Team1 und Team2).</li> <li><code>Spiel</code> hat eine 1:m Beziehung zu <code>Tor</code>.</li> <li><code>Tor</code> hat eine 1:m Beziehung zu <code>Spieler</code></li> </ul> <p>Logisches ERM mit Demorecords</p> <p> logisches ERM Unihockey-Turnier mit Demo-Records </p> <p>Physische Datenbank</p> <p>DATENBANK ERSTELLEN mit</p> <p><code>sqlite3  .\\unihockey-DB.db</code></p> <p>oder mit </p> <p>DB Browser (SQLite)</p> <p>CREATE TABLES:</p> <pre><code>CREATE TABLE \"Team\" (\n\"TeamID\"    INTEGER,\n\"Name\"  TEXT,\nPRIMARY KEY(\"TeamID\")\n);\n\n\nCREATE TABLE \"Turnier\" (\n\"TurnierID\" INTEGER,\n\"Name\"  TEXT,\n\"Datum\" TEXT,\nPRIMARY KEY(\"TurnierID\")\n);\n\nCREATE TABLE \"Team\" (\n\"TeamID\"    INTEGER,\n\"Name\"  TEXT,\nPRIMARY KEY(\"TeamID\")\n);\n\n\nCREATE TABLE \"Spieler\" (\n\"SpielerID\" INTEGER,\n\"Vorname\"   TEXT,\n\"Name\"  TEXT,\n\"TeamID\"    INTEGER,\nPRIMARY KEY(\"SpielerID\"),\nFOREIGN KEY(TeamID) REFERENCES Team(TeamID)\n);\n\nCREATE TABLE \"Spiel\" (\n\"SpielID\"   INTEGER,\n\"TurnierID\" INTEGER,\n\"Team1ID\"   INTEGER,\n\"Team2ID\"   INTEGER,\n\"Zeit\"  TEXT,\n\"Team1Punkte\"   INTEGER,\n\"Team2Punkte\"   INTEGER,\nPRIMARY KEY(\"SpielID\"),\nFOREIGN KEY(TurnierID) REFERENCES Turnier(TurnierID),\nFOREIGN KEY(Team1ID) REFERENCES Team(TeamID),\nFOREIGN KEY(Team2ID) REFERENCES Team(TeamID)\n);\n\n\nCREATE TABLE \"Tor\" (\n\"TorID\" INTEGER,\n\"SpielID\"   INTEGER,\n\"SpielerID\" INTEGER,\n\"Minute\"    INTEGER,\nPRIMARY KEY(\"TorID\"),\nFOREIGN KEY(SpielID) REFERENCES Spiel(SpielID),\nFOREIGN KEY(SpielerID) REFERENCES Spieler(SpielerID)\n);\n</code></pre> <p>INSERT DEMO DATA:</p> <pre><code>INSERT INTO Team (TeamID,Name) VALUES (1,\"Team H\");\nINSERT INTO Team (TeamID,Name) VALUES (2,\"Team F\");\nINSERT INTO Team (TeamID,Name) VALUES (3,\"Team B\");\nINSERT INTO Team (TeamID,Name) VALUES (4,\"Team I\");\nINSERT INTO Team (TeamID,Name) VALUES (5,\"Team X\");\nINSERT INTO Team (TeamID,Name) VALUES (6,\"Team Y\");\nINSERT INTO Team (TeamID,Name) VALUES (7,\"Team W\");\nINSERT INTO Team (TeamID,Name) VALUES (8,\"Team Z\");\n\nINSERT INTO Turnier (TurnierID,Name,Datum) VALUES (1,\"SVSE SM\",\"2024-07-01\");\nINSERT INTO Turnier (TurnierID,Name,Datum) VALUES (2,\"Plausch MischMasch\",\"2024-08-02\");\nINSERT INTO Turnier (TurnierID,Name,Datum) VALUES (3,\"RegioCup\",\"2024-09-11\");\nINSERT INTO Turnier (TurnierID,Name,Datum) VALUES (4,\"Mix Turnier 24\",\"2024-12-12\");\n\n\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (1,\"Fritz\",\"Meier\",1);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (2,\"Hans\",\"M\u00fcller\",1);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (3,\"Kurt\",\"Viroux\",2);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (4,\"Thomas\",\"Benz\",2);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (5,\"Urs\",\"Huber\",1);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (6,\"Georg\",\"Fischlin\",3);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (7,\"Laurent\",\"Jans\",4);\nINSERT INTO Spieler (SpielerID,Vorname,Name,TeamID) VALUES (8,\"Daniel\",\"Cassis\",4);\n\nINSERT INTO Spiel (SpielID,TurnierID,Team1ID,Team2ID,Zeit,Team1Punkte,Team2Punkte) VALUES (1,1,1,2,\"08:00\",3,0);\nINSERT INTO Spiel (SpielID,TurnierID,Team1ID,Team2ID,Zeit,Team1Punkte,Team2Punkte) VALUES (2,1,1,3,\"09:00\",3,0);\nINSERT INTO Spiel (SpielID,TurnierID,Team1ID,Team2ID,Zeit,Team1Punkte,Team2Punkte) VALUES (3,1,1,4,\"10:00\",3,0);\nINSERT INTO Spiel (SpielID,TurnierID,Team1ID,Team2ID,Zeit,Team1Punkte,Team2Punkte) VALUES (4,1,2,1,\"11:00\",0,3);\nINSERT INTO Spiel (SpielID,TurnierID,Team1ID,Team2ID,Zeit,Team1Punkte,Team2Punkte) VALUES (5,1,3,1,\"12:00\",0,3);\n\n\nINSERT INTO Tor (TorID,SpielID,SpielerID,Minute) VALUES (1,1,1,7);\nINSERT INTO Tor (TorID,SpielID,SpielerID,Minute) VALUES (2,1,2,14);\nINSERT INTO Tor (TorID,SpielID,SpielerID,Minute) VALUES (3,3,5,25);\nINSERT INTO Tor (TorID,SpielID,SpielerID,Minute) VALUES (4,4,1,2);\nINSERT INTO Tor (TorID,SpielID,SpielerID,Minute) VALUES (5,5,1,8);\n</code></pre> <p>SQL-QUERY: Wie erh\u00e4lt man die Punkte pro Team?</p> <pre><code>SELECT \n    team.name,\n    SUM(CASE \n        WHEN spiel.team1punkte &gt; spiel.team2punkte THEN \n            CASE WHEN team.teamid = spiel.team1id THEN 3 ELSE 0 END\n        WHEN spiel.team1punkte &lt; spiel.team2punkte THEN \n            CASE WHEN team.teamid = spiel.team2id THEN 3 ELSE 0 END\n        WHEN spiel.team1punkte = spiel.team2punkte THEN \n            CASE WHEN team.teamid = spiel.team1id OR team.teamid = spiel.team2id THEN 1 ELSE 0 END\n    END) AS punkte\nFROM \n    team\nLEFT JOIN \n    spiel ON team.teamid = spiel.team1id OR team.teamid = spiel.team2id\nGROUP BY \n    team.name;    \n</code></pre> <p>Resultat mit vorhandenen Demo-Daten:</p> <p> Punkte pro Team </p> <p>Und nun noch ein Python-Skript, welches die Datenbank mit den Tabellen erstellt, Demodatens\u00e4tze einf\u00fcgt und eine Abfrage macht:</p> Python-Skript zum Erstellen der SQLite-DB mit Tabellen, Demorecords und Query<pre><code>import sqlite3\n\n# Verbindung zur SQLite-Datenbank herstellen (oder erstellen, falls sie nicht existiert)\nconnection = sqlite3.connect('UniHockeyTurnier.db')    \n\n# Einen Cursor erstellen, um SQL-Befehle auszuf\u00fchren\ncursor = connection.cursor()    \n\n# SQL-Befehle zum Erstellen der Tabellen\ncreate_tables_query = \"\"\"\nCREATE TABLE IF NOT EXISTS Team (\n    TeamID INTEGER PRIMARY KEY,\n    Name TEXT\n);    \n\nCREATE TABLE IF NOT EXISTS Turnier (\n    TurnierID INTEGER PRIMARY KEY,\n    Name TEXT,\n    Datum TEXT\n);    \n\nCREATE TABLE IF NOT EXISTS Spieler (\n    SpielerID INTEGER PRIMARY KEY,\n    Vorname TEXT,\n    Name TEXT,\n    TeamID INTEGER,\n    FOREIGN KEY(TeamID) REFERENCES Team(TeamID)\n);    \n\nCREATE TABLE IF NOT EXISTS Spiel (\n    SpielID INTEGER PRIMARY KEY,\n    TurnierID INTEGER,\n    Team1ID INTEGER,\n    Team2ID INTEGER,\n    Zeit TEXT,\n    Team1Punkte INTEGER,\n    Team2Punkte INTEGER,\n    FOREIGN KEY(TurnierID) REFERENCES Turnier(TurnierID),\n    FOREIGN KEY(Team1ID) REFERENCES Team(TeamID),\n    FOREIGN KEY(Team2ID) REFERENCES Team(TeamID)\n);    \n\nCREATE TABLE IF NOT EXISTS Tor (\n    TorID INTEGER PRIMARY KEY,\n    SpielID INTEGER,\n    SpielerID INTEGER,\n    Minute INTEGER,\n    FOREIGN KEY(SpielID) REFERENCES Spiel(SpielID),\n    FOREIGN KEY(SpielerID) REFERENCES Spieler(SpielerID)\n);\n\"\"\"    \n\n# Ausf\u00fchrung der SQL-Befehle\ncursor.executescript(create_tables_query)    \n\n# Datens\u00e4tze einf\u00fcgen\ninsert_data_query = \"\"\"\nINSERT INTO Team (TeamID, Name) VALUES \n(1, 'Team H'), (2, 'Team F'), (3, 'Team B'), (4, 'Team I'), \n(5, 'Team X'), (6, 'Team Y'), (7, 'Team W'), (8, 'Team Z');    \n\nINSERT INTO Turnier (TurnierID, Name, Datum) VALUES \n(1, 'SVSE SM', '2024-07-01'), (2, 'Plausch MischMasch', '2024-08-02'), \n(3, 'RegioCup', '2024-09-11'), (4, 'Mix Turnier 24', '2024-12-12');    \n\nINSERT INTO Spieler (SpielerID, Vorname, Name, TeamID) VALUES \n(1, 'Fritz', 'Meier', 1), (2, 'Hans', 'M\u00fcller', 1), (3, 'Kurt', 'Viroux', 2), \n(4, 'Thomas', 'Benz', 2), (5, 'Urs', 'Huber', 1), (6, 'Georg', 'Fischlin', 3), \n(7, 'Laurent', 'Jans', 4), (8, 'Daniel', 'Cassis', 4);    \n\nINSERT INTO Spiel (SpielID, TurnierID, Team1ID, Team2ID, Zeit, Team1Punkte, Team2Punkte) VALUES \n(1, 1, 1, 2, '08:00', 3, 0), (2, 1, 1, 3, '09:00', 3, 0), (3, 1, 1, 4, '10:00', 3, 0), \n(4, 1, 2, 1, '11:00', 0, 3), (5, 1, 3, 1, '12:00', 0, 3);    \n\nINSERT INTO Tor (TorID, SpielID, SpielerID, Minute) VALUES \n(1, 1, 1, 7), (2, 1, 2, 14), (3, 3, 5, 25), (4, 4, 1, 2), (5, 5, 1, 8);\n\"\"\"    \n\n# Ausf\u00fchrung der SQL-Befehle\ncursor.executescript(insert_data_query)    \n\n# Abfrage ausf\u00fchren und Ergebnisse anzeigen\nquery = \"\"\"\nSELECT team.name, SUM(\n    CASE \n        WHEN spiel.team1punkte &gt; spiel.team2punkte THEN \n            CASE WHEN team.teamid = spiel.team1id THEN 3 ELSE 0 END\n        WHEN spiel.team1punkte &lt; spiel.team2punkte THEN \n            CASE WHEN team.teamid = spiel.team2id THEN 3 ELSE 0 END\n        WHEN spiel.team1punkte = spiel.team2punkte THEN \n            CASE WHEN team.teamid = spiel.team1id OR team.teamid = spiel.team2id THEN 1 ELSE 0 END\n    END) AS punkte\nFROM team\nLEFT JOIN spiel ON team.teamid = spiel.team1id OR team.teamid = spiel.team2id\nGROUP BY team.name;\n\"\"\"    \n\ncursor.execute(query)\nresults = cursor.fetchall()    \n\n# Ergebnisse ausgeben\nfor row in results:\n    print(f\"Team: {row[0]}, Punkte: {row[1]}\")    \n\n# \u00c4nderungen speichern und Verbindung schliessen\nconnection.commit()\nconnection.close()    \n\nprint(\"Datenbank und Tabellen erfolgreich erstellt, Datens\u00e4tze eingef\u00fcgt und Abfrage ausgef\u00fchrt.\")\n</code></pre> <p>Ausgabe:</p> <p> Ausgabe der Punkte pro Team </p> <p>und hier noch ein Python-Skript, welches die Punktest\u00e4nde pro Team mit Hilfe von Pandas durchf\u00fchrt:</p> Python-Skript-Auswertung der Punktest\u00e4nde mit Pandas<pre><code>import sqlite3\nimport pandas as pd    \n\n# Verbindung zur SQLite-Datenbank herstellen\nconnection = sqlite3.connect('UniHockeyTurnier.db')    \n\n# Name und Datum des Turniers\nturnier_name = 'SVSE SM'\nturnier_datum = '2024-07-01'    \n\n# Abfrage der relevanten Tabellen in DataFrames laden\nteams_df = pd.read_sql_query(\"SELECT * FROM Team\", connection)\nspiele_df = pd.read_sql_query(f\"\"\"\nSELECT * \nFROM Spiel \nWHERE TurnierID = (SELECT TurnierID FROM Turnier WHERE Name = '{turnier_name}' AND Datum = '{turnier_datum}')\n\"\"\", connection)    \n\n# Berechnung der Punkte pro Team\npunkte_df = pd.DataFrame()\npunkte_df['TeamID'] = teams_df['TeamID']\npunkte_df['Name'] = teams_df['Name']\npunkte_df['Punkte'] = 0    \n\nfor index, row in spiele_df.iterrows():\n    if row['Team1Punkte'] &gt; row['Team2Punkte']:\n        punkte_df.loc[punkte_df['TeamID'] == row['Team1ID'], 'Punkte'] += 3\n    elif row['Team1Punkte'] &lt; row['Team2Punkte']:\n        punkte_df.loc[punkte_df['TeamID'] == row['Team2ID'], 'Punkte'] += 3\n    else:\n        punkte_df.loc[punkte_df['TeamID'] == row['Team1ID'], 'Punkte'] += 1\n        punkte_df.loc[punkte_df['TeamID'] == row['Team2ID'], 'Punkte'] += 1    \n\n# \u00dcberschrift erstellen\nprint(f\"Punktest\u00e4nde f\u00fcr Turnier {turnier_name} vom {turnier_datum}\")    \n\n# Ergebnisse anzeigen\nprint(punkte_df[['Name', 'Punkte']])    \n\n# Verbindung schliessen\nconnection.close()\n</code></pre> <p>Resultat:</p> <p> Ausgabe der Punkte pro Team mit \u00dcberschrift </p>"},{"location":"le13/ue13-praxis/#aufgabe-3-chatapplikation","title":"Aufgabe 3: CHATAPPLIKATION","text":"<p>UE13-Praxis3 - CHATAPPLIKATION</p> <p>Eine Chatapplikation erlaubt es Nutzern sich zu unterhalten. Die Konversationen sind jeweils zwischen zwei oder mehr Teilnehmer. Sobald mehr als zwei Teilnehmer dabei sind, ist es ein Gruppenchat. Es k\u00f6nnen nat\u00fcrlich Nachrichten im Textformat verschickt werden. Neuerdings werden aber auch Bilder und GIFs unterst\u00fctzt. Bilder/GIFs werden nicht in der Datenbank abgelegt, sondern jeweils nur ein String, welcher den Pfad der Datei (Bild/GIF) repr\u00e4sentiert.    </p> <p>Am Schluss soll \u00fcber alle Konversationen hinweg ein Verlauf (nach Zeit sortiert) der Konversation angezeigt werden.    </p> <p>Beispiel:    </p> <pre><code>Konversation von Seb + Max\n--------------------------\n16:04 Seb: \"Hast Du mir die L\u00f6sung f\u00fcr Aufgabe A?\"\n16:07 Max: \"Ich schicke dir einen Screenshot!\"\n16:08 Max: (Media-Nachricht) path/to/file.jpeg    \n\n\nGruppenchat zwischen Seb + Dan + Max\n-------------------------\n15:52 Dan: \"Habt ihr morgen Abend schon was vor?\"\n15:55 Seb: \"Nein, was willst Du unternehmen?\"\n15:57 Max: \"Weihnachstmarkt?\"\n</code></pre> <ul> <li> <p>Aufgabe    </p> <ul> <li>Erstelle die physische Umsetzung der DB mit SQL-Befehlen in MySQL oder SQLite. F\u00fcge auch Beispieldatens\u00e4tze hinzu. Als Grundlage und Hilfestellung, greifst Du auf das vorher erstellte konzeptionelle und logische ERM zur\u00fcck.</li> <li>Erstelle ein Python Skript, welches Abfragen auf die erstellte DB ausf\u00fchrt und textlich darstellt.</li> </ul> </li> </ul> L\u00f6sungsvorschlag - Chat-Applikation <p>Dieser L\u00f6sungsvorschlag fehlt noch. Wird nachgereicht.</p>"},{"location":"le13/ue13-praxis/#aufgabe-4-formula-1","title":"Aufgabe 4: FORMULA 1","text":"<p>UE13-Praxis4 - FORMULA 1</p> <p>Nachfolgend steht Dir eine SQLite-Datenbank zur Verf\u00fcgung. Es ist Teil der Aufgabe herauszufinden, wie die Datenbank aufgebaut ist und wo welche Informationen gespeichert sind. Die Datenbank hat einige Beziehungen und Tabellen mit \u00e4hnlichen (aber nicht gleichen) Informationen. F\u00fcr Dich bedeutet das, dass es mehrere korrekte L\u00f6sungsans\u00e4tze gibt. Lass Dich also nicht verwirren - studiere die Datenbank und wenn Du die n\u00f6tigen Informationen hast, schreibe das SELECT dazu.</p> <ul> <li> <p>Als Tool um die Datenbank zu \u00f6ffnen und zu analysieren, kannst Du verschiedene Tools verwenden:</p> <ul> <li>DB Browser (SQLite) </li> <li>Eine Alternative ist auch DBeaver Community.</li> <li>Auch die Verwendung von SQLite in der Konsole ist m\u00f6glich:  SQLite in der Konsole </li> </ul> </li> </ul> <p>Bitte beachten!</p> <p>Es wird an der Pr\u00fcfung erwartet, dass ein Tool f\u00fcr SQLite und MySQL auf dem Notebook lauff\u00e4hig ist und bedient werden kann.</p> <p>Download SQLite formula1.db</p> <p>Dein SELECT soll folgende \u00dcbersicht produzieren. Es sollen die erreichten Punkte aller Rennen absteigend sortiert der Season im Jahre 2009 dargestellt werden. Also die erste Zeile soll der Weltmeister mit den meisten Punkten sein: Jenson Button mit 95 Punkten.</p> <pre><code>forename |surname        |total_points|\n---------+---------------+------------+\nJenson   |Button         |          95|\nSebastian|Vettel         |          84|\nRubens   |Barrichello    |          77|\nMark     |Webber         |        69.5|\nLewis    |Hamilton       |          49|\nKimi     |Kimi R\u00e4ikk\u00f6nen |          48|\nNico     |Rosberg        |        34.5|\nJarno    |Trulli         |        32.5|\nFernando |Alonso         |          26|\nTimo     |Glock          |          24|\nFelipe   |Massa          |          22|\nHeikki   |Kovalainen     |          22|\nNick     |Heidfeld       |          19|\nRobert   |Kubica         |          17|\n.\n.\n</code></pre> <ul> <li> <p>Hinweise</p> <ul> <li>Sonderzeichen m\u00fcssen nicht korrekt dargestellt werden!</li> <li>Die Reihenfolge bei Punktegleichheit spielt keine Rolle!</li> <li>Idealerweise muss nur eine einzelne Zahl (2009) angepasst werden und Dein SQLite produziert auch ein korrektes Resultat f\u00fcr andere Jahre</li> </ul> </li> </ul>"},{"location":"le13/ue13-praxis/#hilfestellung-formula-1-db","title":"Hilfestellung FORMULA 1-DB","text":"Hilfestellung <p>Hier siehst Du das ERM der Formula1-DB. </p> <p>Hinweis: In der Pr\u00fcfungsaufgabe war diese Hilfestellung nicht vorhanden.</p> <p> ERM Formula1 Datenbank </p> L\u00f6sungsvorschlag - Formula 1 <pre><code>SELECT \n    drivers.forename,drivers.surname, SUM(results.points) as total_points\nFROM \n    drivers\nJOIN \n    results ON drivers.driverId = results.driverId\nJOIN\n    races ON races.raceId = results.raceId \nWHERE \n    races.year  = 2009\nGROUP BY \n    drivers.surname\nORDER BY \n    total_points DESC\nLIMIT 100\n</code></pre> <p>Erkl\u00e4rungen</p> <ul> <li><code>JOIN</code>: Verkn\u00fcpft die <code>drivers</code>-Tabelle mit der <code>results</code>-Tabelle basierend auf der <code>driver_id</code>.</li> <li><code>SUM</code>: Summiert die Punkte (points) f\u00fcr jeden Fahrer. Dies ist eine Aggregatsfunktion, welche von <code>GROUP BY</code> ben\u00f6tigt wird.</li> <li><code>GROUP BY</code>: Gruppiert die Ergebnisse nach Fahrernamen. </li> <li><code>ORDER BY</code>: Sortiert die Ergebnisse nach der Gesamtsumme der Punkte in absteigender Reihenfolge.</li> <li><code>LIMIT 1</code>: Gibt nur den Fahrer mit den meisten Punkten zur\u00fcck.</li> </ul> <p>Eine Abfrage auf dem Command-prompt, ohne SQlite starten zu m\u00fcssen, k\u00f6nnte so gemacht werden:</p> <p><code>sqlite3  -init abfrage.sql .\\formula1.db .quit</code></p> <p>Hier enth\u00e4lt die Datei <code>abfrage.sql</code> das Query und <code>.quit</code> verl\u00e4sst gleich wieder die SQLite-Umgebung:</p> <p> SQLite-Abfrage in einer Zeile </p>"},{"location":"le13/ue13-praxis/#aufgabe-5-smartwatches","title":"Aufgabe 5: SMARTWATCHES","text":"<p>UE13-Praxis5 - SMARTWATCHES</p> <p>Die Auswahl f\u00fcr Smartwatches soll in einem standardisierten Format abgespeichert werden. Die verschiedenen Hersteller haben verschiedene Produkte, welche zu unterschiedlichen Zeitpunkten auf dem Markt erschienen sind. Jede dieser Uhren hat meist spezifische physische Gegebenheiten (Gr\u00f6sse, Gewicht, Display, etc). Die Uhren haben auch Eigenschaften, welche sie teilen. Nennen wir sie verschiedene Funktionalit\u00e4ten (Features). Ein Beispiel k\u00f6nnte sein Pulssensor, Wasserdichtheit und viele weitere.</p> <ul> <li> <p>Aufgabe</p> <ul> <li>Erstellen Sie dazu ein konzeptionelles und logisches ERM</li> <li>Erstellen Sie die Datenbank physisch mit einer DB und einem Tool ihrer Wahl inkl. Demodaten. Erstellen Sie dazu ein SQL-Skript, welches alles enth\u00e4lt: Erstellung des Schemas, Kreieren der Tabellen mit Constraints, Laden von Demodatens\u00e4tzen.</li> <li>Auch hier sollten sie mit Python auf die DB zugreifen k\u00f6nnen und Abfragen aus\u00fchren.</li> </ul> </li> </ul> L\u00f6sungsvorschlag - SMARTWATCHES <p>KARDINALIT\u00c4TEN</p> <ul> <li>Ein Hersteller hat mehrere Produkte. (One-to-Many Beziehung)</li> <li>Ein Produkt kann mehrere Features haben, und ein Feature kann zu mehreren Produkten geh\u00f6ren. (Many-to-Many Beziehung). Features bezeichnen physische Eigenschaften der Uhr, welche in der Beziehung Produkt-Feature als Attribut erfasst werden kann.</li> </ul> <p>Konzeptionelles ERM SMARTWATCHES</p> <p> konzeptionelles ERM SMARTWATCHES </p> <p>Die m:m - Beziehung bedingt eine Zwischentabelle <code>Produkt_Feature</code>. Darin k\u00f6nnen wir die physischen Eigenschaften mit Attributen abbilden.</p> <p>Logisches ERM SMARTWATCHES</p> <p> logisches ERM SMARTWATCHES </p> <p>Python-Skript zum Erstellen einer MySQL-Datenbank und der Tabellen</p> Erstellung der Smartwatch-Datenbank mit Tabellen<pre><code>import mysql.connector\nfrom mysql.connector import errorcode\n\n# Verbindung zur MySQL-Datenbank herstellen\ntry:\n    connection = mysql.connector.connect(\n        host='localhost',\n        user='tom',\n        password='passwort'\n    )\n\n    cursor = connection.cursor()\n\n    # Datenbank erstellen\n    try:\n        cursor.execute(\"CREATE DATABASE SMARTWATCHES_V1\")\n        print(\"Datenbank 'SMARTWATCHES_V1' erfolgreich erstellt.\")\n    except mysql.connector.Error as err:\n        if err.errno == errorcode.ER_DB_CREATE_EXISTS:\n            print(\"Datenbank 'SMARTWATCHES_V1' existiert bereits.\")\n        else:\n            print(f\"Fehler beim Erstellen der Datenbank: {err}\")\n\n    # Verbindung zur erstellten Datenbank\n    connection.database = 'SMARTWATCHES_V1'\n\n    # Tabelle Hersteller erstellen\n    create_table_hersteller_query = \"\"\"\n    CREATE TABLE Hersteller (\n        HerstellerID INT AUTO_INCREMENT PRIMARY KEY,\n        Name VARCHAR(255),\n        Land VARCHAR(255)\n    )\n    \"\"\"\n    cursor.execute(create_table_hersteller_query)\n    print(\"Tabelle 'Hersteller' erfolgreich erstellt.\")\n\n    # Tabelle Produkt erstellen\n    create_table_produkt_query = \"\"\"\n    CREATE TABLE Produkt (\n        ProduktID INT AUTO_INCREMENT PRIMARY KEY,\n        HerstellerID INT,\n        Modellname VARCHAR(255),\n        Markteinf\u00fchrung DATE,\n        FOREIGN KEY (HerstellerID) REFERENCES Hersteller(HerstellerID)\n    )\n    \"\"\"\n    cursor.execute(create_table_produkt_query)\n    print(\"Tabelle 'Produkt' erfolgreich erstellt.\")\n\n    # Tabelle Feature erstellen\n    create_table_feature_query = \"\"\"\n    CREATE TABLE Feature (\n        FeatureID INT AUTO_INCREMENT PRIMARY KEY,\n        Name VARCHAR(255)\n    )\n    \"\"\"\n    cursor.execute(create_table_feature_query)\n    print(\"Tabelle 'Feature' erfolgreich erstellt.\")\n\n    # Zwischentabelle Produkt_Feature erstellen\n    create_table_produkt_feature_query = \"\"\"\n    CREATE TABLE Produkt_Feature (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        ProduktID INT,\n        FeatureID INT,\n        Pulssensor BOOLEAN NULL,\n        Wasserdichtheit INT NULL,\n        Durchmesser_Lunette INT NULL,\n        Gewicht_g INT NULL,\n        Uhrwerktyp TEXT NULL,\n        FOREIGN KEY (ProduktID) REFERENCES Produkt(ProduktID),\n        FOREIGN KEY (FeatureID) REFERENCES Feature(FeatureID)\n    )\n    \"\"\"\n    cursor.execute(create_table_produkt_feature_query)\n    print(\"Tabelle 'Produkt_Feature' erfolgreich erstellt.\")\n\nexcept mysql.connector.Error as err:\n    print(f\"Fehler: {err}\")\n    connection = None\nfinally:\n    if connection and connection.is_connected():\n        cursor.close()\n        connection.close()\n        print(\"MySQL-Verbindung geschlossen.\")\n</code></pre> <p>Python-Skript zum Laden von Demodaten in die MySQL-DB</p> Laden von Demodaten<pre><code>import mysql.connector\nfrom mysql.connector import errorcode\n\n# Verbindung zur MySQL-Datenbank herstellen\ntry:\n    connection = mysql.connector.connect(\n        host='dbserver.winglabs.ch',\n        user='meinuser',\n        password='meinpasswort',\n        database='SMARTWATCHES_V1'\n    )\n\n    cursor = connection.cursor()\n\n    # Demodaten f\u00fcr die Tabelle Hersteller einf\u00fcgen\n    insert_hersteller_query = \"\"\"\n    INSERT INTO Hersteller (Name, Land) VALUES\n    ('Apple', 'USA'),\n    ('Samsung', 'S\u00fcdkorea'),\n    ('Garmin', 'Schweiz'),\n    ('Fitbit', 'USA'),\n    ('Huawei', 'China')\n    \"\"\"\n    cursor.execute(insert_hersteller_query)\n    print(\"Demodaten in Tabelle 'Hersteller' eingef\u00fcgt.\")\n\n    # Demodaten f\u00fcr die Tabelle Produkt einf\u00fcgen\n    insert_produkt_query = \"\"\"\n    INSERT INTO Produkt (HerstellerID, Modellname, Markteinf\u00fchrung) VALUES\n    (1, 'Apple Watch Series 7', '2021-09-14'),\n    (2, 'Samsung Galaxy Watch 4', '2021-08-11'),\n    (3, 'Garmin Fenix 6', '2019-08-29'),\n    (4, 'Fitbit Charge 5', '2021-08-25'),\n    (5, 'Huawei Watch GT 2', '2019-10-23')\n    \"\"\"\n    cursor.execute(insert_produkt_query)\n    print(\"Demodaten in Tabelle 'Produkt' eingef\u00fcgt.\")\n\n    # Demodaten f\u00fcr die Tabelle Feature einf\u00fcgen\n    insert_feature_query = \"\"\"\n    INSERT INTO Feature (Name) VALUES\n    ('Pulsmessung'),\n    ('Schlaftracking'),\n    ('GPS'),\n    ('Wasserdicht'),\n    ('Bluetooth')\n    \"\"\"\n    cursor.execute(insert_feature_query)\n    print(\"Demodaten in Tabelle 'Feature' eingef\u00fcgt.\")\n\n    # Demodaten f\u00fcr die Tabelle Produkt_Feature einf\u00fcgen\n    insert_produkt_feature_query = \"\"\"\n    INSERT INTO Produkt_Feature (ProduktID, FeatureID, Pulssensor, Wasserdichtheit, Durchmesser_Lunette, Gewicht_g,     Uhrwerktyp) VALUES\n    (1, 1, TRUE, 50, 44, 38, 'Elektronisch'),\n    (1, 4, NULL, 50, NULL, NULL, NULL),\n    (2, 1, TRUE, 50, 44, 30, 'Elektronisch'),\n    (2, 3, NULL, NULL, NULL, NULL, 'Elektronisch'),\n    (3, 3, NULL, 100, 47, 83, 'Elektronisch'),\n    (4, 1, TRUE, 50, 35, 24, 'Elektronisch'),\n    (5, 2, NULL, 50, 46, 41, 'Elektronisch'),\n    (5, 4, NULL, 50, NULL, NULL, NULL)\n    \"\"\"\n    cursor.execute(insert_produkt_feature_query)\n    print(\"Demodaten in Tabelle 'Produkt_Feature' eingef\u00fcgt.\")\n\n    connection.commit()\n\nexcept mysql.connector.Error as err:\n    print(f\"Fehler: {err}\")\nfinally:\n    if connection.is_connected():\n        cursor.close()\n        connection.close()\n        print(\"MySQL-Verbindung geschlossen.\")\n</code></pre> einfache Query: Liste der Modelle mit Feature<pre><code>import mysql.connector\n\n# Verbindung zur MySQL-Datenbank herstellen\ntry:\n    connection = mysql.connector.connect(\n        host='localhost',\n        user='dein_user',\n        password='passwort',\n        database='SMARTWATCHES_V1'\n    )\n\n    cursor = connection.cursor()\n\n    # Abfrage erstellen\n    query = \"\"\"\n    SELECT Produkt.Modellname, Feature.Name AS Feature\n    FROM Produkt_Feature\n    JOIN Produkt ON Produkt_Feature.ProduktID = Produkt.ProduktID\n    JOIN Feature ON Produkt_Feature.FeatureID = Feature.FeatureID\n    \"\"\"\n\n    cursor.execute(query)\n    results = cursor.fetchall()\n\n    # Ergebnisse anzeigen\n    for row in results:\n        print(f\"Modellname: {row[0]}, Feature: {row[1]}\")\n\nexcept mysql.connector.Error as err:\n    print(f\"Fehler: {err}\")\nfinally:\n    if connection.is_connected():\n        cursor.close()\n        connection.close()\n        print(\"MySQL-Verbindung geschlossen.\")\n</code></pre> <p>Resultat der Abfrage:</p> <p> einfache Abfrage der Smartwatch-DB </p>"},{"location":"le13/ue13-theorie/","title":"Theoretische Fragen","text":"<p>UE13-Theorie-Frage 1 - NoSQL oder RDBMS?</p> <p>Was w\u00fcrdest Du f\u00fcr eine global eingesetzte Chat-Anwendung (Beispiel Whatsapp oder Telegram) bevorzugen? Nenne mindestens 4 Gr\u00fcnde f\u00fcr Deine Wahl.</p> L\u00f6sungsvorschlag Frage 1 <p>F\u00fcr eine Chat-Anwendung wie WhatsApp w\u00fcrde man in der Regel eine NoSQL-Datenbank bevorzugen. Hier sind einige Gr\u00fcnde f\u00fcr NoSQL:</p> <ol> <li>Hohe Skalierbarkeit: NoSQL-Datenbanken, wie z.B. Cassandra oder MongoDB, sind f\u00fcr eine horizontale Skalierung konzipiert, was bedeutet, dass sie einfach \u00fcber mehrere Server oder Knoten hinweg verteilt werden k\u00f6nnen. Dies ist entscheidend f\u00fcr Chat-Anwendungen, die Millionen von Benutzern und Nachrichten in Echtzeit verarbeiten m\u00fcssen.</li> <li>Flexibilit\u00e4t bei der Datenmodellierung: Chat-Anwendungen erfordern oft flexible Datenmodelle, um verschiedene Arten von Nachrichten, Anh\u00e4ngen, Metadaten und Benutzerinformationen zu speichern. NoSQL-Datenbanken bieten schemalose oder schema-leichte Strukturen, die sich leicht an wechselnde Anforderungen anpassen lassen.</li> <li>Hohe Schreib- und Lesegeschwindigkeit: NoSQL-Datenbanken sind f\u00fcr hohe Schreib- und Lesegeschwindigkeiten optimiert, was f\u00fcr die Echtzeit-Kommunikation in Chat-Anwendungen unerl\u00e4sslich ist. Sie k\u00f6nnen grosse Mengen von Daten schnell speichern und abrufen.</li> <li>Eventual Consistency: NoSQL-Datenbanken unterst\u00fctzen oft das Konzept der eventual consistency, was bedeutet, dass die Daten nicht sofort, aber schliesslich konsistent sein werden. Dies erm\u00f6glicht eine h\u00f6here Verf\u00fcgbarkeit und Leistung, was f\u00fcr eine Chat-Anwendung von Vorteil ist.</li> <li>Verteilung und Replikation: NoSQL-Datenbanken bieten eingebaute Mechanismen zur Datenverteilung und -replikation \u00fcber verschiedene geografische Standorte. Dies stellt sicher, dass die Anwendung auch bei Ausf\u00e4llen eines oder mehrerer Knoten weiterhin verf\u00fcgbar bleibt und die Latenzzeiten f\u00fcr Benutzer weltweit minimiert werden.</li> <li>Echtzeit-Analyse und Monitoring: Einige NoSQL-Datenbanken sind f\u00fcr die Echtzeitanalyse optimiert, was es erm\u00f6glicht, sofortige Einblicke in die Nutzungsmuster und Leistungsmetriken der Anwendung zu gewinnen.</li> <li>Hohe Verf\u00fcgbarkeit: NoSQL-Datenbanken sind oft so konzipiert, dass sie eine hohe Verf\u00fcgbarkeit gew\u00e4hrleisten, was f\u00fcr eine Chat-Anwendung, die rund um die Uhr funktionieren muss, unerl\u00e4sslich ist.</li> </ol> <p>Nat\u00fcrlich gibt es auch Szenarien, in denen relationale Datenbanken verwendet werden k\u00f6nnten, aber f\u00fcr die speziellen Anforderungen einer grossen, globalen Chat-Anwendung wie WhatsApp sind die oben genannten Vorteile von NoSQL-Datenbanken oft ausschlaggebend.</p> <p>Trotzdem: Welche Argumente w\u00fcrden f\u00fcr ein relationales Datenbanksystem sprechen?</p> <ol> <li>Datenintegrit\u00e4t: Relationale Datenbanksysteme (RDBMS) bieten starke Mechanismen zur Gew\u00e4hrleistung der Datenintegrit\u00e4t durch die Verwendung von Prim\u00e4r- und Fremdschl\u00fcsseln sowie durch Referenzielle Integrit\u00e4t. Dies stellt sicher, dass die Beziehungen zwischen den Daten konsistent bleiben.</li> <li>Strukturierte Daten: RDBMS eignen sich hervorragend f\u00fcr strukturierte Daten mit klar definierten Schemas. Die Daten sind in Tabellen organisiert, was die Verwaltung und Abfrage erleichtert.</li> <li>ACID-Eigenschaften: Relationale Datenbanken unterst\u00fctzen die ACID-Eigenschaften (Atomicity, Consistency, Isolation, Durability), die sicherstellen, dass Transaktionen zuverl\u00e4ssig und sicher ausgef\u00fchrt werden. Dies ist besonders wichtig f\u00fcr Anwendungen, die hohe Zuverl\u00e4ssigkeit und Datenintegrit\u00e4t erfordern.</li> <li>Leistungsstarke Abfragesprache: SQL ist eine leistungsstarke und standardisierte Sprache, die von relationalen Datenbanken unterst\u00fctzt wird. SQL erm\u00f6glicht komplexe Abfragen und Manipulationen der Daten und ist weit verbreitet und gut dokumentiert.</li> <li>Datenkonsistenz: In relationalen Datenbanken wird die Konsistenz der Daten durch strikte Transaktionsprotokolle und Sperrmechanismen gew\u00e4hrleistet. Dies ist besonders wichtig f\u00fcr Anwendungen, bei denen die Genauigkeit und Zuverl\u00e4ssigkeit der Daten entscheidend sind.</li> <li>Etablierte Technologien: RDBMS sind seit Jahrzehnten etabliert und weit verbreitet. Es gibt eine F\u00fclle von Ressourcen, Werkzeugen und Unterst\u00fctzung f\u00fcr die Implementierung und Wartung von relationalen Datenbanken.</li> <li>Normalisierung: Die Normalisierung in RDBMS hilft, Redundanzen zu minimieren und die Datenkonsistenz zu verbessern, indem Daten auf mehrere Tabellen verteilt und Beziehungen zwischen ihnen definiert werden.</li> <li>Skalierbarkeit: W\u00e4hrend NoSQL-Datenbanken oft als besser skalierbar angesehen werden, bieten moderne RDBMS auch Skalierungsm\u00f6glichkeiten sowohl vertikal (durch Hinzuf\u00fcgen von mehr Ressourcen zu einem einzigen Server) als auch horizontal (durch Partitionierung und Replikation der Daten \u00fcber mehrere Server).</li> <li>Sicherheitsfunktionen: Relationale Datenbanken bieten robuste Sicherheitsfunktionen, einschliesslich Benutzerberechtigungen, Zugriffskontrollen und Verschl\u00fcsselung. Diese Funktionen sind entscheidend f\u00fcr den Schutz sensibler Daten.</li> <li>Transaktionen und Rollbacks: RDBMS unterst\u00fctzen Transaktionen, die mehrere Operationen als eine Einheit behandeln. Wenn eine Operation fehlschl\u00e4gt, k\u00f6nnen alle \u00c4nderungen zur\u00fcckgesetzt werden, um den vorherigen konsistenten Zustand wiederherzustellen.</li> </ol> <p>Diese Eigenschaften machen relationale Datenbanksysteme besonders geeignet f\u00fcr Anwendungen, die hohe Anforderungen an Datenintegrit\u00e4t, Konsistenz und Zuverl\u00e4ssigkeit stellen. </p> <p>UE13-Theorie-Frage 2</p> <p>Angenommen, wir haben eine Tabelle Mitarbeiter mit den folgenden Attributen:</p> <pre><code>Mitarbeiter-ID (Prim\u00e4rschl\u00fcssel)\nAbteilungs-ID\nAbteilungsname\nAbteilungsleiter\n</code></pre> <p>Was f\u00fcr ein Problem erkennst Du hier? Welche Regel wird hier verletzt? Wie lautet dazu der Fachbegriff. Erkl\u00e4re in eigenen Worten.</p> L\u00f6sungsvorschlag Frage 2 <p>Hier wird die transitive Abh\u00e4ngigkeit verletzt. Diese soll in einer relationalen Datenbank unbedingt vermieden werden.</p> <p>Eine transitive Abh\u00e4ngigkeit in einer Datenbank tritt auf, wenn ein Nichtschl\u00fcsselattribut von einem anderen Nichtschl\u00fcsselattribut \u00fcber ein weiteres Nichtschl\u00fcsselattribut funktional abh\u00e4ngig ist. Mit anderen Worten: Wenn Attribut A von Attribut B abh\u00e4ngt, und Attribut B von Attribut C abh\u00e4ngt, dann ist Attribut A transitiv von Attribut C abh\u00e4ngig.</p> <p>In diesem Beispiel ist Mitarbeiter-ID der Prim\u00e4rschl\u00fcssel. Es gibt eine funktionale Abh\u00e4ngigkeit zwischen Mitarbeiter-ID und Abteilungs-ID (da jeder Mitarbeiter zu einer Abteilung geh\u00f6rt) und eine funktionale Abh\u00e4ngigkeit zwischen Abteilungs-ID und Abteilungsname sowie Abteilungsleiter. Dies f\u00fchrt zu einer transitiven Abh\u00e4ngigkeit zwischen Mitarbeiter-ID und Abteilungsname sowie Abteilungsleiter \u00fcber Abteilungs-ID.</p> <p>Warum sollten transitive Abh\u00e4ngigkeiten vermieden werden?</p> <ol> <li>Datenanomalien:<ol> <li>Einf\u00fcgeanomalie: Es kann schwierig sein, neue Daten einzuf\u00fcgen, ohne vorhandene Daten zu duplizieren. Zum Beispiel kann man keinen neuen Abteilungsleiter hinzuf\u00fcgen, ohne einen Mitarbeiter hinzuzuf\u00fcgen.</li> <li>L\u00f6schanomalie: L\u00f6schen eines Mitarbeiters kann dazu f\u00fchren, dass Informationen \u00fcber die Abteilung verloren gehen, wenn der Mitarbeiter der letzte in dieser Abteilung ist.</li> <li>\u00c4nderungsanomalie: Wenn der Abteilungsname oder -leiter ge\u00e4ndert werden muss, muss dies an mehreren Stellen erfolgen, was zu Inkonsistenzen f\u00fchren kann.</li> </ol> </li> <li>Redundanz: Durch transitive Abh\u00e4ngigkeiten werden Daten redundant gespeichert. Dies erh\u00f6ht den Speicherplatzbedarf und f\u00fchrt zu Problemen bei der Datenkonsistenz.</li> <li>Schwierige Wartung: Die Verwaltung und Wartung einer Datenbank mit transitiven Abh\u00e4ngigkeiten ist komplexer und fehleranf\u00e4lliger.</li> </ol> <p>L\u00f6sung:</p> <p>Um transitive Abh\u00e4ngigkeiten zu vermeiden, sollte die Datenbank in die dritte Normalform (3NF) gebracht werden. In der 3NF sollten alle Nichtschl\u00fcsselattribute direkt und nur von Schl\u00fcsseln abh\u00e4ngen.</p> <p>Im obigen Beispiel k\u00f6nnte man die Tabelle in zwei Tabellen aufteilen:</p> <ul> <li>Mitarbeiter (Mitarbeiter-ID, Abteilungs-ID)</li> <li>Abteilung (Abteilungs-ID, Abteilungsname, Abteilungsleiter)</li> </ul> <p>Dies eliminiert die transitiven Abh\u00e4ngigkeiten und normalisiert die Datenbankstruktur.</p> <p>UE13-Theorie-Frage 3</p> <p>Du versuchst einen Constraint zu erstellen, damit eine Referenz (FOREIGN KEY) zwischen <code>ticket</code> und <code>person</code> erstellt wird. Dabei erscheint jedoch ein Fehler. Beschreibe warum das DBMS so reagiert? Wieso l\u00e4sst es nicht zu diese Referenz zu erstellen?</p> <pre><code>CREATE TABLE `ticket` (\n    `id` INT NOT NULL AUTO_INCREMENT,\n    `id_person` INT NOT NULL,\n    `seat` VARCHAR(12),\n    PRIMARY KEY (`id`),\n    CONSTRAINT `ticket_person`\n      FOREIGN KEY (`id_person`)\n      REFERENCES`person`(`id`)\n      ON UPDATE SET NULL\n      ON DELETE SET NULL\n); \n</code></pre> <p>Fehlermeldung:</p> <p><code>ERROR 1830 (HY000): Column 'id_person' cannot be NOT NULL: needed in a foreign key constraint 'ticket_person' SET NULL</code></p> L\u00f6sungsvorschlag Frage 3 <p>Der Fehler tritt auf, weil das DBMS (Datenbankmanagementsystem) nicht zul\u00e4sst, dass eine NOT NULL-Spalte in einer Fremdschl\u00fcssel-Beziehung mit <code>ON UPDATE SET NULL</code> oder <code>ON DELETE SET NULL</code> verwendet wird.</p> <p>Grund f\u00fcr den Fehler:</p> <ul> <li>Fremdschl\u00fcssel-Bedingung: Wenn eine Fremdschl\u00fcsselbedingung ON UPDATE SET NULL oder ON DELETE SET NULL definiert ist, bedeutet dies, dass der Fremdschl\u00fcsselwert (id_person in diesem Fall) auf NULL gesetzt werden soll, wenn der referenzierte Datensatz in der person-Tabelle aktualisiert oder gel\u00f6scht wird.</li> <li>NOT NULL-Spalte: Die Spalte id_person ist als NOT NULL definiert. Das bedeutet, dass sie keine NULL-Werte enthalten darf.</li> </ul> <p>Der Konflikt entsteht, weil die Fremdschl\u00fcsselregel besagt, dass <code>id_person</code> auf <code>NULL</code> gesetzt werden soll, aber die <code>NOT NULL</code>-Einschr\u00e4nkung dies verhindert. Das DBMS kann diese widerspr\u00fcchlichen Anweisungen nicht ausf\u00fchren, daher wird der Fehler <code>ERROR 1830 (HY000): Column 'id_person' cannot be NOT NULL: needed in a foreign key constraint 'ticket_person' SET NULL</code> angezeigt.</p> <p>UE13-Theorie-Frage 4</p> <p>Welche M\u00f6glichkeiten gibt es, das Problem in <code>UE13-Theorie 3</code> zu l\u00f6sen? Nenne eine m\u00f6gliche L\u00f6sung, indem Du das SQL korrigierst. Welche Zeilen \u00e4nderst Du wie?</p> L\u00f6sungsvorschlag Frage 4 <p>Variante 1:</p> <p>\u00c4ndern Sie die Spalte <code>id_person</code>, um <code>NULL</code>-Werte zuzulassen: Wenn es in Ordnung ist, dass <code>id_person</code> <code>NULL</code>-Werte enthalten kann, k\u00f6nnen Sie die <code>NOT NULL</code>-Einschr\u00e4nkung entfernen.</p> <pre><code>CREATE TABLE `ticket` (\n`id` INT **NOT NULL** AUTO_INCREMENT,\n`id_person` INT,\n`seat` VARCHAR(12),\nPRIMARY KEY (`id`),\nCONSTRAINT `ticket_person`\n  FOREIGN KEY (`id_person`)\n  REFERENCES `person` (`id`)\n  ON UPDATE SET NULL\n  ON DELETE SET NULL\n);\n</code></pre> <p>Variante 2:</p> <p>\u00c4ndern Sie die Fremdschl\u00fcsselbedingung: Wenn die <code>NOT NULL</code>-Einschr\u00e4nkung beibehalten werden soll, k\u00f6nnen Sie die Fremdschl\u00fcsselbedingung \u00e4ndern, sodass die Spalte nicht auf <code>NULL</code> gesetzt wird. Zum Beispiel:</p> <pre><code>CREATE TABLE `ticket` (\n    `id` INT NOT NULL AUTO_INCREMENT,\n    `id_person` INT NOT NULL,\n    `seat` VARCHAR(12),\n   PRIMARY KEY (`id`),\n   CONSTRAINT `ticket_person`\n     FOREIGN KEY (`id_person`)\n     REFERENCES `person` (`id`)\n     **ON UPDATE CASCADE**\n     **ON DELETE RESTRICT**\n);\n</code></pre> <p>In diesem Beispiel wird <code>ON UPDATE CASCADE</code> sicherstellen, dass \u00c4nderungen an der <code>person</code>-Tabelle auf die <code>ticket</code>-Tabelle \u00fcbertragen werden, und <code>ON DELETE RESTRICT</code> verhindert das L\u00f6schen eines Datensatzes aus der <code>person</code>-Tabelle, wenn er von der <code>ticket</code>-Tabelle referenziert wird.</p> <p>UE13-Theorie-Frage 5</p> <p>Wodurch kann der unten angegebene Fehler verursacht werden? Welche Massnahmen w\u00fcrdest Du hier ergreifen?</p> <p><code>ERROR 1824 (HY000): Failed to open the referenced table 'person'</code></p> L\u00f6sungsvorschlag Frage 5 <p>Der Fehler <code>ERROR 1824 (HY000)</code>: <code>Failed to open the referenced table 'person'</code> tritt normalerweise auf, wenn die referenzierte Tabelle (<code>person</code>) aus irgendeinem Grund nicht ge\u00f6ffnet oder gefunden werden kann. Dies kann durch verschiedene Faktoren verursacht werden:</p> <ol> <li>Tabelle existiert nicht: Die Tabelle <code>person</code> k\u00f6nnte nicht existieren, entweder weil sie nie erstellt wurde oder weil sie gel\u00f6scht wurde.</li> <li>Datenbankname nicht angegeben: Wenn die referenzierte Tabelle in einer anderen Datenbank als der aktuellen liegt, aber der Datenbankname nicht angegeben wurde, kann dies zu diesem Fehler f\u00fchren.</li> <li>Zugriffsrechte: M\u00f6glicherweise hat der Benutzer, der die Fremdschl\u00fcsseleinschr\u00e4nkung erstellt, nicht die erforderlichen Rechte, um auf die Tabelle <code>person</code> zuzugreifen.</li> <li>Tabelle besch\u00e4digt: Die Tabelle <code>person</code> k\u00f6nnte besch\u00e4digt oder in einem inkonsistenten Zustand sein, was das \u00d6ffnen der Tabelle verhindert.</li> <li>Gross-/Kleinschreibung: In einigen Datenbanksystemen wird zwischen Gross- und Kleinschreibung unterschieden. Wenn die Tabellennamen im <code>CREATE TABLE</code>-Statement und in der Datenbank unterschiedlich geschrieben sind, kann dies zu dem Fehler f\u00fchren.</li> </ol> <p>UE13-Theorie-Frage 6</p> <p>Beschreibe was nachfolgendes Query macht. </p> <p><code>SELECT COUNT(DISTINCT</code>address<code>) AS</code>street<code>FROM</code>contacts<code>;</code></p> L\u00f6sungsvorschlag Frage 6 <p>Gib mir die Anzahl unterschiedlicher Adresszeilen in der Tabelle contacts. </p> <p>UE13-Theorie-Frage 7</p> <p>Erkl\u00e4re in Deinen eigenen Worten, was ein ORM konzeptionell macht.</p> L\u00f6sungsvorschlag Frage 7 <p>Ein ORM (Object Relational Mapping) Konzept ist ein Programmierparadigma, das es Entwicklern erm\u00f6glicht, Datenbankoperationen auf eine objektorientierte Weise durchzuf\u00fchren. Anstatt SQL-Abfragen direkt in den Code zu schreiben, k\u00f6nnen Entwickler Datenbankoperationen durch das Arbeiten mit Objekten und deren Methoden ausf\u00fchren.    </p> <p>Eine der bekanntesten ORM-Bibliotheken in Python ist SQLAlchemy und Peewee.</p> <p>UE13-Theorie-Frage 8</p> <p>Beschreibe die Idee hinter einem JOIN und was ist der Unterschied eines RIGHT JOIN und eines LEFT JOIN?</p> L\u00f6sungsvorschlag Frage 8 <p>Die Idee hinter einem JOIN in SQL besteht darin, Daten aus zwei oder mehr Tabellen basierend auf einem gemeinsamen Feld zu kombinieren. Dies erm\u00f6glicht es, Informationen aus verschiedenen Tabellen in einer einzigen Abfrage zu verkn\u00fcpfen und zu analysieren.</p> <p>Arten von JOINs und ihre Unterschiede:</p> <ul> <li>INNER JOIN oder JOIN: Nur die Datens\u00e4tze, bei denen die verkn\u00fcpften Felder in beiden Tabellen \u00fcbereinstimmen, werden in das Ergebnis einbezogen.</li> <li>LEFT JOIN: Alle Datens\u00e4tze aus der linken Tabelle und die \u00fcbereinstimmenden Datens\u00e4tze aus der rechten Tabelle werden einbezogen. Wenn keine \u00dcbereinstimmung in der rechten Tabelle gefunden wird, werden NULL-Werte f\u00fcr die Felder der rechten Tabelle zur\u00fcckgegeben.</li> <li>RIGHT JOIN: Alle Datens\u00e4tze aus der rechten Tabelle und die \u00fcbereinstimmenden Datens\u00e4tze aus der linken Tabelle werden einbezogen. Wenn keine \u00dcbereinstimmung in der linken Tabelle gefunden wird, werden NULL-Werte f\u00fcr die Felder der linken Tabelle zur\u00fcckgegeben.</li> </ul> <p>UE13-Theorie-Frage 9</p> <p>Sie sind dabei eine Datenbank zu modelieren: Es gibt verschiedene Kategorien und dazugeh\u00f6rige Produkte. Dabei soll es m\u00f6glich sein, dass beliebige Unterkategorien erstellt werden k\u00f6nnen. Ein Produkt geh\u00f6rt entweder einer Kategorie oder Unterkategorie an. Beschreiben Sie in Worten (kein Code) eine m\u00f6gliche L\u00f6sung.</p> L\u00f6sungsvorschlag Frage 9 <ul> <li>Um die Hierarchie von Kategorien und Unterkategorien darzustellen, wird das Attribut ElternID in der Kategorien-Tabelle verwendet. Dies erlaubt es, beliebig tief verschachtelte Kategorien zu erstellen. Eine Hauptkategorie hat eine ElternID von NULL, w\u00e4hrend eine Unterkategorie auf die KategorieID ihrer \u00fcbergeordneten Kategorie verweist.</li> <li>Jedes Produkt in der Produkte-Tabelle wird einer Kategorie oder Unterkategorie zugewiesen durch das Attribut KategorieID, das auf die entsprechende KategorieID in der Kategorien-Tabelle verweist.</li> </ul> <p>UE13-Theorie-Frage 10</p> <p>Nenne vier verschiedene Datentypen eines Datenbankmanagementsystems und gebe Beispielwerte an.</p> L\u00f6sungsvorschlag Frage 10 <p>Antworten sind  auch in den gemachten Aufgaben.</p> <ul> <li>DECIMAL: wird verwendet, um genaue numerische Werte zu speichern. Dies ist besonders n\u00fctzlich f\u00fcr finanzielle Daten oder andere Anwendungen, bei denen Genauigkeit entscheidend ist. Man kann die Genauigkeit (Anzahl der Ziffern insgesamt) und die Skalierung (Anzahl der Ziffern nach dem Dezimalpunkt) explizit angeben, z.B. <code>DECIMAL(10, 4)</code> bedeutet eine maximale Zahl mit 10 Ziffern insgesamt, von denen 4 nach dem Dezimalpunkt liegen. Beispiele: 12345.6789, -9876.54321, 100.00, 0.0001</li> <li>FLOAT: wird verwendet, um Fliesskommazahlen zu speichern, die eine gewisse Genauigkeit haben, aber nicht exakt sind. Es ist besonders n\u00fctzlich f\u00fcr wissenschaftliche Berechnungen und Anwendungen, bei denen die Genauigkeit flexibel sein kann. Es unterst\u00fctzt die Darstellung von Werten in wissenschaftlicher Notation (z.B. 1.23456789e+10 f\u00fcr 12345678900). Beispiele: 3.1415927, -2.71828</li> <li>CHAR: steht f\u00fcr \"Character\" und wird verwendet, um Zeichenketten fester L\u00e4nge zu speichern. Wenn du einen CHAR(10)-Datentyp definierst, bedeutet das, dass die Zeichenkette immer genau 10 Zeichen lang ist.</li> <li>VARCHAR: steht f\u00fcr \"Variable Character\" und wird verwendet, um Zeichenketten variabler L\u00e4nge zu speichern. Wenn du einen VARCHAR(50)-Datentyp definierst, kann die Zeichenkette jede beliebige L\u00e4nge bis zu 50 Zeichen haben. Im Gegensatz zu CHAR wird eine Zeichenkette in VARCHAR nicht mit Leerzeichen aufgef\u00fcllt. Sie wird in der exakten L\u00e4nge gespeichert, die eingegeben wurde. Zum Beispiel wird \"Hi\" in einem VARCHAR(5)-Feld als \"Hi\" gespeichert, belegt also nur 2 Byte und reserviert nicht 5.</li> </ul> <p>UE13-Theorie-Frage 11</p> <p>Es existieren konzeptionellen Elementen in einem RDBMS, welche Elementen in InfluxDB sehr \u00e4hnlich sind. Die Begriffe sind jedoch unterschiedlich. Stellen Sie 4 Begriffe in einer Vergleichstabelle einander gegen\u00fcber.</p> Element in RDBMS Element in InfluxDB . . . . . . . . L\u00f6sungsvorschlag Frage 11 Element in RDBMS Element in InfluxDB Datenbank/Schema Bucket Tabelle Measurement Attribut Field Index Tag"},{"location":"le14/","title":"LE14 - Pr\u00fcfungsvorbereitung","text":"<p>.</p>"},{"location":"lekn/","title":"LE08-Kompetenznachweis (Theorie) zu Themen aus LE01 - LE07","text":"<p>Gewichtung 35%</p> <p>Dauer der Pr\u00fcfung: 45 Min.</p> <p>Hilfsmittel: keine</p> <p>Datum: 13.11.2024</p> <p>Zeit: 14:30</p> <p>Raum: W.23</p>"},{"location":"schlusspruefung/","title":"Schlusspr\u00fcfung","text":"<p>Gewichtung: 65% davon Theorie 40% und Praxis 60%.</p> <p>Datum: Mittwoch, 29.1.25</p> <p>Zeit: 14:00</p> <p>Ort: W.23</p>"},{"location":"schlusspruefung/#prufungsteile","title":"Pr\u00fcfungsteile","text":"<ul> <li> <p>THEORETISCHE AUFGABEN (45 Minuten)</p> <ul> <li>KPRIM und Freitext Aufgaben zu den behandelten Themen</li> <li>zu den theoretischen Aufgabentypen z\u00e4hlen auch Aufgaben zur Beurteilung von vorgegebenem Code (SQL, Python) oder einem vorgegebenem ERM</li> <li>keine Hilfsmittel erlaubt</li> </ul> </li> <li> <p>PRAKTISCHE AUFGABEN (75 Minuten)</p> <ul> <li>Das Erstellen von ERM's aufgrund einer fachlichen Anforderung. Dazu kann Bleistift und Papier verwendet werden.</li> <li>Das Erstellen von SQL-Queries zum Ermitteln der gew\u00fcnschten Informationen </li> <li>Das Erstellen von DB's mit Tabellen, Keys und Constraints inkl. dem Einlesen von Demodatens\u00e4tzen mit Hilfe von SQL-Skripts</li> <li>Python-Programmierung in Verbindung mit einer Datenbank. Eingaben, Queries mit Verarbeitung der Daten mittels eines Python-Skripts wird im Rahmen der gemachten \u00dcbungen gefordert.</li> <li>Ihr Arbeitsger\u00e4t ist mit allen Tools, welche wir f\u00fcr die \u00dcbungen verwendet haben, ausgestattet und lauff\u00e4hig. Insbesondere MySQL-Workbench, DB Browser f\u00fcr SQLite (oder DBeaver) und eine Python-Programmierumgebung ihrer Wahl. Zus\u00e4tzlich oder andere Tools sind NICHT verboten. Es gilt nur die Anforderung, dass die Aufgaben gel\u00f6st werden k\u00f6nnen, insbesondere auch dann, wenn Ihnen eine Datenbank auf einem entfernten Rechner zur Verf\u00fcgung gestellt wird.</li> <li>Abgabe Ihrer Ergebnisse der praktischen Aufgaben erfolgt in Form eines SQL's, eines SQL-Skriptes, eines Python-Skripts oder auf einem Blatt Papier f\u00fcr ERMs.</li> <li>F\u00fcr die praktischen Aufgaben sind alle Hilfsmittel erlaubt, ausser die Verwendung von LLM's (ChatGPT, etc). </li> </ul> </li> </ul>"}]}